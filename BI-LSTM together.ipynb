{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17b3775b-c199-4060-830e-06afce97368b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_6252\\2678955929.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[col] = pd.to_numeric(y[col], errors='coerce')\n",
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_6252\\2678955929.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y.fillna(0, inplace=True)\n",
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_6252\\2678955929.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y.fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/245\n",
      "3/3 [==============================] - 5s 522ms/step - loss: 0.6915 - accuracy: 0.5250 - val_loss: 0.6914 - val_accuracy: 0.5000\n",
      "Epoch 2/245\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6882 - accuracy: 0.5750 - val_loss: 0.6928 - val_accuracy: 0.5000\n",
      "Epoch 3/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6867 - accuracy: 0.6000 - val_loss: 0.6943 - val_accuracy: 0.4500\n",
      "Epoch 4/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6843 - accuracy: 0.5750 - val_loss: 0.6958 - val_accuracy: 0.4500\n",
      "Epoch 5/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6838 - accuracy: 0.5750 - val_loss: 0.6974 - val_accuracy: 0.5000\n",
      "Epoch 6/245\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.6824 - accuracy: 0.5750 - val_loss: 0.6982 - val_accuracy: 0.5500\n",
      "Epoch 7/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6809 - accuracy: 0.5875 - val_loss: 0.6987 - val_accuracy: 0.5000\n",
      "Epoch 8/245\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6801 - accuracy: 0.5875 - val_loss: 0.6994 - val_accuracy: 0.5000\n",
      "Epoch 9/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6791 - accuracy: 0.5750 - val_loss: 0.7005 - val_accuracy: 0.5500\n",
      "Epoch 10/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6779 - accuracy: 0.5625 - val_loss: 0.7015 - val_accuracy: 0.5500\n",
      "Epoch 11/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6766 - accuracy: 0.5500 - val_loss: 0.7033 - val_accuracy: 0.5500\n",
      "Epoch 12/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6754 - accuracy: 0.5500 - val_loss: 0.7051 - val_accuracy: 0.5500\n",
      "Epoch 13/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6741 - accuracy: 0.5500 - val_loss: 0.7071 - val_accuracy: 0.5500\n",
      "Epoch 14/245\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6728 - accuracy: 0.5500 - val_loss: 0.7091 - val_accuracy: 0.5500\n",
      "Epoch 15/245\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6712 - accuracy: 0.5500 - val_loss: 0.7126 - val_accuracy: 0.5500\n",
      "Epoch 16/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6702 - accuracy: 0.5500 - val_loss: 0.7176 - val_accuracy: 0.5500\n",
      "Epoch 17/245\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6685 - accuracy: 0.5500 - val_loss: 0.7216 - val_accuracy: 0.5500\n",
      "Epoch 18/245\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6684 - accuracy: 0.5500 - val_loss: 0.7282 - val_accuracy: 0.5500\n",
      "Epoch 19/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6660 - accuracy: 0.5375 - val_loss: 0.7313 - val_accuracy: 0.5500\n",
      "Epoch 20/245\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6647 - accuracy: 0.5375 - val_loss: 0.7345 - val_accuracy: 0.5500\n",
      "Epoch 21/245\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6630 - accuracy: 0.5625 - val_loss: 0.7353 - val_accuracy: 0.5000\n",
      "Epoch 22/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6619 - accuracy: 0.5750 - val_loss: 0.7401 - val_accuracy: 0.5000\n",
      "Epoch 23/245\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6602 - accuracy: 0.5750 - val_loss: 0.7444 - val_accuracy: 0.5000\n",
      "Epoch 24/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6586 - accuracy: 0.5750 - val_loss: 0.7496 - val_accuracy: 0.5000\n",
      "Epoch 25/245\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6569 - accuracy: 0.5750 - val_loss: 0.7587 - val_accuracy: 0.5000\n",
      "Epoch 26/245\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6557 - accuracy: 0.6000 - val_loss: 0.7671 - val_accuracy: 0.5000\n",
      "Epoch 27/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6538 - accuracy: 0.5875 - val_loss: 0.7686 - val_accuracy: 0.5000\n",
      "Epoch 28/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6521 - accuracy: 0.5875 - val_loss: 0.7771 - val_accuracy: 0.4500\n",
      "Epoch 29/245\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6505 - accuracy: 0.5750 - val_loss: 0.7847 - val_accuracy: 0.4500\n",
      "Epoch 30/245\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6481 - accuracy: 0.5750 - val_loss: 0.8016 - val_accuracy: 0.4000\n",
      "Epoch 31/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6469 - accuracy: 0.6000 - val_loss: 0.8117 - val_accuracy: 0.4000\n",
      "Epoch 32/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6458 - accuracy: 0.6125 - val_loss: 0.8167 - val_accuracy: 0.4000\n",
      "Epoch 33/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6427 - accuracy: 0.6125 - val_loss: 0.8115 - val_accuracy: 0.4000\n",
      "Epoch 34/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6413 - accuracy: 0.6125 - val_loss: 0.8133 - val_accuracy: 0.4500\n",
      "Epoch 35/245\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6411 - accuracy: 0.6125 - val_loss: 0.8053 - val_accuracy: 0.4500\n",
      "Epoch 36/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6394 - accuracy: 0.6250 - val_loss: 0.8085 - val_accuracy: 0.5000\n",
      "Epoch 37/245\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6397 - accuracy: 0.6375 - val_loss: 0.8178 - val_accuracy: 0.5500\n",
      "Epoch 38/245\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.6380 - accuracy: 0.6250 - val_loss: 0.8586 - val_accuracy: 0.4500\n",
      "Epoch 39/245\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6347 - accuracy: 0.6125 - val_loss: 0.8806 - val_accuracy: 0.4500\n",
      "Epoch 40/245\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6395 - accuracy: 0.6000 - val_loss: 0.8935 - val_accuracy: 0.4500\n",
      "Epoch 41/245\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6350 - accuracy: 0.6375 - val_loss: 0.8679 - val_accuracy: 0.5000\n",
      "Epoch 42/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6340 - accuracy: 0.6250 - val_loss: 0.8480 - val_accuracy: 0.5000\n",
      "Epoch 43/245\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6337 - accuracy: 0.6375 - val_loss: 0.8480 - val_accuracy: 0.5000\n",
      "Epoch 44/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6312 - accuracy: 0.6375 - val_loss: 0.8726 - val_accuracy: 0.5000\n",
      "Epoch 45/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6318 - accuracy: 0.6500 - val_loss: 0.8823 - val_accuracy: 0.5000\n",
      "Epoch 46/245\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6317 - accuracy: 0.6500 - val_loss: 0.8592 - val_accuracy: 0.5000\n",
      "Epoch 47/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6299 - accuracy: 0.6375 - val_loss: 0.8666 - val_accuracy: 0.5000\n",
      "Epoch 48/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6285 - accuracy: 0.6500 - val_loss: 0.8652 - val_accuracy: 0.5000\n",
      "Epoch 49/245\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6280 - accuracy: 0.6500 - val_loss: 0.8616 - val_accuracy: 0.5000\n",
      "Epoch 50/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6257 - accuracy: 0.6625 - val_loss: 0.8794 - val_accuracy: 0.5000\n",
      "Epoch 51/245\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6260 - accuracy: 0.6625 - val_loss: 0.8869 - val_accuracy: 0.5500\n",
      "Epoch 52/245\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6266 - accuracy: 0.6625 - val_loss: 0.8918 - val_accuracy: 0.5500\n",
      "Epoch 53/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6258 - accuracy: 0.6500 - val_loss: 0.8792 - val_accuracy: 0.5500\n",
      "Epoch 54/245\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6234 - accuracy: 0.6500 - val_loss: 0.8517 - val_accuracy: 0.5500\n",
      "Epoch 55/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6214 - accuracy: 0.6625 - val_loss: 0.8345 - val_accuracy: 0.5500\n",
      "Epoch 56/245\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6245 - accuracy: 0.6750 - val_loss: 0.8255 - val_accuracy: 0.5500\n",
      "Epoch 57/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6208 - accuracy: 0.6875 - val_loss: 0.8405 - val_accuracy: 0.5500\n",
      "Epoch 58/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6206 - accuracy: 0.6500 - val_loss: 0.8632 - val_accuracy: 0.5500\n",
      "Epoch 59/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6175 - accuracy: 0.6625 - val_loss: 0.8653 - val_accuracy: 0.5500\n",
      "Epoch 60/245\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6164 - accuracy: 0.6750 - val_loss: 0.8530 - val_accuracy: 0.5500\n",
      "Epoch 61/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6122 - accuracy: 0.6625 - val_loss: 0.8556 - val_accuracy: 0.5500\n",
      "Epoch 62/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6097 - accuracy: 0.6625 - val_loss: 0.8621 - val_accuracy: 0.5500\n",
      "Epoch 63/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6049 - accuracy: 0.6875 - val_loss: 0.8669 - val_accuracy: 0.5500\n",
      "Epoch 64/245\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6004 - accuracy: 0.7000 - val_loss: 0.8717 - val_accuracy: 0.5000\n",
      "Epoch 65/245\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5949 - accuracy: 0.7000 - val_loss: 0.9075 - val_accuracy: 0.5500\n",
      "Epoch 66/245\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5884 - accuracy: 0.6875 - val_loss: 0.8965 - val_accuracy: 0.4500\n",
      "Epoch 67/245\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5825 - accuracy: 0.7250 - val_loss: 0.8777 - val_accuracy: 0.4500\n",
      "Epoch 68/245\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5843 - accuracy: 0.7500 - val_loss: 0.9212 - val_accuracy: 0.4500\n",
      "Epoch 69/245\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5682 - accuracy: 0.7375 - val_loss: 1.0306 - val_accuracy: 0.3500\n",
      "Epoch 70/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.5968 - accuracy: 0.7000 - val_loss: 0.9773 - val_accuracy: 0.4500\n",
      "Epoch 71/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.5581 - accuracy: 0.7500 - val_loss: 0.9062 - val_accuracy: 0.4500\n",
      "Epoch 72/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5942 - accuracy: 0.7125 - val_loss: 0.9185 - val_accuracy: 0.5000\n",
      "Epoch 73/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5839 - accuracy: 0.7375 - val_loss: 0.9353 - val_accuracy: 0.4500\n",
      "Epoch 74/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5632 - accuracy: 0.7625 - val_loss: 0.9680 - val_accuracy: 0.4500\n",
      "Epoch 75/245\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5430 - accuracy: 0.7250 - val_loss: 1.0452 - val_accuracy: 0.5000\n",
      "Epoch 76/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5619 - accuracy: 0.7625 - val_loss: 1.1097 - val_accuracy: 0.4000\n",
      "Epoch 77/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5624 - accuracy: 0.7625 - val_loss: 1.0524 - val_accuracy: 0.4000\n",
      "Epoch 78/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.5477 - accuracy: 0.7750 - val_loss: 0.9810 - val_accuracy: 0.4000\n",
      "Epoch 79/245\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.5358 - accuracy: 0.7750 - val_loss: 0.9774 - val_accuracy: 0.4000\n",
      "Epoch 80/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5330 - accuracy: 0.7625 - val_loss: 0.9904 - val_accuracy: 0.4000\n",
      "Epoch 81/245\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.5257 - accuracy: 0.7625 - val_loss: 1.0322 - val_accuracy: 0.3500\n",
      "Epoch 82/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5167 - accuracy: 0.7750 - val_loss: 1.0879 - val_accuracy: 0.3500\n",
      "Epoch 83/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5201 - accuracy: 0.7625 - val_loss: 1.1879 - val_accuracy: 0.3000\n",
      "Epoch 84/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5296 - accuracy: 0.7750 - val_loss: 1.1329 - val_accuracy: 0.3500\n",
      "Epoch 85/245\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.5102 - accuracy: 0.7875 - val_loss: 1.0839 - val_accuracy: 0.4000\n",
      "Epoch 86/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5010 - accuracy: 0.7875 - val_loss: 1.0939 - val_accuracy: 0.3500\n",
      "Epoch 87/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4942 - accuracy: 0.7750 - val_loss: 1.1049 - val_accuracy: 0.3500\n",
      "Epoch 88/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4890 - accuracy: 0.7875 - val_loss: 1.1156 - val_accuracy: 0.3500\n",
      "Epoch 89/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4817 - accuracy: 0.7875 - val_loss: 1.1417 - val_accuracy: 0.3500\n",
      "Epoch 90/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4799 - accuracy: 0.7875 - val_loss: 1.1592 - val_accuracy: 0.3500\n",
      "Epoch 91/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4754 - accuracy: 0.7750 - val_loss: 1.1419 - val_accuracy: 0.3500\n",
      "Epoch 92/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4711 - accuracy: 0.8000 - val_loss: 1.1966 - val_accuracy: 0.3500\n",
      "Epoch 93/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4690 - accuracy: 0.7875 - val_loss: 1.3940 - val_accuracy: 0.3000\n",
      "Epoch 94/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4669 - accuracy: 0.8000 - val_loss: 1.2766 - val_accuracy: 0.3000\n",
      "Epoch 95/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4394 - accuracy: 0.8250 - val_loss: 1.1967 - val_accuracy: 0.3500\n",
      "Epoch 96/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4621 - accuracy: 0.8000 - val_loss: 1.2001 - val_accuracy: 0.4000\n",
      "Epoch 97/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4353 - accuracy: 0.8125 - val_loss: 1.4018 - val_accuracy: 0.3000\n",
      "Epoch 98/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4373 - accuracy: 0.8125 - val_loss: 1.4605 - val_accuracy: 0.3500\n",
      "Epoch 99/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4275 - accuracy: 0.8375 - val_loss: 1.2652 - val_accuracy: 0.3000\n",
      "Epoch 100/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4118 - accuracy: 0.8250 - val_loss: 1.2254 - val_accuracy: 0.3500\n",
      "Epoch 101/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4166 - accuracy: 0.8125 - val_loss: 1.2806 - val_accuracy: 0.4000\n",
      "Epoch 102/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4013 - accuracy: 0.8250 - val_loss: 1.2310 - val_accuracy: 0.4500\n",
      "Epoch 103/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3970 - accuracy: 0.8375 - val_loss: 1.2876 - val_accuracy: 0.4000\n",
      "Epoch 104/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.3924 - accuracy: 0.8625 - val_loss: 1.3457 - val_accuracy: 0.4000\n",
      "Epoch 105/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.3835 - accuracy: 0.8500 - val_loss: 1.3408 - val_accuracy: 0.4000\n",
      "Epoch 106/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.3831 - accuracy: 0.8250 - val_loss: 1.3825 - val_accuracy: 0.4000\n",
      "Epoch 107/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3773 - accuracy: 0.8375 - val_loss: 1.4837 - val_accuracy: 0.3500\n",
      "Epoch 108/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3644 - accuracy: 0.8375 - val_loss: 1.4880 - val_accuracy: 0.3000\n",
      "Epoch 109/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3556 - accuracy: 0.8500 - val_loss: 1.4615 - val_accuracy: 0.3000\n",
      "Epoch 110/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3503 - accuracy: 0.8500 - val_loss: 1.3899 - val_accuracy: 0.4500\n",
      "Epoch 111/245\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.3409 - accuracy: 0.8625 - val_loss: 1.3821 - val_accuracy: 0.4500\n",
      "Epoch 112/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3365 - accuracy: 0.8625 - val_loss: 1.3724 - val_accuracy: 0.4500\n",
      "Epoch 113/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3305 - accuracy: 0.8625 - val_loss: 1.4422 - val_accuracy: 0.5000\n",
      "Epoch 114/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3195 - accuracy: 0.8625 - val_loss: 1.4062 - val_accuracy: 0.4000\n",
      "Epoch 115/245\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.3156 - accuracy: 0.8625 - val_loss: 1.4918 - val_accuracy: 0.4000\n",
      "Epoch 116/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3043 - accuracy: 0.8750 - val_loss: 1.5406 - val_accuracy: 0.4000\n",
      "Epoch 117/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2979 - accuracy: 0.8875 - val_loss: 1.5338 - val_accuracy: 0.4000\n",
      "Epoch 118/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2883 - accuracy: 0.8875 - val_loss: 1.5292 - val_accuracy: 0.4000\n",
      "Epoch 119/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2828 - accuracy: 0.8750 - val_loss: 1.5525 - val_accuracy: 0.4000\n",
      "Epoch 120/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2748 - accuracy: 0.8875 - val_loss: 1.5685 - val_accuracy: 0.4000\n",
      "Epoch 121/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2659 - accuracy: 0.8875 - val_loss: 1.5819 - val_accuracy: 0.4000\n",
      "Epoch 122/245\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2576 - accuracy: 0.8875 - val_loss: 1.5972 - val_accuracy: 0.4000\n",
      "Epoch 123/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2517 - accuracy: 0.9000 - val_loss: 1.6285 - val_accuracy: 0.4000\n",
      "Epoch 124/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2478 - accuracy: 0.8875 - val_loss: 1.6155 - val_accuracy: 0.4000\n",
      "Epoch 125/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2340 - accuracy: 0.9125 - val_loss: 1.6209 - val_accuracy: 0.5000\n",
      "Epoch 126/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2350 - accuracy: 0.9125 - val_loss: 1.6283 - val_accuracy: 0.4500\n",
      "Epoch 127/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2208 - accuracy: 0.9125 - val_loss: 1.6435 - val_accuracy: 0.4000\n",
      "Epoch 128/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2135 - accuracy: 0.9125 - val_loss: 1.7110 - val_accuracy: 0.4000\n",
      "Epoch 129/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2075 - accuracy: 0.9125 - val_loss: 1.7552 - val_accuracy: 0.3500\n",
      "Epoch 130/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2068 - accuracy: 0.9250 - val_loss: 1.7396 - val_accuracy: 0.4000\n",
      "Epoch 131/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1999 - accuracy: 0.9500 - val_loss: 1.7094 - val_accuracy: 0.4000\n",
      "Epoch 132/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1926 - accuracy: 0.9500 - val_loss: 1.7837 - val_accuracy: 0.4000\n",
      "Epoch 133/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1808 - accuracy: 0.9500 - val_loss: 1.8454 - val_accuracy: 0.3500\n",
      "Epoch 134/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1759 - accuracy: 0.9625 - val_loss: 1.8493 - val_accuracy: 0.3500\n",
      "Epoch 135/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1645 - accuracy: 0.9625 - val_loss: 1.8846 - val_accuracy: 0.4000\n",
      "Epoch 136/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1641 - accuracy: 0.9500 - val_loss: 1.8739 - val_accuracy: 0.3500\n",
      "Epoch 137/245\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.1541 - accuracy: 0.9500 - val_loss: 1.8842 - val_accuracy: 0.3500\n",
      "Epoch 138/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1490 - accuracy: 0.9500 - val_loss: 2.0000 - val_accuracy: 0.3500\n",
      "Epoch 139/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1435 - accuracy: 0.9625 - val_loss: 2.0205 - val_accuracy: 0.3500\n",
      "Epoch 140/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1393 - accuracy: 0.9625 - val_loss: 1.8869 - val_accuracy: 0.4000\n",
      "Epoch 141/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1420 - accuracy: 0.9750 - val_loss: 2.0238 - val_accuracy: 0.4000\n",
      "Epoch 142/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1384 - accuracy: 0.9500 - val_loss: 2.0594 - val_accuracy: 0.4000\n",
      "Epoch 143/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1371 - accuracy: 0.9625 - val_loss: 2.1155 - val_accuracy: 0.4000\n",
      "Epoch 144/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1505 - accuracy: 0.9625 - val_loss: 2.1132 - val_accuracy: 0.4000\n",
      "Epoch 145/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1257 - accuracy: 0.9625 - val_loss: 2.0935 - val_accuracy: 0.5000\n",
      "Epoch 146/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1297 - accuracy: 0.9625 - val_loss: 2.1297 - val_accuracy: 0.5500\n",
      "Epoch 147/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1250 - accuracy: 0.9750 - val_loss: 2.1491 - val_accuracy: 0.5500\n",
      "Epoch 148/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1201 - accuracy: 0.9750 - val_loss: 2.1506 - val_accuracy: 0.4500\n",
      "Epoch 149/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1091 - accuracy: 0.9750 - val_loss: 2.1610 - val_accuracy: 0.4500\n",
      "Epoch 150/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1047 - accuracy: 0.9750 - val_loss: 2.2486 - val_accuracy: 0.4000\n",
      "Epoch 151/245\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0998 - accuracy: 0.9750 - val_loss: 2.2732 - val_accuracy: 0.4000\n",
      "Epoch 152/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0978 - accuracy: 0.9875 - val_loss: 2.2309 - val_accuracy: 0.4000\n",
      "Epoch 153/245\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0909 - accuracy: 0.9625 - val_loss: 2.2033 - val_accuracy: 0.4000\n",
      "Epoch 154/245\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0909 - accuracy: 0.9625 - val_loss: 2.2039 - val_accuracy: 0.4000\n",
      "Epoch 155/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0870 - accuracy: 0.9625 - val_loss: 2.2416 - val_accuracy: 0.3500\n",
      "Epoch 156/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0823 - accuracy: 0.9625 - val_loss: 2.2859 - val_accuracy: 0.3500\n",
      "Epoch 157/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0816 - accuracy: 0.9750 - val_loss: 2.3253 - val_accuracy: 0.3500\n",
      "Epoch 158/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0771 - accuracy: 0.9875 - val_loss: 2.3284 - val_accuracy: 0.3500\n",
      "Epoch 159/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0749 - accuracy: 0.9750 - val_loss: 2.3295 - val_accuracy: 0.3500\n",
      "Epoch 160/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0725 - accuracy: 0.9875 - val_loss: 2.3551 - val_accuracy: 0.3500\n",
      "Epoch 161/245\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0702 - accuracy: 0.9875 - val_loss: 2.3678 - val_accuracy: 0.3500\n",
      "Epoch 162/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0661 - accuracy: 0.9875 - val_loss: 2.3581 - val_accuracy: 0.3500\n",
      "Epoch 163/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0662 - accuracy: 0.9875 - val_loss: 2.3443 - val_accuracy: 0.3500\n",
      "Epoch 164/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0644 - accuracy: 0.9875 - val_loss: 2.3500 - val_accuracy: 0.3500\n",
      "Epoch 165/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0609 - accuracy: 0.9875 - val_loss: 2.3523 - val_accuracy: 0.3500\n",
      "Epoch 166/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0589 - accuracy: 0.9875 - val_loss: 2.3594 - val_accuracy: 0.4000\n",
      "Epoch 167/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0576 - accuracy: 1.0000 - val_loss: 2.3850 - val_accuracy: 0.3500\n",
      "Epoch 168/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0556 - accuracy: 1.0000 - val_loss: 2.3879 - val_accuracy: 0.3500\n",
      "Epoch 169/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0512 - accuracy: 1.0000 - val_loss: 2.4159 - val_accuracy: 0.3500\n",
      "Epoch 170/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 2.4118 - val_accuracy: 0.3500\n",
      "Epoch 171/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0479 - accuracy: 1.0000 - val_loss: 2.3782 - val_accuracy: 0.3500\n",
      "Epoch 172/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0470 - accuracy: 1.0000 - val_loss: 2.3736 - val_accuracy: 0.3500\n",
      "Epoch 173/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0450 - accuracy: 1.0000 - val_loss: 2.3953 - val_accuracy: 0.4000\n",
      "Epoch 174/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0419 - accuracy: 1.0000 - val_loss: 2.4207 - val_accuracy: 0.4000\n",
      "Epoch 175/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0420 - accuracy: 1.0000 - val_loss: 2.4346 - val_accuracy: 0.3500\n",
      "Epoch 176/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0380 - accuracy: 1.0000 - val_loss: 2.4000 - val_accuracy: 0.3500\n",
      "Epoch 177/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0402 - accuracy: 1.0000 - val_loss: 2.4061 - val_accuracy: 0.3500\n",
      "Epoch 178/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 2.4407 - val_accuracy: 0.3500\n",
      "Epoch 179/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 2.4667 - val_accuracy: 0.4000\n",
      "Epoch 180/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 2.4759 - val_accuracy: 0.3500\n",
      "Epoch 181/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 2.4729 - val_accuracy: 0.3500\n",
      "Epoch 182/245\n",
      "3/3 [==============================] - 0s 38ms/step - loss: 0.0301 - accuracy: 1.0000 - val_loss: 2.4860 - val_accuracy: 0.3500\n",
      "Epoch 183/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 2.4953 - val_accuracy: 0.3500\n",
      "Epoch 184/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 2.4971 - val_accuracy: 0.3500\n",
      "Epoch 185/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 2.4717 - val_accuracy: 0.4000\n",
      "Epoch 186/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 2.4712 - val_accuracy: 0.4500\n",
      "Epoch 187/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 2.4949 - val_accuracy: 0.5000\n",
      "Epoch 188/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 2.5338 - val_accuracy: 0.5000\n",
      "Epoch 189/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 2.5504 - val_accuracy: 0.4500\n",
      "Epoch 190/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 2.5629 - val_accuracy: 0.4000\n",
      "Epoch 191/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 2.5630 - val_accuracy: 0.4000\n",
      "Epoch 192/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 2.5705 - val_accuracy: 0.4500\n",
      "Epoch 193/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 2.5658 - val_accuracy: 0.4500\n",
      "Epoch 194/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 2.5659 - val_accuracy: 0.4500\n",
      "Epoch 195/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 2.5735 - val_accuracy: 0.4500\n",
      "Epoch 196/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 2.5974 - val_accuracy: 0.4500\n",
      "Epoch 197/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 2.6183 - val_accuracy: 0.4500\n",
      "Epoch 198/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 2.6196 - val_accuracy: 0.4500\n",
      "Epoch 199/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 2.6043 - val_accuracy: 0.4500\n",
      "Epoch 200/245\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 2.5967 - val_accuracy: 0.4500\n",
      "Epoch 201/245\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 2.6139 - val_accuracy: 0.4500\n",
      "Epoch 202/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 2.6454 - val_accuracy: 0.4500\n",
      "Epoch 203/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 2.6465 - val_accuracy: 0.4500\n",
      "Epoch 204/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 2.6183 - val_accuracy: 0.4500\n",
      "Epoch 205/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 2.6457 - val_accuracy: 0.4500\n",
      "Epoch 206/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 2.6549 - val_accuracy: 0.4500\n",
      "Epoch 207/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 2.6571 - val_accuracy: 0.4500\n",
      "Epoch 208/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 2.6616 - val_accuracy: 0.4500\n",
      "Epoch 209/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 2.6754 - val_accuracy: 0.4500\n",
      "Epoch 210/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 2.6784 - val_accuracy: 0.4500\n",
      "Epoch 211/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 2.6999 - val_accuracy: 0.4500\n",
      "Epoch 212/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 2.7037 - val_accuracy: 0.4500\n",
      "Epoch 213/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 2.7042 - val_accuracy: 0.4500\n",
      "Epoch 214/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 2.7061 - val_accuracy: 0.4500\n",
      "Epoch 215/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 2.7142 - val_accuracy: 0.4500\n",
      "Epoch 216/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 2.7305 - val_accuracy: 0.4500\n",
      "Epoch 217/245\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 2.7382 - val_accuracy: 0.4500\n",
      "Epoch 218/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 2.7401 - val_accuracy: 0.4500\n",
      "Epoch 219/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 2.7562 - val_accuracy: 0.4500\n",
      "Epoch 220/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 2.7672 - val_accuracy: 0.4500\n",
      "Epoch 221/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 2.7677 - val_accuracy: 0.4500\n",
      "Epoch 222/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 2.7742 - val_accuracy: 0.4500\n",
      "Epoch 223/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 2.7624 - val_accuracy: 0.4500\n",
      "Epoch 224/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 2.7592 - val_accuracy: 0.4500\n",
      "Epoch 225/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 2.7656 - val_accuracy: 0.4500\n",
      "Epoch 226/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 2.7630 - val_accuracy: 0.4500\n",
      "Epoch 227/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 2.7788 - val_accuracy: 0.4500\n",
      "Epoch 228/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 2.7899 - val_accuracy: 0.4500\n",
      "Epoch 229/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 2.7902 - val_accuracy: 0.4500\n",
      "Epoch 230/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 2.7813 - val_accuracy: 0.4500\n",
      "Epoch 231/245\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 2.7933 - val_accuracy: 0.4500\n",
      "Epoch 232/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 2.8122 - val_accuracy: 0.4500\n",
      "Epoch 233/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 2.8389 - val_accuracy: 0.4500\n",
      "Epoch 234/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 2.8512 - val_accuracy: 0.4500\n",
      "Epoch 235/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 2.8330 - val_accuracy: 0.4500\n",
      "Epoch 236/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 2.8211 - val_accuracy: 0.4500\n",
      "Epoch 237/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 2.8143 - val_accuracy: 0.4500\n",
      "Epoch 238/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 2.8220 - val_accuracy: 0.4500\n",
      "Epoch 239/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 2.8482 - val_accuracy: 0.4500\n",
      "Epoch 240/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 2.8654 - val_accuracy: 0.4000\n",
      "Epoch 241/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 2.8697 - val_accuracy: 0.4000\n",
      "Epoch 242/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 2.8838 - val_accuracy: 0.4000\n",
      "Epoch 243/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 2.8955 - val_accuracy: 0.4000\n",
      "Epoch 244/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 2.8828 - val_accuracy: 0.4500\n",
      "Epoch 245/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 2.8739 - val_accuracy: 0.4500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1529e087950>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from  tensorflow . keras.models import Sequential\n",
    "from  tensorflow . keras.layers import Bidirectional,LSTM, Dense\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv('Clinical data ppf and eph.csv')\n",
    "\n",
    "# Split the data into input (X) and output (y) samples\n",
    "X = data.drop(columns=['intraop_ppf', 'intraop_eph'])\n",
    "y = data[['intraop_ppf', 'intraop_eph']]\n",
    "\n",
    "# Convert non-numeric values to NaN for each column in X\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "# Convert non-numeric values to NaN for each column in y\n",
    "for col in y.columns:\n",
    "    y[col] = pd.to_numeric(y[col], errors='coerce')\n",
    "\n",
    "# Fill NaN values with a specific value (e.g., 0) in X and y\n",
    "X.fillna(0, inplace=True)\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Convert X to a numpy array\n",
    "X = X.to_numpy()\n",
    "\n",
    "# Fill NaN values in y with 0\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Reshape the input data to be 3D for BI-LSTM input [samples, timesteps, features]\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "X_train = np.random.randn(100, 10, 1)  \n",
    "y_train = np.random.randint(0, 2, size=(100,)) \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False), input_shape=(10, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=245, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c844119-9326-4c61-b75b-2dc04ebb8f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for output parameter 0: 0.8928571428571429\n",
      "Precision for output parameter 1: 0.8928571428571429\n"
     ]
    }
   ],
   "source": [
    "# Reshape y_train and y_pred to have shape (n_samples, 2)\n",
    "y_train_reshaped = y_train.reshape(-1, 1)  # Assuming y_train is a 1D array with shape (n_samples,)\n",
    "y_pred_reshaped = y_pred.reshape(-1, 1)  # Assuming y_pred is a 1D array with shape (n_samples,)\n",
    "\n",
    "# Concatenate y_train_reshaped and y_pred_reshaped along the second axis to create a (n_samples, 2) array\n",
    "y_train_combined = np.concatenate((y_train_reshaped, y_pred_reshaped), axis=1)\n",
    "\n",
    "# Calculate precision for each output parameter separately\n",
    "precision_0 = precision_score(y_train_combined[:, 0], y_train_combined[:, 1])\n",
    "precision_1 = precision_score(y_train_combined[:, 0], y_train_combined[:, 1])\n",
    "\n",
    "print(\"Precision for output parameter 0:\", precision_0)\n",
    "print(\"Precision for output parameter 1:\", precision_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c40f4a89-981a-49f0-aff8-4f7e3409622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for output parameter 0: 0.9259259259259259\n",
      "Recall for output parameter 1: 0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Reshape y_train and y_pred to have shape (n_samples, 2)\n",
    "y_train_reshaped = y_train.reshape(-1, 1)  # Assuming y_train is a 1D array with shape (n_samples,)\n",
    "y_pred_reshaped = y_pred.reshape(-1, 1)  # Assuming y_pred is a 1D array with shape (n_samples,)\n",
    "\n",
    "# Concatenate y_train_reshaped and y_pred_reshaped along the second axis to create a (n_samples, 2) array\n",
    "y_train_combined = np.concatenate((y_train_reshaped, y_pred_reshaped), axis=1)\n",
    "\n",
    "# Calculate recall for each output parameter separately\n",
    "recall_0 = recall_score(y_train_combined[:, 0], y_train_combined[:, 1])\n",
    "recall_1 = recall_score(y_train_combined[:, 0], y_train_combined[:, 1])\n",
    "\n",
    "print(\"Recall for output parameter 0:\", recall_0)\n",
    "print(\"Recall for output parameter 1:\", recall_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63fd4695-510f-4726-b036-57679b520fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for output parameter 0: 0.9090909090909091\n",
      "F1 score for output parameter 1: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Reshape y_train and y_pred to have shape (n_samples, 2)\n",
    "y_train_reshaped = y_train.reshape(-1, 1)  # Assuming y_train is a 1D array with shape (n_samples,)\n",
    "y_pred_reshaped = y_pred.reshape(-1, 1)  # Assuming y_pred is a 1D array with shape (n_samples,)\n",
    "\n",
    "# Concatenate y_train_reshaped and y_pred_reshaped along the second axis to create a (n_samples, 2) array\n",
    "y_train_combined = np.concatenate((y_train_reshaped, y_pred_reshaped), axis=1)\n",
    "\n",
    "# Calculate F1 score for each output parameter separately\n",
    "f1_score_0 = f1_score(y_train_combined[:, 0], y_train_combined[:, 1])\n",
    "f1_score_1 = f1_score(y_train_combined[:, 0], y_train_combined[:, 1])\n",
    "\n",
    "print(\"F1 score for output parameter 0:\", f1_score_0)\n",
    "print(\"F1 score for output parameter 1:\", f1_score_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17b45a86-0d41-4d46-9268-35c7b6597fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for output parameter 0: 0.14\n",
      "Mean Absolute Error for output parameter 1: 0.06\n"
     ]
    }
   ],
   "source": [
    "# Reshape y_train and y_pred to have shape (n_samples, 2)\n",
    "y_train_reshaped = y_train.reshape(-1, 2)  # Reshape y_train to have shape (n_samples, 2)\n",
    "y_pred_reshaped = y_pred.reshape(-1, 2)  # Reshape y_pred to have shape (n_samples, 2)\n",
    "\n",
    "# Calculate MAE for each output parameter separately\n",
    "mae_0 = mean_absolute_error(y_train_reshaped[:, 0], y_pred_reshaped[:, 0])\n",
    "mae_1 = mean_absolute_error(y_train_reshaped[:, 1], y_pred_reshaped[:, 1])\n",
    "\n",
    "print(\"Mean Absolute Error for output parameter 0:\", mae_0)\n",
    "print(\"Mean Absolute Error for output parameter 1:\", mae_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d62c0fc8-f859-43a0-aa9d-a27631c92393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for output parameter 0: 0.1\n",
      "Mean Squared Error for output parameter 1: 0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Reshape y_train and y_pred to have shape (n_samples, 2)\n",
    "y_train_reshaped = y_train.reshape(-1, 1)  # Assuming y_train is a 1D array with shape (n_samples,)\n",
    "y_pred_reshaped = y_pred.reshape(-1, 1)  # Assuming y_pred is a 1D array with shape (n_samples,)\n",
    "\n",
    "# Concatenate y_train_reshaped and y_pred_reshaped along the second axis to create a (n_samples, 2) array\n",
    "y_train_combined = np.concatenate((y_train_reshaped, y_pred_reshaped), axis=1)\n",
    "\n",
    "# Calculate MSE for each output parameter separately\n",
    "mse_0 = mean_squared_error(y_train_combined[:, 0], y_train_combined[:, 1])\n",
    "mse_1 = mean_squared_error(y_train_combined[:, 0], y_train_combined[:, 1])\n",
    "\n",
    "print(\"Mean Squared Error for output parameter 0:\", mse_0)\n",
    "print(\"Mean Squared Error for output parameter 1:\", mse_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "822aee30-7b12-49a2-94af-2d9f4d8d9353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 0s 42ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 2.1773 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 2.1702 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 2.1857 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 2.1929 - val_accuracy: 0.5000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 2.1670 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 2.1421 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 2.1640 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 2.2029 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 2.1915 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 2.1668 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5HklEQVR4nO3deVxVdf7H8fcFWRVwAVkcEixzy8RcCG0qi0IrSsdyLRW3qXCLaUpzr0lalRosc34u0+SWltYvSzNsMtPSNLdxKdPcAckExQS89/z+6OedbqB59cKBw+v5eJwH3O/9nnM+h1ve9+N7vuccm2EYhgAAACzCy+wCAAAAPIlwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwA8BjbDabJk2a5PZ6P/zwg2w2m+bOnevxmgBUP4QbwGLmzp0rm80mm82mtWvXlnrfMAxFR0fLZrPpnnvuMaFCAChfhBvAovz9/TV//vxS7Z999pkOHz4sPz8/E6oCgPJHuAEs6q677tLixYt17tw5l/b58+erTZs2ioiIMKmy6qOwsNDsEoBqiXADWFTv3r31448/atWqVc624uJiLVmyRH369ClzncLCQv3lL39RdHS0/Pz81KRJE7300ksyDMOlX1FRkR577DGFhYUpKChI9957rw4fPlzmNo8cOaKBAwcqPDxcfn5+atGihWbPnn1Zx3TixAk9/vjjatmypWrVqqXg4GB16dJFW7duLdX37NmzmjRpkq699lr5+/srMjJSf/rTn/T99987+zgcDr3yyitq2bKl/P39FRYWps6dO+vrr7+WdPG5QL+dXzRp0iTZbDbt3LlTffr0UZ06dXTTTTdJkrZt26YBAwaoUaNG8vf3V0REhAYOHKgff/yxzL/XoEGDFBUVJT8/P8XGxuqRRx5RcXGx9u3bJ5vNpmnTppVab926dbLZbFqwYIG7f1bAcmqYXQCA8hETE6OEhAQtWLBAXbp0kSR99NFHys/PV69evfTqq6+69DcMQ/fee68+/fRTDRo0SHFxcVq5cqX++te/6siRIy5fqIMHD9Zbb72lPn36qEOHDlq9erXuvvvuUjXk5OToxhtvlM1m07BhwxQWFqaPPvpIgwYNUkFBgUaNGuXWMe3bt0/Lli3TAw88oNjYWOXk5OiNN97QLbfcop07dyoqKkqSZLfbdc899ygrK0u9evXSyJEjderUKa1atUo7duzQ1VdfLUkaNGiQ5s6dqy5dumjw4ME6d+6cPv/8c3355Zdq27atW7Wd98ADD6hx48aaMmWKMxSuWrVK+/btU0pKiiIiIvSf//xHM2fO1H/+8x99+eWXstlskqSjR4+qffv2OnnypIYOHaqmTZvqyJEjWrJkic6cOaNGjRqpY8eOmjdvnh577DGX/c6bN09BQUG67777LqtuwFIMAJYyZ84cQ5KxceNGIzMz0wgKCjLOnDljGIZhPPDAA0anTp0MwzCMhg0bGnfffbdzvWXLlhmSjL/97W8u27v//vsNm81m7N271zAMw9iyZYshyXj00Udd+vXp08eQZEycONHZNmjQICMyMtLIy8tz6durVy8jJCTEWdf+/fsNScacOXMuemxnz5417Ha7S9v+/fsNPz8/4+mnn3a2zZ4925BkTJ06tdQ2HA6HYRiGsXr1akOSMWLEiAv2uVhdvz3WiRMnGpKM3r17l+p7/jh/bcGCBYYkY82aNc62fv36GV5eXsbGjRsvWNMbb7xhSDJ27drlfK+4uNgIDQ01+vfvX2o9oDritBRgYT169NDPP/+sDz74QKdOndIHH3xwwVNSH374oby9vTVixAiX9r/85S8yDEMfffSRs5+kUv1+OwpjGIbeeecdJScnyzAM5eXlOZekpCTl5+dr8+bNbh2Pn5+fvLx++WfLbrfrxx9/VK1atdSkSROXbb3zzjsKDQ3V8OHDS23j/CjJO++8I5vNpokTJ16wz+V4+OGHS7UFBAQ4fz979qzy8vJ04403SpKzbofDoWXLlik5ObnMUaPzNfXo0UP+/v6aN2+e872VK1cqLy9PDz744GXXDVgJ4QawsLCwMCUmJmr+/Pl69913Zbfbdf/995fZ98CBA4qKilJQUJBLe7NmzZzvn//p5eXlPLVzXpMmTVxeHz9+XCdPntTMmTMVFhbmsqSkpEiScnNz3Toeh8OhadOmqXHjxvLz81NoaKjCwsK0bds25efnO/t9//33atKkiWrUuPCZ9++//15RUVGqW7euWzX8ntjY2FJtJ06c0MiRIxUeHq6AgACFhYU5+52v+/jx4yooKNB111130e3Xrl1bycnJLlfCzZs3Tw0aNNBtt93mwSMBqi7m3AAW16dPHw0ZMkTZ2dnq0qWLateuXSH7dTgckqQHH3xQ/fv3L7PP9ddf79Y2p0yZovHjx2vgwIF65plnVLduXXl5eWnUqFHO/XnShUZw7Hb7Bdf59SjNeT169NC6dev017/+VXFxcapVq5YcDoc6d+58WXX369dPixcv1rp169SyZUu9//77evTRR52jWkB1R7gBLK5bt27685//rC+//FKLFi26YL+GDRvqk08+0alTp1xGb3bv3u18//xPh8PhHB05b8+ePS7bO38lld1uV2JiokeOZcmSJerUqZNmzZrl0n7y5EmFhoY6X1999dX66quvVFJSIh8fnzK3dfXVV2vlypU6ceLEBUdv6tSp49z+r50fxboUP/30k7KysjR58mRNmDDB2f7dd9+59AsLC1NwcLB27Njxu9vs3LmzwsLCNG/ePMXHx+vMmTN66KGHLrkmwOqI+YDF1apVS6+//romTZqk5OTkC/a76667ZLfblZmZ6dI+bdo02Ww25xVX53/+9mqrjIwMl9fe3t7q3r273nnnnTK/sI8fP+72sXh7e5e6LH3x4sU6cuSIS1v37t2Vl5dX6lgkOdfv3r27DMPQ5MmTL9gnODhYoaGhWrNmjcv7r732mls1/3qb5/327+Xl5aWuXbvqf//3f52XopdVkyTVqFFDvXv31ttvv625c+eqZcuWbo+CAVbGyA1QDVzotNCvJScnq1OnTho7dqx++OEHtWrVSh9//LHee+89jRo1yjnHJi4uTr1799Zrr72m/Px8dejQQVlZWdq7d2+pbT733HP69NNPFR8fryFDhqh58+Y6ceKENm/erE8++UQnTpxw6zjuuecePf3000pJSVGHDh20fft2zZs3T40aNXLp169fP7355ptKS0vThg0b9Mc//lGFhYX65JNP9Oijj+q+++5Tp06d9NBDD+nVV1/Vd9995zxF9Pnnn6tTp04aNmyYpF8ue3/uuec0ePBgtW3bVmvWrNG33357yTUHBwfr5ptv1gsvvKCSkhI1aNBAH3/8sfbv31+q75QpU/Txxx/rlltu0dChQ9WsWTMdO3ZMixcv1tq1a11OKfbr10+vvvqqPv30Uz3//PNu/R0ByzPtOi0A5eLXl4JfzG8vBTcMwzh16pTx2GOPGVFRUYaPj4/RuHFj48UXX3Rehnzezz//bIwYMcKoV6+eUbNmTSM5Odk4dOhQqcujDcMwcnJyjNTUVCM6Otrw8fExIiIijNtvv92YOXOms487l4L/5S9/MSIjI42AgACjY8eOxvr1641bbrnFuOWWW1z6njlzxhg7dqwRGxvr3O/9999vfP/9984+586dM1588UWjadOmhq+vrxEWFmZ06dLF2LRpk8t2Bg0aZISEhBhBQUFGjx49jNzc3AteCn78+PFSdR8+fNjo1q2bUbt2bSMkJMR44IEHjKNHj5b59zpw4IDRr18/IywszPDz8zMaNWpkpKamGkVFRaW226JFC8PLy8s4fPjwRf9uQHVjM4zfjJUCAKqE1q1bq27dusrKyjK7FKBSYc4NAFRBX3/9tbZs2aJ+/fqZXQpQ6TByAwBVyI4dO7Rp0ya9/PLLysvL0759++Tv7292WUClwsgNAFQhS5YsUUpKikpKSrRgwQKCDVAGRm4AAIClMHIDAAAshXADAAAspdrdxM/hcOjo0aMKCgq6oif/AgCAimMYhk6dOqWoqKjffY5atQs3R48eVXR0tNllAACAy3Do0CH94Q9/uGifahduzj8Q8NChQwoODja5GgAAcCkKCgoUHR3t8mDfC6l24eb8qajg4GDCDQAAVcylTClhQjEAALAUwg0AALAUwg0AALCUajfn5lLZ7XaVlJSYXQY8wMfHR97e3maXAQCoIISb3zAMQ9nZ2Tp58qTZpcCDateurYiICO5tBADVAOHmN84Hm/r16yswMJAvwyrOMAydOXNGubm5kqTIyEiTKwIAlDfCza/Y7XZnsKlXr57Z5cBDAgICJEm5ubmqX78+p6gAwOKYUPwr5+fYBAYGmlwJPO38Z8o8KgCwPsJNGTgVZT18pgBQfRBuAACApZgabtasWaPk5GRFRUXJZrNp2bJlv7vOv//9b91www3y8/PTNddco7lz55Z7ndVVTEyMMjIyzC4DAAC3mBpuCgsL1apVK02fPv2S+u/fv1933323OnXqpC1btmjUqFEaPHiwVq5cWc6VVm42m+2iy6RJky5ruxs3btTQoUM9WywAAOXM1KulunTpoi5dulxy/xkzZig2NlYvv/yyJKlZs2Zau3atpk2bpqSkpPIq85IYhiGHYc6+Dx856vz97bcXadLEidq5a7ezrVatWrL/f3GGYchut6tGjd//6OvWC5Uk57pVmd1hyGEY+rn4nBxe58wuBwAsL8DH27T5jlXqUvD169crMTHRpS0pKUmjRo264DpFRUUqKipyvi4oKCiX2hyG9J+j+eWy7d8X4PzttMNXhqQfHb+0bVy/VoN7JGv6m28r88Vn9d3unZox711FRDbQS0+P1bZvvtbPZ86o0TXXasToCbrxj7c6t9Ul4Xr1HfSIHhz8iCSpVXQdTXzhFa3J+ljrP1ut+hGR+sv4Z3TrnXdV5MFeFuNcsXJPntXQpWt15JTd7HIAwPJ2Pp2kQF9zYkaVmlCcnZ2t8PBwl7bw8HAVFBTo559/LnOd9PR0hYSEOJfo6Gi39mkYhs4Un7uk5WyJ3aOLYXhuxOSV9MkaOXqilq3+Stc2baEzZ07rptvu0MwFy7RoxWfqcOvtGpHSW8eOHLrodmZMe15J93TV4o/X6qbb7tCYEX9W/k8/eaxOAACuVJUaubkcY8aMUVpamvN1QUGBWwHn5xK7mk8wZ07P9kl3up16v64dIC+bTS2iQiRJx+vVlCQ99+zfdO999/23Y4sY/SnxJufLuzreoHVZH2nPhn8rMXWYJMnH20sRIf7ObUnSoIEpevzRQZKkm+Ne0vzZb6jg0G51aNH5so6xopw9e1Y1zvjrg+E3yc/f3+xyAMDyAnzMu2FqlQo3ERERysnJcWnLyclRcHCw8y60v+Xn5yc/P7+KKM/jvL1s8vZy73yl1//39/7Nz/bt27ls6/Tp05o0aZKWL1+uY8eO6dy5c/r55591+NAhl35eNtca4lq1cr4ODqql4OBg/Zh33O06K5q3l01eNpsCfGvI36RhUgBAxahS/8onJCToww8/dGlbtWqVEhISym2fAT7e2vm0OZOVPZl6a9as6fL68ccf16pVq/TSSy/pmmuuUUBAgO6//34VFxdfdDs+Pj4ur202mxwOh8fqBADgSpkabk6fPq29e/c6X+/fv19btmxR3bp1ddVVV2nMmDE6cuSI3nzzTUnSww8/rMzMTD3xxBMaOHCgVq9erbffflvLly8vtxptNptpE6LK0xdffKEBAwaoW7dukn75LH744QdziwIAwANMnVD89ddfq3Xr1mrdurUkKS0tTa1bt9aECRMkSceOHdPBgwed/WNjY7V8+XKtWrVKrVq10ssvv6z/+Z//Mf0y8KqocePGevfdd7VlyxZt3bpVffr0YQQGAGAJpg5J3HrrrRe9Iqisuw/feuut+uabb8qxquph6tSpGjhwoDp06KDQ0FA9+eST5XaZPAAAFclmePJ64yqgoKBAISEhys/PV3BwsMt7Z8+e1f79+xUbGyt/rqixFD5bAKjaLvb9/VtV6j43AAAAv4dwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwA0m/PNZi1KhRztcxMTHKyMi46Do2m03Lli274n17ajsAAEiEG0tITk5W586dy3zv888/l81m07Zt29za5saNGzV06FBPlOc0adIkxcXFlWo/duyYunTp4tF9AQCqL8KNBQwaNEirVq3S4cOHS703Z84ctW3bVtdff71b2wwLC1NgYKCnSryoiIgI+fn5Vci+AADWR7ixgHvuuUdhYWGlnqJ++vRpLV68WF27dlXv3r3VoEEDBQYGqmXLllqwYMFFt/nb01Lfffedbr75Zvn7+6t58+ZatWpVqXWefPJJXXvttQoMDFSjRo00fvx4lZSUSPrlCe+TJ0/W1q1bZbPZZLPZnPX+9rTU9u3bddtttykgIED16tXT0KFDdfr0aef7AwYMUNeuXfXSSy8pMjJS9erVU2pqqnNfAIDqrYbZBVR6hiGVnDFn3z6Bks32u91q1Kihfv36ae7cuRo7dqxs/7/O4sWLZbfb9eCDD2rx4sV68sknFRwcrOXLl+uhhx7S1Vdfrfbt2//u9h0Oh/70pz8pPDxcX331lfLz813m55wXFBSkuXPnKioqStu3b9eQIUMUFBSkJ554Qj179tSOHTu0YsUKffLJJ5KkkJCQUtsoLCxUUlKSEhIStHHjRuXm5mrw4MEaNmyYS3j79NNPFRkZqU8//VR79+5Vz549FRcXpyFDhvzu8QAArI1w83tKzkhToszZ91NHJd+al9R14MCBevHFF/XZZ5/p1ltvlfTLKanu3burYcOGevzxx519hw8frpUrV+rtt9++pHDzySefaPfu3Vq5cqWion75W0yZMqXUPJlx48Y5f4+JidHjjz+uhQsX6oknnlBAQIBq1aqlGjVqKCIi4oL7mj9/vs6ePas333xTNWv+cuyZmZlKTk7W888/r/DwcElSnTp1lJmZKW9vbzVt2lR33323srKyCDcAAE5LWUXTpk3VoUMHzZ49W5K0d+9eff755xo0aJDsdrueeeYZtWzZUnXr1lWtWrW0cuVKHTx48JK2vWvXLkVHRzuDjSQlJCSU6rdo0SJ17NhRERERqlWrlsaNG3fJ+/j1vlq1auUMNpLUsWNHORwO7dmzx9nWokULeXt7O19HRkYqNzfXrX0BAKyJkZvf4xP4ywiKWft2w6BBgzR8+HBNnz5dc+bM0dVXX61bbrlFzz//vF555RVlZGSoZcuWqlmzpkaNGqXi4mKPlbp+/Xr17dtXkydPVlJSkkJCQrRw4UK9/PLLHtvHr/n4+Li8ttlscjgc5bIvAEDVQrj5PTbbJZ8aMluPHj00cuRIzZ8/X2+++aYeeeQR2Ww2ffHFF7rvvvv04IMPSvplDs23336r5s2bX9J2mzVrpkOHDunYsWOKjIyUJH355ZcufdatW6eGDRtq7NixzrYDBw649PH19ZXdbv/dfc2dO1eFhYXO0ZsvvvhCXl5eatKkySXVCwCo3jgtZSG1atVSz549NWbMGB07dkwDBgyQJDVu3FirVq3SunXrtGvXLv35z39WTk7OJW83MTFR1157rfr376+tW7fq888/dwkx5/dx8OBBLVy4UN9//71effVVLV261KVPTEyM9u/fry1btigvL09FRUWl9tW3b1/5+/urf//+2rFjhz799FMNHz5cDz30kHO+DQAAF0O4sZhBgwbpp59+UlJSknOOzLhx43TDDTcoKSlJt956qyIiItS1a9dL3qaXl5eWLl2qn3/+We3bt9fgwYP17LPPuvS599579dhjj2nYsGGKi4vTunXrNH78eJc+3bt3V+fOndWpUyeFhYWVeTl6YGCgVq5cqRMnTqhdu3a6//77dfvttyszM9P9PwYAoFqyGYZhmF1ERSooKFBISIjy8/MVHBzs8t7Zs2e1f/9+xcbGyt/f36QKUR74bAGgarvY9/dvMXIDAAAshXADAAAshXADAAAshXADAAAshXBThmo2x7pa4DMFgOqDcPMr5+96e+aMSQ/KRLk5/5n+9s7GAADr4Q7Fv+Lt7a3atWs7n1EUGBjofMI2qibDMHTmzBnl5uaqdu3aLs+jAgBYE+HmN84/sZqHMFpL7dq1L/o0cgCAdRBufsNmsykyMlL169dXSUmJ2eXAA3x8fBixAYBqhHBzAd7e3nwhAgBQBTGhGAAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWArhBgAAWIrp4Wb69OmKiYmRv7+/4uPjtWHDhgv2LSkp0dNPP62rr75a/v7+atWqlVasWFGB1QIAgMrO1HCzaNEipaWlaeLEidq8ebNatWqlpKQk5ebmltl/3LhxeuONN/T3v/9dO3fu1MMPP6xu3brpm2++qeDKAQBAZWUzDMMwa+fx8fFq166dMjMzJUkOh0PR0dEaPny4Ro8eXap/VFSUxo4dq9TUVGdb9+7dFRAQoLfeeuuS9llQUKCQkBDl5+crODjYMwcCAADKlTvf36aN3BQXF2vTpk1KTEz8bzFeXkpMTNT69evLXKeoqEj+/v4ubQEBAVq7du0F91NUVKSCggKXBQAAWJdp4SYvL092u13h4eEu7eHh4crOzi5znaSkJE2dOlXfffedHA6HVq1apXfffVfHjh274H7S09MVEhLiXKKjoz16HAAAoHIxfUKxO1555RU1btxYTZs2la+vr4YNG6aUlBR5eV34MMaMGaP8/HzncujQoQqsGAAAVDTTwk1oaKi8vb2Vk5Pj0p6Tk6OIiIgy1wkLC9OyZctUWFioAwcOaPfu3apVq5YaNWp0wf34+fkpODjYZQEAANZlWrjx9fVVmzZtlJWV5WxzOBzKyspSQkLCRdf19/dXgwYNdO7cOb3zzju67777yrtcAABQRdQwc+dpaWnq37+/2rZtq/bt2ysjI0OFhYVKSUmRJPXr108NGjRQenq6JOmrr77SkSNHFBcXpyNHjmjSpElyOBx64oknzDwMAABQiZgabnr27Knjx49rwoQJys7OVlxcnFasWOGcZHzw4EGX+TRnz57VuHHjtG/fPtWqVUt33XWX/vWvf6l27domHQEAAKhsTL3PjRm4zw0AAFVPlbjPDQAAQHkg3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsh3AAAAEsxPdxMnz5dMTEx8vf3V3x8vDZs2HDR/hkZGWrSpIkCAgIUHR2txx57TGfPnq2gagEAQGVnarhZtGiR0tLSNHHiRG3evFmtWrVSUlKScnNzy+w/f/58jR49WhMnTtSuXbs0a9YsLVq0SE899VQFVw4AACorU8PN1KlTNWTIEKWkpKh58+aaMWOGAgMDNXv27DL7r1u3Th07dlSfPn0UExOjO++8U7179/7d0R4AAFB9mBZuiouLtWnTJiUmJv63GC8vJSYmav369WWu06FDB23atMkZZvbt26cPP/xQd9111wX3U1RUpIKCApcFAABYVw2zdpyXlye73a7w8HCX9vDwcO3evbvMdfr06aO8vDzddNNNMgxD586d08MPP3zR01Lp6emaPHmyR2sHAACVl+kTit3x73//W1OmTNFrr72mzZs3691339Xy5cv1zDPPXHCdMWPGKD8/37kcOnSoAisGAAAVzbSRm9DQUHl7eysnJ8elPScnRxEREWWuM378eD300EMaPHiwJKlly5YqLCzU0KFDNXbsWHl5lc5qfn5+8vPz8/wBAACASsm0kRtfX1+1adNGWVlZzjaHw6GsrCwlJCSUuc6ZM2dKBRhvb29JkmEY5VcsAACoMkwbuZGktLQ09e/fX23btlX79u2VkZGhwsJCpaSkSJL69eunBg0aKD09XZKUnJysqVOnqnXr1oqPj9fevXs1fvx4JScnO0MOAACo3twONzExMRo4cKAGDBigq6666op23rNnTx0/flwTJkxQdna24uLitGLFCuck44MHD7qM1IwbN042m03jxo3TkSNHFBYWpuTkZD377LNXVAcAALAOm+Hm+ZyMjAzNnTtXO3bsUKdOnTRo0CB169atysxrKSgoUEhIiPLz8xUcHGx2OQAA4BK48/3t9pybUaNGacuWLdqwYYOaNWum4cOHKzIyUsOGDdPmzZsvu2gAAABPcHvk5rdKSkr02muv6cknn1RJSYlatmypESNGKCUlRTabzVN1egwjNwAAVD3ufH9f9oTikpISLV26VHPmzNGqVat04403atCgQTp8+LCeeuopffLJJ5o/f/7lbh4AAOCyuB1uNm/erDlz5mjBggXy8vJSv379NG3aNDVt2tTZp1u3bmrXrp1HCwUAALgUboebdu3a6Y477tDrr7+url27ysfHp1Sf2NhY9erVyyMFAgAAuMPtcLNv3z41bNjwon1q1qypOXPmXHZRAAAAl8vtq6Vyc3P11VdflWr/6quv9PXXX3ukKAAAgMvldrhJTU0t8+GTR44cUWpqqkeKAgAAuFxuh5udO3fqhhtuKNXeunVr7dy50yNFAQAAXC63w42fn1+pJ3lL0rFjx1SjhqmPqgIAAHA/3Nx5550aM2aM8vPznW0nT57UU089pTvuuMOjxQEAALjL7aGWl156STfffLMaNmyo1q1bS5K2bNmi8PBw/etf//J4gQAAAO5wO9w0aNBA27Zt07x587R161YFBAQoJSVFvXv3LvOeNwAAABXpsibJ1KxZU0OHDvV0LQAAAFfssmcA79y5UwcPHlRxcbFL+7333nvFRQEAAFyuy7pDcbdu3bR9+3bZbDadf6j4+SeA2+12z1YIAADgBrevlho5cqRiY2OVm5urwMBA/ec//9GaNWvUtm1b/fvf/y6HEgEAAC6d2yM369ev1+rVqxUaGiovLy95eXnppptuUnp6ukaMGKFvvvmmPOoEAAC4JG6P3NjtdgUFBUmSQkNDdfToUUlSw4YNtWfPHs9WBwAA4Ca3R26uu+46bd26VbGxsYqPj9cLL7wgX19fzZw5U40aNSqPGgEAAC6Z2+Fm3LhxKiwslCQ9/fTTuueee/THP/5R9erV06JFizxeIAAAgDtsxvnLna7AiRMnVKdOHecVU5VZQUGBQkJClJ+fr+DgYLPLAQAAl8Cd72+35tyUlJSoRo0a2rFjh0t73bp1q0SwAQAA1udWuPHx8dFVV13FvWwAAECl5fbVUmPHjtVTTz2lEydOlEc9AAAAV8TtCcWZmZnau3evoqKi1LBhQ9WsWdPl/c2bN3usOAAAAHe5HW66du1aDmUAAAB4hkeulqpKuFoKAICqp9yulgIAAKjs3D4t5eXlddHLvrmSCgAAmMntcLN06VKX1yUlJfrmm2/0z3/+U5MnT/ZYYQAAAJfDY3Nu5s+fr0WLFum9997zxObKDXNuAACoekyZc3PjjTcqKyvLU5sDAAC4LB4JNz///LNeffVVNWjQwBObAwAAuGxuz7n57QMyDcPQqVOnFBgYqLfeesujxQEAALjL7XAzbdo0l3Dj5eWlsLAwxcfHq06dOh4tDgAAwF1uh5sBAwaUQxkAAACe4facmzlz5mjx4sWl2hcvXqx//vOfHikKAADgcrkdbtLT0xUaGlqqvX79+poyZYpHigIAALhcboebgwcPKjY2tlR7w4YNdfDgQY8UBQAAcLncDjf169fXtm3bSrVv3bpV9erV80hRAAAAl8vtcNO7d2+NGDFCn376qex2u+x2u1avXq2RI0eqV69e5VEjAADAJXP7aqlnnnlGP/zwg26//XbVqPHL6g6HQ/369WPODQAAMN1lP1vqu+++05YtWxQQEKCWLVuqYcOGnq6tXPBsKQAAqh53vr/dHrk5r3HjxmrcuPHlrg4AAFAu3J5z0717dz3//POl2l944QU98MADHikKAADgcrkdbtasWaO77rqrVHuXLl20Zs0ajxQFAABwudwON6dPn5avr2+pdh8fHxUUFHikKAAAgMvldrhp2bKlFi1aVKp94cKFat68uUeKAgAAuFxuTygeP368/vSnP+n777/XbbfdJknKysrS/PnztWTJEo8XCAAA4A63w01ycrKWLVumKVOmaMmSJQoICFCrVq20evVq1a1btzxqBAAAuGSXfZ+b8woKCrRgwQLNmjVLmzZtkt1u91Rt5YL73AAAUPW48/3t9pyb89asWaP+/fsrKipKL7/8sm677TZ9+eWXl7s5AAAAj3DrtFR2drbmzp2rWbNmqaCgQD169FBRUZGWLVvGZGIAAFApXPLITXJyspo0aaJt27YpIyNDR48e1d///vfyrA0AAMBtlzxy89FHH2nEiBF65JFHeOwCAACotC555Gbt2rU6deqU2rRpo/j4eGVmZiovL88jRUyfPl0xMTHy9/dXfHy8NmzYcMG+t956q2w2W6nl7rvv9kgtAACgarvkcHPjjTfqH//4h44dO6Y///nPWrhwoaKiouRwOLRq1SqdOnXqsgpYtGiR0tLSNHHiRG3evFmtWrVSUlKScnNzy+z/7rvv6tixY85lx44d8vb25rlWAABA0hVeCr5nzx7NmjVL//rXv3Ty5Endcccdev/9993aRnx8vNq1a6fMzExJksPhUHR0tIYPH67Ro0f/7voZGRmaMGGCjh07ppo1a/5ufy4FBwCg6qmQS8ElqUmTJnrhhRd0+PBhLViwwO31i4uLtWnTJiUmJv63IC8vJSYmav369Ze0jVmzZqlXr16XFGwAAID1uX2H4rJ4e3ura9eu6tq1q1vr5eXlyW63Kzw83KU9PDxcu3fv/t31N2zYoB07dmjWrFkX7FNUVKSioiLnax7uCQCAtV3RyI3ZZs2apZYtW6p9+/YX7JOenq6QkBDnEh0dXYEVAgCAimZquAkNDZW3t7dycnJc2nNychQREXHRdQsLC7Vw4UINGjToov3GjBmj/Px853Lo0KErrhsAAFRepoYbX19ftWnTRllZWc42h8OhrKwsJSQkXHTdxYsXq6ioSA8++OBF+/n5+Sk4ONhlAQAA1uWROTdXIi0tTf3791fbtm3Vvn17ZWRkqLCwUCkpKZKkfv36qUGDBkpPT3dZb9asWeratavq1atnRtkAAKCSMj3c9OzZU8ePH9eECROUnZ2tuLg4rVixwjnJ+ODBg/Lych1g2rNnj9auXauPP/7YjJIBAEAldkX3uamKuM8NAABVT4Xd5wYAAKCyIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLIdwAAABLMT3cTJ8+XTExMfL391d8fLw2bNhw0f4nT55UamqqIiMj5efnp2uvvVYffvhhBVULAAAquxpm7nzRokVKS0vTjBkzFB8fr4yMDCUlJWnPnj2qX79+qf7FxcW64447VL9+fS1ZskQNGjTQgQMHVLt27YovHgAAVEo2wzAMs3YeHx+vdu3aKTMzU5LkcDgUHR2t4cOHa/To0aX6z5gxQy+++KJ2794tHx+fy9pnQUGBQkJClJ+fr+Dg4CuqHwAAVAx3vr9NOy1VXFysTZs2KTEx8b/FeHkpMTFR69evL3Od999/XwkJCUpNTVV4eLiuu+46TZkyRXa7vaLKBgAAlZxpp6Xy8vJkt9sVHh7u0h4eHq7du3eXuc6+ffu0evVq9e3bVx9++KH27t2rRx99VCUlJZo4cWKZ6xQVFamoqMj5uqCgwHMHAQAAKh3TJxS7w+FwqH79+po5c6batGmjnj17auzYsZoxY8YF10lPT1dISIhziY6OrsCKAQBARTMt3ISGhsrb21s5OTku7Tk5OYqIiChzncjISF177bXy9vZ2tjVr1kzZ2dkqLi4uc50xY8YoPz/fuRw6dMhzBwEAACod08KNr6+v2rRpo6ysLGebw+FQVlaWEhISylynY8eO2rt3rxwOh7Pt22+/VWRkpHx9fctcx8/PT8HBwS4LAACwLlNPS6Wlpekf//iH/vnPf2rXrl165JFHVFhYqJSUFElSv379NGbMGGf/Rx55RCdOnNDIkSP17bffavny5ZoyZYpSU1PNOgQAAFDJmHqfm549e+r48eOaMGGCsrOzFRcXpxUrVjgnGR88eFBeXv/NX9HR0Vq5cqUee+wxXX/99WrQoIFGjhypJ5980qxDAAAAlYyp97kxA/e5AQCg6qkS97kBAAAoD4QbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKYQbAABgKZUi3EyfPl0xMTHy9/dXfHy8NmzYcMG+c+fOlc1mc1n8/f0rsFoAAFCZmR5uFi1apLS0NE2cOFGbN29Wq1atlJSUpNzc3AuuExwcrGPHjjmXAwcOVGDFAACgMqthdgFTp07VkCFDlJKSIkmaMWOGli9frtmzZ2v06NFlrmOz2RQREVGRZf4+w5BKzphdBQAAlYNPoGSzmbJrU8NNcXGxNm3apDFjxjjbvLy8lJiYqPXr119wvdOnT6thw4ZyOBy64YYbNGXKFLVo0aLMvkVFRSoqKnK+Ligo8NwB/FrJGWlKVPlsGwCAquapo5JvTVN2beppqby8PNntdoWHh7u0h4eHKzs7u8x1mjRpotmzZ+u9997TW2+9JYfDoQ4dOujw4cNl9k9PT1dISIhziY6O9vhxAACAysP001LuSkhIUEJCgvN1hw4d1KxZM73xxht65plnSvUfM2aM0tLSnK8LCgrKJ+D4BP6SUgEAwC/fiyYxNdyEhobK29tbOTk5Lu05OTmXPKfGx8dHrVu31t69e8t838/PT35+fldc6++y2UwbfgMAAP9l6mkpX19ftWnTRllZWc42h8OhrKwsl9GZi7Hb7dq+fbsiIyPLq0wAAFCFmH5aKi0tTf3791fbtm3Vvn17ZWRkqLCw0Hn1VL9+/dSgQQOlp6dLkp5++mndeOONuuaaa3Ty5Em9+OKLOnDggAYPHmzmYQAAgErC9HDTs2dPHT9+XBMmTFB2drbi4uK0YsUK5yTjgwcPysvrvwNMP/30k4YMGaLs7GzVqVNHbdq00bp169S8eXOzDgEAAFQiNsMwDLOLqEgFBQUKCQlRfn6+goODzS4HAABcAne+v02/QzEAAIAnEW4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClEG4AAIClmP74hYp2/obMBQUFJlcCAAAu1fnv7Ut5sEK1CzenTp2SJEVHR5tcCQAAcNepU6cUEhJy0T7V7tlSDodDR48eVVBQkGw2m0e3XVBQoOjoaB06dIjnVlUCfB6VC59H5cLnUfnwmVycYRg6deqUoqKiXB6oXZZqN3Lj5eWlP/zhD+W6j+DgYP7DrET4PCoXPo/Khc+j8uEzubDfG7E5jwnFAADAUgg3AADAUgg3HuTn56eJEyfKz8/P7FIgPo/Khs+jcuHzqHz4TDyn2k0oBgAA1sbIDQAAsBTCDQAAsBTCDQAAsBTCDQAAsBTCjYdMnz5dMTEx8vf3V3x8vDZs2GB2SdVWenq62rVrp6CgINWvX19du3bVnj17zC4L/++5556TzWbTqFGjzC6l2jpy5IgefPBB1atXTwEBAWrZsqW+/vprs8uqlux2u8aPH6/Y2FgFBATo6quv1jPPPHNJz0/ChRFuPGDRokVKS0vTxIkTtXnzZrVq1UpJSUnKzc01u7Rq6bPPPlNqaqq+/PJLrVq1SiUlJbrzzjtVWFhodmnV3saNG/XGG2/o+uuvN7uUauunn35Sx44d5ePjo48++kg7d+7Uyy+/rDp16phdWrX0/PPP6/XXX1dmZqZ27dql559/Xi+88IL+/ve/m11alcal4B4QHx+vdu3aKTMzU9Ivz6+Kjo7W8OHDNXr0aJOrw/Hjx1W/fn199tlnuvnmm80up9o6ffq0brjhBr322mv629/+pri4OGVkZJhdVrUzevRoffHFF/r888/NLgWS7rnnHoWHh2vWrFnOtu7duysgIEBvvfWWiZVVbYzcXKHi4mJt2rRJiYmJzjYvLy8lJiZq/fr1JlaG8/Lz8yVJdevWNbmS6i01NVV33323y/8rqHjvv/++2rZtqwceeED169dX69at9Y9//MPssqqtDh06KCsrS99++60kaevWrVq7dq26dOlicmVVW7V7cKan5eXlyW63Kzw83KU9PDxcu3fvNqkqnOdwODRq1Ch17NhR1113ndnlVFsLFy7U5s2btXHjRrNLqfb27dun119/XWlpaXrqqae0ceNGjRgxQr6+vurfv7/Z5VU7o0ePVkFBgZo2bSpvb2/Z7XY9++yz6tu3r9mlVWmEG1haamqqduzYobVr15pdSrV16NAhjRw5UqtWrZK/v7/Z5VR7DodDbdu21ZQpUyRJrVu31o4dOzRjxgzCjQnefvttzZs3T/Pnz1eLFi20ZcsWjRo1SlFRUXweV4Bwc4VCQ0Pl7e2tnJwcl/acnBxFRESYVBUkadiwYfrggw+0Zs0a/eEPfzC7nGpr06ZNys3N1Q033OBss9vtWrNmjTIzM1VUVCRvb28TK6xeIiMj1bx5c5e2Zs2a6Z133jGpourtr3/9q0aPHq1evXpJklq2bKkDBw4oPT2dcHMFmHNzhXx9fdWmTRtlZWU52xwOh7KyspSQkGBiZdWXYRgaNmyYli5dqtWrVys2Ntbskqq122+/Xdu3b9eWLVucS9u2bdW3b19t2bKFYFPBOnbsWOrWCN9++60aNmxoUkXV25kzZ+Tl5fpV7O3tLYfDYVJF1sDIjQekpaWpf//+atu2rdq3b6+MjAwVFhYqJSXF7NKqpdTUVM2fP1/vvfeegoKClJ2dLUkKCQlRQECAydVVP0FBQaXmO9WsWVP16tVjHpQJHnvsMXXo0EFTpkxRjx49tGHDBs2cOVMzZ840u7RqKTk5Wc8++6yuuuoqtWjRQt98842mTp2qgQMHml1alcal4B6SmZmpF198UdnZ2YqLi9Orr76q+Ph4s8uqlmw2W5ntc+bM0YABAyq2GJTp1ltv5VJwE33wwQcaM2aMvvvuO8XGxiotLU1Dhgwxu6xq6dSpUxo/fryWLl2q3NxcRUVFqXfv3powYYJ8fX3NLq/KItwAAABLYc4NAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINgGrPZrNp2bJlZpcBwEMINwBMNWDAANlstlJL586dzS4NQBXFs6UAmK5z586aM2eOS5ufn59J1QCo6hi5AWA6Pz8/RUREuCx16tSR9Mspo9dff11dunRRQECAGjVqpCVLlrisv337dt12220KCAhQvXr1NHToUJ0+fdqlz+zZs9WiRQv5+fkpMjJSw4YNc3k/Ly9P3bp1U2BgoBo3bqz333+/fA8aQLkh3ACo9MaPH6/u3btr69at6tu3r3r16qVdu3ZJkgoLC5WUlKQ6depo48aNWrx4sT755BOX8PL6668rNTVVQ4cO1fbt2/X+++/rmmuucdnH5MmT1aNHD23btk133XWX+vbtqxMnTlTocQLwEAMATNS/f3/D29vbqFmzpsvy7LPPGoZhGJKMhx9+2GWd+Ph445FHHjEMwzBmzpxp1KlTxzh9+rTz/eXLlxteXl5Gdna2YRiGERUVZYwdO/aCNUgyxo0b53x9+vRpQ5Lx0Ucfeew4AVQc5twAMF2nTp30+uuvu7TVrVvX+XtCQoLLewkJCdqyZYskadeuXWrVqpVq1qzpfL9jx45yOBzas2ePbDabjh49qttvv/2iNVx//fXO32vWrKng4GDl5uZe7iEBMBHhBoDpatasWeo0kacEBARcUj8fHx+X1zabTQ6HozxKAlDOmHMDoNL78ssvS71u1qyZJKlZs2baunWrCgsLne9/8cUX8vLyUpMmTRQUFKSYmBhlZWVVaM0AzMPIDQDTFRUVKTs726WtRo0aCg0NlSQtXrxYbdu21U033aR58+Zpw4YNmjVrliSpb9++mjhxovr3769Jkybp+PHjGj58uB566CGFh4dLkiZNmqSHH35Y9evXV5cuXXTq1Cl98cUXGj58eMUeKIAKQbgBYLoVK1YoMjLSpa1JkybavXu3pF+uZFq4cKEeffRRRUZGasGCBWrevLkkKTAwUCtXrtTIkSPVrl07BQYGqnv37po6dapzW/3799fZs2c1bdo0Pf744woNDdX9999fcQcIoELZDMMwzC4CAC7EZrNp6dKl6tq1q9mlAKgimHMDAAAshXADAAAshTk3ACo1zpwDcBcjNwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFIINwAAwFL+D4lLpYLay8NEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train your model and capture the history object\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f6dbc12-8a4a-4026-ac85-e17d5179e436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAsYUlEQVR4nO3df1RU9b7/8deAAiMCmgKKkqjHq6YGKsrxRy4rbqTlSa/3aEmJ2NVroml484uJPzKVtA6XUsMfS83jj5uVP0+W5aGOpcdfgdoPRS3WUS4eEE4JgoHKzPePlnPXHNBAgYE+z8das5azZ+897y0Vz/bsmbHY7Xa7AAAADOLm6gEAAADqGgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjEMAAagTb7/9tiwWS6W3hIQEx3qffPKJnn32WXXv3l3u7u4KCQmp1vMUFxdr3rx56t69u7y9vdWiRQuFhYVp2rRpunjxYg0fFYCGqpGrBwBglgULFqh9+/ZOy7p37+7485YtW7R161b16tVLQUFB1dr39evXNWjQIGVmZiomJkZTp05VcXGxvv32W23ZskUjRoyo9j4B/DoRQADq1JAhQxQeHn7LxxcvXqw1a9aocePGevzxx/XNN99Ued87d+7U8ePHtXnzZo0ZM8bpsdLSUl27du2O566ukpISeXt719nzAageXgIDUK8EBQWpcePGd7Tt999/L0kaMGBAhce8vLzk6+vrtCwzM1OjRo2Sv7+/rFarOnfurNmzZzutc/z4cQ0ZMkS+vr5q2rSpHn74YR0+fNhpnZsv7+3fv1+TJ09WQECA2rZt63j8o48+0gMPPCBvb2/5+Pjoscce07fffntHxwigZnAGCECdKiwsVEFBgdOyli1b1si+27VrJ0n64x//qMTERFkslluu+9VXX+mBBx5Q48aNNXHiRIWEhOj777/Xn/70Jy1atEiS9O233+qBBx6Qr6+vZs6cqcaNG2vVqlUaPHiw9u/fr4iICKd9Tp48Wf7+/po7d65KSkokSRs3blRMTIyioqK0ZMkSXb16VampqRo4cKCOHz9e7WucANQQOwDUgfXr19slVXq7lccee8zerl27Kj/H1atX7Z07d7ZLsrdr184+btw4+9q1a+15eXkV1h00aJDdx8fHfv78eaflNpvN8efhw4fbPTw87N9//71j2cWLF+0+Pj72QYMGVTi2gQMH2m/cuOFYfuXKFXuzZs3sEyZMcHqO3Nxcu5+fX4XlAOoOZ4AA1KkVK1boX/7lX2pl31arVUeOHNGiRYv07rvv6u2339bbb78tNzc3TZ48Wa+//ro8PT2Vn5+vzz//XNOmTdO9997rtI+bZ43Ky8v1ySefaPjw4erQoYPj8datW2vMmDFas2aNioqKnF5WmzBhgtzd3R339+3bp8uXL+upp55yOuvl7u6uiIgIffbZZ7Xy9wDglxFAAOpU3759b3sR9N3y8/PT0qVLtXTpUp0/f15paWl6/fXXtXz5cvn5+WnhwoXKysqS5Pzus3+Wn5+vq1evqnPnzhUe69q1q2w2m7Kzs9WtWzfH8n9+d9u5c+ckSQ899FClz/HP1yQBqDsEEIBfrXbt2mn8+PEaMWKEOnTooM2bN2vhwoW19nxWq9Xpvs1mk/TzdUCtWrWqsH6jRvwnGHAV/u0D8KvXvHlzdezY0fGW+psvad3uLfb+/v5q0qSJzpw5U+GxzMxMubm5KTg4+LbP27FjR0lSQECAIiMj73R8ALWAt8ED+NU4efJkhXeYSdL58+d16tQpx8tZ/v7+GjRokNatW6cLFy44rWu32yX9fJ3OI488ol27dulvf/ub4/G8vDxt2bJFAwcO/MWXsKKiouTr66vFixfr+vXrFR7Pz8+v7iECqCGcAQJQr3z11VfavXu3JOm7775TYWGh42Wr0NBQDRs27Jbb7tu3T/PmzdPvfvc7/fa3v1XTpk2VlZWldevWqaysTPPnz3es++abb2rgwIHq1auXJk6cqPbt2+tvf/ub9uzZoxMnTkiSFi5cqH379mngwIGaPHmyGjVqpFWrVqmsrExLly79xWPx9fVVamqqnnnmGfXq1UtPPvmk/P39deHCBe3Zs0cDBgzQ8uXL7/wvC8AdI4AA1CsZGRmaM2eO07Kb92NiYm4bQCNHjtSVK1f0ySef6NNPP9UPP/yg5s2bq2/fvpoxY4YefPBBx7qhoaE6fPiw5syZo9TUVJWWlqpdu3YaNWqUY51u3brpiy++0KxZs5SUlCSbzaaIiAht2rSpwmcA3cqYMWMUFBSkV199Va+99prKysrUpk0bPfDAA4qNja3OXw2AGmSx3zzfCwAAYAiuAQIAAMYhgAAAgHEIIAAAYBwCCAAAGIcAAgAAxiGAAACAcfgcoErYbDZdvHhRPj4+jm+GBgAA9ZvdbteVK1cUFBQkN7fbn+MhgCpx8eLFX/yOHwAAUD9lZ2erbdu2t12HAKqEj4+PpJ//An/pu34AAED9UFRUpODgYMfv8dshgCpx82UvX19fAggAgAamKpevcBE0AAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjEMAAQAA4xBAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjEMAAQAA4xBAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA49SKAVqxYoZCQEHl5eSkiIkJHjx695brXr1/XggUL1LFjR3l5eSk0NFR79+695fqvvvqqLBaLpk+fXguTAwCAhsjlAbR161bFx8dr3rx5ysjIUGhoqKKionTp0qVK109MTNSqVau0bNkynTp1SpMmTdKIESN0/PjxCuseO3ZMq1at0v3331/bhwEAABoQlwdQcnKyJkyYoNjYWN13331auXKlmjRponXr1lW6/saNG/XSSy9p6NCh6tChg5577jkNHTpUf/jDH5zWKy4uVnR0tNasWaPmzZvXxaEAAIAGwqUBdO3aNaWnpysyMtKxzM3NTZGRkTp06FCl25SVlcnLy8tpmdVq1YEDB5yWxcXF6bHHHnPaNwAAgCQ1cuWTFxQUqLy8XIGBgU7LAwMDlZmZWek2UVFRSk5O1qBBg9SxY0elpaVp+/btKi8vd6zzzjvvKCMjQ8eOHavSHGVlZSorK3PcLyoquoOjAQAADYXLXwKrrjfeeEOdOnVSly5d5OHhoSlTpig2NlZubj8fSnZ2tqZNm6bNmzdXOFN0K0lJSfLz83PcgoODa/MQAACAi7k0gFq2bCl3d3fl5eU5Lc/Ly1OrVq0q3cbf3187d+5USUmJzp8/r8zMTDVt2lQdOnSQJKWnp+vSpUvq1auXGjVqpEaNGmn//v1688031ahRI6czRTfNmjVLhYWFjlt2dnbNHywAAKg3XBpAHh4e6t27t9LS0hzLbDab0tLS1K9fv9tu6+XlpTZt2ujGjRvatm2bnnjiCUnSww8/rK+//lonTpxw3MLDwxUdHa0TJ07I3d29wr48PT3l6+vrdAMAAL9eLr0GSJLi4+MVExOj8PBw9e3bVykpKSopKVFsbKwkaezYsWrTpo2SkpIkSUeOHFFOTo7CwsKUk5Oj+fPny2azaebMmZIkHx8fde/e3ek5vL291aJFiwrLAQCAmVweQKNHj1Z+fr7mzp2r3NxchYWFae/evY4Loy9cuOC4vkeSSktLlZiYqKysLDVt2lRDhw7Vxo0b1axZMxcdAQAAaGgsdrvd7uoh6puioiL5+fmpsLCQl8MAAGggqvP7u8G9CwwAAOBuEUAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjEMAAQAA4xBAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjEMAAQAA4xBAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjEMAAQAA4xBAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjEMAAQAA4xBAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjEMAAQAA4xBAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxTLwJoxYoVCgkJkZeXlyIiInT06NFbrnv9+nUtWLBAHTt2lJeXl0JDQ7V3716ndZKSktSnTx/5+PgoICBAw4cP15kzZ2r7MAAAQAPh8gDaunWr4uPjNW/ePGVkZCg0NFRRUVG6dOlSpesnJiZq1apVWrZsmU6dOqVJkyZpxIgROn78uGOd/fv3Ky4uTocPH9a+fft0/fp1PfLIIyopKamrwwIAAPWYxW632105QEREhPr06aPly5dLkmw2m4KDgzV16lQlJCRUWD8oKEizZ89WXFycY9nIkSNltVq1adOmSp8jPz9fAQEB2r9/vwYNGvSLMxUVFcnPz0+FhYXy9fW9wyMDAAB1qTq/v116BujatWtKT09XZGSkY5mbm5siIyN16NChSrcpKyuTl5eX0zKr1aoDBw7c8nkKCwslSffcc88t91lUVOR0AwAAv14uDaCCggKVl5crMDDQaXlgYKByc3Mr3SYqKkrJyck6d+6cbDab9u3bp+3bt+vvf/97pevbbDZNnz5dAwYMUPfu3StdJykpSX5+fo5bcHDw3R0YAACo11x+DVB1vfHGG+rUqZO6dOkiDw8PTZkyRbGxsXJzq/xQ4uLi9M033+idd9655T5nzZqlwsJCxy07O7u2xgcAAPWASwOoZcuWcnd3V15entPyvLw8tWrVqtJt/P39tXPnTpWUlOj8+fPKzMxU06ZN1aFDhwrrTpkyRR988IE+++wztW3b9pZzeHp6ytfX1+kGAAB+vVwaQB4eHurdu7fS0tIcy2w2m9LS0tSvX7/bbuvl5aU2bdroxo0b2rZtm5544gnHY3a7XVOmTNGOHTv06aefqn379rV2DAAAoOFp5OoB4uPjFRMTo/DwcPXt21cpKSkqKSlRbGysJGns2LFq06aNkpKSJElHjhxRTk6OwsLClJOTo/nz58tms2nmzJmOfcbFxWnLli3atWuXfHx8HNcT+fn5yWq11v1BAgCAesXlATR69Gjl5+dr7ty5ys3NVVhYmPbu3eu4MPrChQtO1/eUlpYqMTFRWVlZatq0qYYOHaqNGzeqWbNmjnVSU1MlSYMHD3Z6rvXr12vcuHG1fUgAAKCec/nnANVHfA4QAAANT4P5HCAAAABXIIAAAIBxCCAAAGAcAggAABiHAAIAAMYhgAAAgHEIIAAAYBwCCAAAGIcAAgAAxiGAAACAcQggAABgHAIIAAAYhwACAADGIYAAAIBxCCAAAGAcAggAABiHAAIAAMYhgAAAgHEIIAAAYBwCCAAAGIcAAgAAxiGAAACAcQggAABgHAIIAAAYhwACAADGIYAAAIBxCCAAAGAcAggAABiHAAIAAMYhgAAAgHEIIAAAYBwCCAAAGIcAAgAAxiGAAACAcQggAABgHAIIAAAYhwACAADGIYAAAIBxCCAAAGCcOwqgGzdu6M9//rNWrVqlK1euSJIuXryo4uLiGh0OAACgNjSq7gbnz5/Xo48+qgsXLqisrEz/+q//Kh8fHy1ZskRlZWVauXJlbcwJAABQY6p9BmjatGkKDw/Xjz/+KKvV6lg+YsQIpaWl1ehwAAAAtaHaZ4C++OIL/fWvf5WHh4fT8pCQEOXk5NTYYAAAALWl2meAbDabysvLKyz/3//9X/n4+NTIUAAAALWp2gH0yCOPKCUlxXHfYrGouLhY8+bN09ChQ2tyNgAAgFphsdvt9upskJ2drUcffVR2u13nzp1TeHi4zp07p5YtW+rzzz9XQEBAbc1aZ4qKiuTn56fCwkL5+vq6ehwAAFAF1fn9Xe0Akn5+G/zWrVt18uRJFRcXq1evXoqOjna6KLohI4AAAGh4ai2Arl+/ri5duuiDDz5Q165d73rQ+ooAAgCg4anO7+9qXQPUuHFjlZaW3tVwAAAArlbti6Dj4uK0ZMkS3bhxozbmAQAAqHXV/hygY8eOKS0tTZ988ol69Oghb29vp8e3b99eY8MBAADUhmoHULNmzTRy5MjamAUAAKBOVDuA1q9fXxtzAAAA1JlqB9BN+fn5OnPmjCSpc+fO8vf3r7GhAAAAalO1L4IuKSnR+PHj1bp1aw0aNEiDBg1SUFCQnn32WV29erU2ZgQAAKhR1Q6g+Ph47d+/X3/60590+fJlXb58Wbt27dL+/fs1Y8aM2pgRAACgRlX7k6Bbtmyp999/X4MHD3Za/tlnn2nUqFHKz8+vyflcgg9CBACg4am1D0KUpKtXryowMLDC8oCAAF4CAwAADUK1A6hfv36aN2+e0ydC//TTT3r55ZfVr1+/Gh0OAACgNlT7XWBvvPGGoqKi1LZtW4WGhkqSTp48KS8vL3388cc1PiAAAEBNu6Nvg7969ao2b96szMxMSVLXrl35NngAAOBS1fn9fUefA9SkSRNNmDDhjoYDAABwtWpfA5SUlKR169ZVWL5u3TotWbKkRoYCAACoTdUOoFWrVqlLly4Vlnfr1k0rV66skaEAAABqU7UDKDc3V61bt66w3N/fX3//+9/vaIgVK1YoJCREXl5eioiI0NGjR2+57vXr17VgwQJ17NhRXl5eCg0N1d69e+9qnwAAwCzVDqDg4GAdPHiwwvKDBw8qKCio2gNs3bpV8fHxmjdvnjIyMhQaGqqoqChdunSp0vUTExO1atUqLVu2TKdOndKkSZM0YsQIHT9+/I73CQAAzFLtd4EtXbpUS5cu1WuvvaaHHnpIkpSWlqaZM2dqxowZmjVrVrUGiIiIUJ8+fbR8+XJJks1mU3BwsKZOnaqEhIQK6wcFBWn27NmKi4tzLBs5cqSsVqs2bdp0R/v8Z7X1LjC73a6frpfX2P4AAGjIrI3dZbFYamx/tfousBdffFH/+Mc/NHnyZF27dk2S5OXlpf/3//5ftePn2rVrSk9Pd9rOzc1NkZGROnToUKXblJWVycvLy2mZ1WrVgQMH7mqfZWVljvtFRUXVOo6q+ul6ue6by2clAQAgSacWRKmJxx29If2uVfslMIvFoiVLlig/P1+HDx/WyZMn9cMPP2ju3LnVfvKCggKVl5dX+GqNwMBA5ebmVrpNVFSUkpOTde7cOdlsNu3bt0/bt293XH90J/tMSkqSn5+f4xYcHFztYwEAAA3HHWdX06ZN1adPH50/f17ff/+9unTpIje3avdUtb3xxhuaMGGCunTpIovFoo4dOyo2NrbSt+ZX1axZsxQfH++4X1RUVCsRZG3srlMLomp8vwAANETWxu4ue+4qB9C6det0+fJlp1CYOHGi1q5dK0nq3LmzPv7442qFQ8uWLeXu7q68vDyn5Xl5eWrVqlWl2/j7+2vnzp0qLS3VP/7xDwUFBSkhIUEdOnS44316enrK09OzynPfKYvF4rJTfQAA4P9U+ZTN6tWr1bx5c8f9vXv3av369frjH/+oY8eOqVmzZnr55Zer9eQeHh7q3bu30tLSHMtsNpvS0tJ+8YtVvby81KZNG924cUPbtm3TE088cdf7BAAAZqjy6Yhz584pPDzccX/Xrl164oknFB0dLUlavHixYmNjqz1AfHy8YmJiFB4err59+yolJUUlJSWOfY0dO1Zt2rRRUlKSJOnIkSPKyclRWFiYcnJyNH/+fNlsNs2cObPK+wQAAGarcgD99NNPTm8p++tf/6pnn33Wcb9Dhw63vMj4dkaPHq38/HzNnTtXubm5CgsL0969ex0XMV+4cMHp2qLS0lIlJiYqKytLTZs21dChQ7Vx40Y1a9asyvsEAABmq/LnAHXt2lWLFi3Sv/3bv6mgoECtWrXSkSNH1Lt3b0nS0aNH9bvf/e6OIqi+4dvgAQBoeGrlc4BiYmIUFxenb7/9Vp9++qm6dOniiB/p5zNC3bt3v/OpAQAA6kiVA2jmzJm6evWqtm/frlatWum9995zevzgwYN66qmnanxAAACAmlbtr8IwAS+BAQDQ8FTn93ftf3IhAABAPUMAAQAA4xBAAADAOAQQAAAwDgEEAACMU2MBlJ2drfHjx9fU7gAAAGpNjQXQDz/8oA0bNtTU7gAAAGpNlT8Icffu3bd9PCsr666HAQAAqAtVDqDhw4fLYrHodp+baLFYamQoAACA2lTll8Bat26t7du3y2azVXrLyMiozTkBAABqTJUDqHfv3kpPT7/l4790dggAAKC+qPJLYC+++KJKSkpu+fhvfvMbffbZZzUyFAAAQG3iy1ArwZehAgDQ8NTKl6FmZWXxEhcAAPhVqHIAderUSfn5+Y77o0ePVl5eXq0MBQAAUJuqHED/fPbnww8/vO01QQAAAPUV3wUGAACMU+UAslgsFT7okA8+BAAADVGV3wZvt9s1btw4eXp6SpJKS0s1adIkeXt7O623ffv2mp0QAACghlU5gGJiYpzuP/300zU+DAAAQF2ocgCtX7++NucAAACoM1wEDQAAjEMAAQAA4xBAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjEMAAQAA4xBAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjEMAAQAA4xBAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjEMAAQAA4xBAAADAOC4PoBUrVigkJEReXl6KiIjQ0aNHb7t+SkqKOnfuLKvVquDgYL3wwgsqLS11PF5eXq45c+aoffv2slqt6tixo1555RXZ7fbaPhQAANBANHLlk2/dulXx8fFauXKlIiIilJKSoqioKJ05c0YBAQEV1t+yZYsSEhK0bt069e/fX2fPntW4ceNksViUnJwsSVqyZIlSU1O1YcMGdevWTV9++aViY2Pl5+en559/vq4PEQAA1EMWuwtPjURERKhPnz5avny5JMlmsyk4OFhTp05VQkJChfWnTJmi06dPKy0tzbFsxowZOnLkiA4cOCBJevzxxxUYGKi1a9c61hk5cqSsVqs2bdpUpbmKiork5+enwsJC+fr63s0hAgCAOlKd398uewns2rVrSk9PV2Rk5P8N4+amyMhIHTp0qNJt+vfvr/T0dMfLZFlZWfrwww81dOhQp3XS0tJ09uxZSdLJkyd14MABDRkypBaPBgAANCQuewmsoKBA5eXlCgwMdFoeGBiozMzMSrcZM2aMCgoKNHDgQNntdt24cUOTJk3SSy+95FgnISFBRUVF6tKli9zd3VVeXq5FixYpOjr6lrOUlZWprKzMcb+oqOgujw4AANRnLr8Iujr+8pe/aPHixXrrrbeUkZGh7du3a8+ePXrllVcc67z77rvavHmztmzZooyMDG3YsEGvv/66NmzYcMv9JiUlyc/Pz3ELDg6ui8MBAAAu4rJrgK5du6YmTZro/fff1/Dhwx3LY2JidPnyZe3atavCNg888IB++9vf6rXXXnMs27RpkyZOnKji4mK5ubkpODhYCQkJiouLc6yzcOFCbdq06ZZnlio7AxQcHMw1QAAANCAN4hogDw8P9e7d2+mCZpvNprS0NPXr16/Sba5evSo3N+eR3d3dJcnxNvdbrWOz2W45i6enp3x9fZ1uAADg18ulb4OPj49XTEyMwsPD1bdvX6WkpKikpESxsbGSpLFjx6pNmzZKSkqSJA0bNkzJycnq2bOnIiIi9N1332nOnDkaNmyYI4SGDRumRYsW6d5771W3bt10/PhxJScna/z48S47TgAAUL+4NIBGjx6t/Px8zZ07V7m5uQoLC9PevXsdF0ZfuHDB6WxOYmKiLBaLEhMTlZOTI39/f0fw3LRs2TLNmTNHkydP1qVLlxQUFKT//M//1Ny5c+v8+AAAQP3k0s8Bqq/4HCAAABqeBnENEAAAgKsQQAAAwDgEEAAAMA4BBAAAjEMAAQAA4xBAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjEMAAQAA4xBAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjEMAAQAA4xBAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjEMAAQAA4xBAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjEMAAQAA4xBAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDgEEAAAMA4BBAAAjOPyAFqxYoVCQkLk5eWliIgIHT169Lbrp6SkqHPnzrJarQoODtYLL7yg0tJSp3VycnL09NNPq0WLFrJarerRo4e+/PLL2jwMAADQgDRy5ZNv3bpV8fHxWrlypSIiIpSSkqKoqCidOXNGAQEBFdbfsmWLEhIStG7dOvXv319nz57VuHHjZLFYlJycLEn68ccfNWDAAD344IP66KOP5O/vr3Pnzql58+Z1fXgAAKCestjtdrurnjwiIkJ9+vTR8uXLJUk2m03BwcGaOnWqEhISKqw/ZcoUnT59WmlpaY5lM2bM0JEjR3TgwAFJUkJCgg4ePKgvvvjijucqKiqSn5+fCgsL5evre8f7AQAAdac6v79d9hLYtWvXlJ6ersjIyP8bxs1NkZGROnToUKXb9O/fX+np6Y6XybKysvThhx9q6NChjnV2796t8PBw/f73v1dAQIB69uypNWvW3HaWsrIyFRUVOd0AAMCvl8sCqKCgQOXl5QoMDHRaHhgYqNzc3Eq3GTNmjBYsWKCBAweqcePG6tixowYPHqyXXnrJsU5WVpZSU1PVqVMnffzxx3ruuef0/PPPa8OGDbecJSkpSX5+fo5bcHBwzRwkAACol1x+EXR1/OUvf9HixYv11ltvKSMjQ9u3b9eePXv0yiuvONax2Wzq1auXFi9erJ49e2rixImaMGGCVq5cecv9zpo1S4WFhY5bdnZ2XRwOAABwEZddBN2yZUu5u7srLy/PaXleXp5atWpV6TZz5szRM888o//4j/+QJPXo0UMlJSWaOHGiZs+eLTc3N7Vu3Vr33Xef03Zdu3bVtm3bbjmLp6enPD097/KIAABAQ+GyM0AeHh7q3bu30wXNNptNaWlp6tevX6XbXL16VW5uziO7u7tLkm5eyz1gwACdOXPGaZ2zZ8+qXbt2NTk+AABowFz6Nvj4+HjFxMQoPDxcffv2VUpKikpKShQbGytJGjt2rNq0aaOkpCRJ0rBhw5ScnKyePXsqIiJC3333nebMmaNhw4Y5QuiFF15Q//79tXjxYo0aNUpHjx7V6tWrtXr1apcdJwAAqF9cGkCjR49Wfn6+5s6dq9zcXIWFhWnv3r2OC6MvXLjgdMYnMTFRFotFiYmJysnJkb+/v4YNG6ZFixY51unTp4927NihWbNmacGCBWrfvr1SUlIUHR1d58cHAADqJ5d+DlB9xecAAQDQ8DSIzwECAABwFQIIAAAYhwACAADGIYAAAIBxCCAAAGAcAggAABiHAAIAAMYhgAAAgHEIIAAAYBwCCAAAGIcAAgAAxiGAAACAcQggAABgHAIIAAAYhwACAADGIYAAAIBxCCAAAGAcAggAABiHAAIAAMYhgAAAgHEIIAAAYBwCCAAAGIcAAgAAxiGAAACAcQggAABgHAIIAAAYhwACAADGIYAAAIBxGrl6gPrIbrdLkoqKilw8CQAAqKqbv7dv/h6/HQKoEleuXJEkBQcHu3gSAABQXVeuXJGfn99t17HYq5JJhrHZbLp48aJ8fHxksVhqdN9FRUUKDg5Wdna2fH19a3TfqD5+HvULP4/6hZ9H/cPP5PbsdruuXLmioKAgubnd/iofzgBVws3NTW3btq3V5/D19eUf3nqEn0f9ws+jfuHnUf/wM7m1XzrzcxMXQQMAAOMQQAAAwDgEUB3z9PTUvHnz5Onp6epRIH4e9Q0/j/qFn0f9w8+k5nARNAAAMA5ngAAAgHEIIAAAYBwCCAAAGIcAAgAAxiGA6tCKFSsUEhIiLy8vRURE6OjRo64eyVhJSUnq06ePfHx8FBAQoOHDh+vMmTOuHguSXn31VVksFk2fPt3VoxgtJydHTz/9tFq0aCGr1aoePXroyy+/dPVYRiovL9ecOXPUvn17Wa1WdezYUa+88kqVvu8Kt0YA1ZGtW7cqPj5e8+bNU0ZGhkJDQxUVFaVLly65ejQj7d+/X3FxcTp8+LD27dun69ev65FHHlFJSYmrRzPasWPHtGrVKt1///2uHsVoP/74owYMGKDGjRvro48+0qlTp/SHP/xBzZs3d/VoRlqyZIlSU1O1fPlynT59WkuWLNHSpUu1bNkyV4/WoPE2+DoSERGhPn36aPny5ZJ+/r6x4OBgTZ06VQkJCS6eDvn5+QoICND+/fs1aNAgV49jpOLiYvXq1UtvvfWWFi5cqLCwMKWkpLh6LCMlJCTo4MGD+uKLL1w9CiQ9/vjjCgwM1Nq1ax3LRo4cKavVqk2bNrlwsoaNM0B14Nq1a0pPT1dkZKRjmZubmyIjI3Xo0CEXToabCgsLJUn33HOPiycxV1xcnB577DGnf0/gGrt371Z4eLh+//vfKyAgQD179tSaNWtcPZax+vfvr7S0NJ09e1aSdPLkSR04cEBDhgxx8WQNG1+GWgcKCgpUXl6uwMBAp+WBgYHKzMx00VS4yWazafr06RowYIC6d+/u6nGM9M477ygjI0PHjh1z9SiQlJWVpdTUVMXHx+ull17SsWPH9Pzzz8vDw0MxMTGuHs84CQkJKioqUpcuXeTu7q7y8nItWrRI0dHRrh6tQSOAYLy4uDh98803OnDggKtHMVJ2dramTZumffv2ycvLy9XjQD//T0F4eLgWL14sSerZs6e++eYbrVy5kgBygXfffVebN2/Wli1b1K1bN504cULTp09XUFAQP4+7QADVgZYtW8rd3V15eXlOy/Py8tSqVSsXTQVJmjJlij744AN9/vnnatu2ravHMVJ6erouXbqkXr16OZaVl5fr888/1/Lly1VWViZ3d3cXTmie1q1b67777nNa1rVrV23bts1FE5ntxRdfVEJCgp588klJUo8ePXT+/HklJSURQHeBa4DqgIeHh3r37q20tDTHMpvNprS0NPXr18+Fk5nLbrdrypQp2rFjhz799FO1b9/e1SMZ6+GHH9bXX3+tEydOOG7h4eGKjo7WiRMniB8XGDBgQIWPhTh79qzatWvnoonMdvXqVbm5Of+6dnd3l81mc9FEvw6cAaoj8fHxiomJUXh4uPr27auUlBSVlJQoNjbW1aMZKS4uTlu2bNGuXbvk4+Oj3NxcSZKfn5+sVquLpzOLj49PhWuvvL291aJFC67JcpEXXnhB/fv31+LFizVq1CgdPXpUq1ev1urVq109mpGGDRumRYsW6d5771W3bt10/PhxJScna/z48a4erUHjbfB1aPny5XrttdeUm5ursLAwvfnmm4qIiHD1WEayWCyVLl+/fr3GjRtXt8OggsGDB/M2eBf74IMPNGvWLJ07d07t27dXfHy8JkyY4OqxjHTlyhXNmTNHO3bs0KVLlxQUFKSnnnpKc+fOlYeHh6vHa7AIIAAAYByuAQIAAMYhgAAAgHEIIAAAYBwCCAAAGIcAAgAAxiGAAACAcQggAABgHAIIAKrAYrFo586drh4DQA0hgADUe+PGjZPFYqlwe/TRR109GoAGiu8CA9AgPProo1q/fr3TMk9PTxdNA6Ch4wwQgAbB09NTrVq1cro1b95c0s8vT6WmpmrIkCGyWq3q0KGD3n//faftv/76az300EOyWq1q0aKFJk6cqOLiYqd11q1bp27dusnT01OtW7fWlClTnB4vKCjQiBEj1KRJE3Xq1Em7d++u3YMGUGsIIAC/CnPmzNHIkSN18uRJRUdH68knn9Tp06clSSUlJYqKilLz5s117Ngxvffee/rzn//sFDipqamKi4vTxIkT9fXXX2v37t36zW9+4/QcL7/8skaNGqWvvvpKQ4cOVXR0tH744Yc6PU4ANcQOAPVcTEyM3d3d3e7t7e10W7Rokd1ut9sl2SdNmuS0TUREhP25556z2+12++rVq+3Nmze3FxcXOx7fs2eP3c3NzZ6bm2u32+32oKAg++zZs285gyR7YmKi435xcbFdkv2jjz6qseMEUHe4BghAg/Dggw8qNTXVadk999zj+HO/fv2cHuvXr59OnDghSTp9+rRCQ0Pl7e3teHzAgAGy2Ww6c+aMLBaLLl68qIcffvi2M9x///2OP3t7e8vX11eXLl2600MC4EIEEIAGwdvbu8JLUjXFarVWab3GjRs73bdYLLLZbLUxEoBaxjVAAH4VDh8+XOF+165dJUldu3bVyZMnVVJS4nj84MGDcnNzU+fOneXj46OQkBClpaXV6cwAXIczQAAahLKyMuXm5jota9SokVq2bClJeu+99xQeHq6BAwdq8+bNOnr0qNauXStJio6O1rx58xQTE6P58+crPz9fU6dO1TPPPKPAwEBJ0vz58zVp0iQFBARoyJAhunLlig4ePKipU6fW7YECqBMEEIAGYe/evWrdurXTss6dOyszM1PSz+/QeueddzR58mS1bt1a//M//6P77rtPktSkSRN9/PHHmjZtmvr06aMmTZpo5MiRSk5OduwrJiZGpaWl+u///m/913/9l1q2bKl///d/r7sDBFCnLHa73e7qIQDgblgsFu3YsUPDhw939SgAGgiuAQIAAMYhgAAAgHG4BghAg8cr+QCqizNAAADAOAQQAAAwDgEEAACMQwABAADjEEAAAMA4BBAAADAOAQQAAIxDAAEAAOMQQAAAwDj/H1YRGY19n16zAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Assuming history is the object returned by model.fit()\n",
    "# Calculate precision, recall, and F1 score for each epoch\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "mses = []\n",
    "maes = []\n",
    "\n",
    "for i in range(len(history.history['accuracy'])):\n",
    "    y_pred = model.predict(X_train)\n",
    "    y_pred_binary = np.round(y_pred)\n",
    "    precisions.append(precision_score(y_train, y_pred_binary, average='micro'))\n",
    "    recalls.append(recall_score(y_train, y_pred_binary, average='micro'))\n",
    "    f1_scores.append(f1_score(y_train, y_pred_binary, average='micro'))\n",
    "    mses.append(mean_squared_error(y_train, y_pred))\n",
    "    maes.append(mean_absolute_error(y_train, y_pred))\n",
    "\n",
    "# Plot precision\n",
    "plt.figure()\n",
    "plt.plot(precisions)\n",
    "plt.title('Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()\n",
    "\n",
    "# Plot recall\n",
    "plt.figure()\n",
    "plt.plot(recalls)\n",
    "plt.title('Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()\n",
    "\n",
    "# Plot F1 score\n",
    "plt.figure()\n",
    "plt.plot(f1_scores)\n",
    "plt.title('F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.show()\n",
    "\n",
    "# Plot MSE\n",
    "plt.figure()\n",
    "plt.plot(mses)\n",
    "plt.title('Mean Squared Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()\n",
    "\n",
    "# Plot MAE\n",
    "plt.figure()\n",
    "plt.plot(maes)\n",
    "plt.title('Mean Absolute Error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd28789b-17bd-4801-95a6-a8b128c32c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_6252\\3646409883.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[col] = pd.to_numeric(y[col], errors='coerce')\n",
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_6252\\3646409883.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y.fillna(0, inplace=True)\n",
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_6252\\3646409883.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y.fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "3/3 [==============================] - 5s 498ms/step - loss: 0.6885 - accuracy: 0.6000 - val_loss: 0.6925 - val_accuracy: 0.3000\n",
      "Epoch 2/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.6883 - accuracy: 0.5750 - val_loss: 0.6928 - val_accuracy: 0.3000\n",
      "Epoch 3/300\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6881 - accuracy: 0.5750 - val_loss: 0.6929 - val_accuracy: 0.3000\n",
      "Epoch 4/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6882 - accuracy: 0.5750 - val_loss: 0.6932 - val_accuracy: 0.3000\n",
      "Epoch 5/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6879 - accuracy: 0.5875 - val_loss: 0.6934 - val_accuracy: 0.3000\n",
      "Epoch 6/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6879 - accuracy: 0.5750 - val_loss: 0.6936 - val_accuracy: 0.3500\n",
      "Epoch 7/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6877 - accuracy: 0.5500 - val_loss: 0.6938 - val_accuracy: 0.3500\n",
      "Epoch 8/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6876 - accuracy: 0.5500 - val_loss: 0.6940 - val_accuracy: 0.3500\n",
      "Epoch 9/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6875 - accuracy: 0.5500 - val_loss: 0.6941 - val_accuracy: 0.3500\n",
      "Epoch 10/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6874 - accuracy: 0.5500 - val_loss: 0.6941 - val_accuracy: 0.3500\n",
      "Epoch 11/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6876 - accuracy: 0.5500 - val_loss: 0.6942 - val_accuracy: 0.3500\n",
      "Epoch 12/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6873 - accuracy: 0.5500 - val_loss: 0.6942 - val_accuracy: 0.3500\n",
      "Epoch 13/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6873 - accuracy: 0.5500 - val_loss: 0.6944 - val_accuracy: 0.3500\n",
      "Epoch 14/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6871 - accuracy: 0.5625 - val_loss: 0.6946 - val_accuracy: 0.3500\n",
      "Epoch 15/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6873 - accuracy: 0.5500 - val_loss: 0.6947 - val_accuracy: 0.3500\n",
      "Epoch 16/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6870 - accuracy: 0.5500 - val_loss: 0.6948 - val_accuracy: 0.3500\n",
      "Epoch 17/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6870 - accuracy: 0.5500 - val_loss: 0.6947 - val_accuracy: 0.3500\n",
      "Epoch 18/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6869 - accuracy: 0.5750 - val_loss: 0.6949 - val_accuracy: 0.3500\n",
      "Epoch 19/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6868 - accuracy: 0.5375 - val_loss: 0.6950 - val_accuracy: 0.3500\n",
      "Epoch 20/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6867 - accuracy: 0.5375 - val_loss: 0.6951 - val_accuracy: 0.3500\n",
      "Epoch 21/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6867 - accuracy: 0.5375 - val_loss: 0.6952 - val_accuracy: 0.3500\n",
      "Epoch 22/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6866 - accuracy: 0.5375 - val_loss: 0.6954 - val_accuracy: 0.3500\n",
      "Epoch 23/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6865 - accuracy: 0.5375 - val_loss: 0.6957 - val_accuracy: 0.4000\n",
      "Epoch 24/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6865 - accuracy: 0.5375 - val_loss: 0.6958 - val_accuracy: 0.4000\n",
      "Epoch 25/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6864 - accuracy: 0.5375 - val_loss: 0.6957 - val_accuracy: 0.3500\n",
      "Epoch 26/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6864 - accuracy: 0.5375 - val_loss: 0.6958 - val_accuracy: 0.4000\n",
      "Epoch 27/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6864 - accuracy: 0.5375 - val_loss: 0.6958 - val_accuracy: 0.4000\n",
      "Epoch 28/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6862 - accuracy: 0.5375 - val_loss: 0.6960 - val_accuracy: 0.4000\n",
      "Epoch 29/300\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.6861 - accuracy: 0.5375 - val_loss: 0.6961 - val_accuracy: 0.4000\n",
      "Epoch 30/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6861 - accuracy: 0.5375 - val_loss: 0.6963 - val_accuracy: 0.4000\n",
      "Epoch 31/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6861 - accuracy: 0.5375 - val_loss: 0.6965 - val_accuracy: 0.4500\n",
      "Epoch 32/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6860 - accuracy: 0.5375 - val_loss: 0.6967 - val_accuracy: 0.4500\n",
      "Epoch 33/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6859 - accuracy: 0.5375 - val_loss: 0.6966 - val_accuracy: 0.4500\n",
      "Epoch 34/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6858 - accuracy: 0.5375 - val_loss: 0.6967 - val_accuracy: 0.4500\n",
      "Epoch 35/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6858 - accuracy: 0.5375 - val_loss: 0.6968 - val_accuracy: 0.4500\n",
      "Epoch 36/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6859 - accuracy: 0.5375 - val_loss: 0.6968 - val_accuracy: 0.4500\n",
      "Epoch 37/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6857 - accuracy: 0.5375 - val_loss: 0.6968 - val_accuracy: 0.4500\n",
      "Epoch 38/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6856 - accuracy: 0.5375 - val_loss: 0.6968 - val_accuracy: 0.4500\n",
      "Epoch 39/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6856 - accuracy: 0.5375 - val_loss: 0.6969 - val_accuracy: 0.4500\n",
      "Epoch 40/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6856 - accuracy: 0.5375 - val_loss: 0.6970 - val_accuracy: 0.4500\n",
      "Epoch 41/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6858 - accuracy: 0.5375 - val_loss: 0.6973 - val_accuracy: 0.4500\n",
      "Epoch 42/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6854 - accuracy: 0.5500 - val_loss: 0.6974 - val_accuracy: 0.4500\n",
      "Epoch 43/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6854 - accuracy: 0.5375 - val_loss: 0.6976 - val_accuracy: 0.4500\n",
      "Epoch 44/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6853 - accuracy: 0.5500 - val_loss: 0.6976 - val_accuracy: 0.4500\n",
      "Epoch 45/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6853 - accuracy: 0.5500 - val_loss: 0.6978 - val_accuracy: 0.4500\n",
      "Epoch 46/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6853 - accuracy: 0.5375 - val_loss: 0.6980 - val_accuracy: 0.4500\n",
      "Epoch 47/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6851 - accuracy: 0.5500 - val_loss: 0.6980 - val_accuracy: 0.4500\n",
      "Epoch 48/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6851 - accuracy: 0.5500 - val_loss: 0.6981 - val_accuracy: 0.4500\n",
      "Epoch 49/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6853 - accuracy: 0.5500 - val_loss: 0.6984 - val_accuracy: 0.5000\n",
      "Epoch 50/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6850 - accuracy: 0.5500 - val_loss: 0.6983 - val_accuracy: 0.4500\n",
      "Epoch 51/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6849 - accuracy: 0.5500 - val_loss: 0.6985 - val_accuracy: 0.4500\n",
      "Epoch 52/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6849 - accuracy: 0.5500 - val_loss: 0.6986 - val_accuracy: 0.4500\n",
      "Epoch 53/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6850 - accuracy: 0.5500 - val_loss: 0.6984 - val_accuracy: 0.4500\n",
      "Epoch 54/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6848 - accuracy: 0.5500 - val_loss: 0.6985 - val_accuracy: 0.4500\n",
      "Epoch 55/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6848 - accuracy: 0.5500 - val_loss: 0.6984 - val_accuracy: 0.4500\n",
      "Epoch 56/300\n",
      "3/3 [==============================] - 0s 45ms/step - loss: 0.6847 - accuracy: 0.5500 - val_loss: 0.6984 - val_accuracy: 0.4500\n",
      "Epoch 57/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6847 - accuracy: 0.5500 - val_loss: 0.6985 - val_accuracy: 0.4500\n",
      "Epoch 58/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6849 - accuracy: 0.5500 - val_loss: 0.6986 - val_accuracy: 0.4500\n",
      "Epoch 59/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6846 - accuracy: 0.5500 - val_loss: 0.6985 - val_accuracy: 0.4500\n",
      "Epoch 60/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6845 - accuracy: 0.5500 - val_loss: 0.6984 - val_accuracy: 0.4500\n",
      "Epoch 61/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6846 - accuracy: 0.5375 - val_loss: 0.6985 - val_accuracy: 0.4500\n",
      "Epoch 62/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6844 - accuracy: 0.5375 - val_loss: 0.6985 - val_accuracy: 0.4500\n",
      "Epoch 63/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6844 - accuracy: 0.5375 - val_loss: 0.6986 - val_accuracy: 0.4500\n",
      "Epoch 64/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6844 - accuracy: 0.5500 - val_loss: 0.6988 - val_accuracy: 0.4500\n",
      "Epoch 65/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6844 - accuracy: 0.5500 - val_loss: 0.6990 - val_accuracy: 0.4500\n",
      "Epoch 66/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6844 - accuracy: 0.5500 - val_loss: 0.6989 - val_accuracy: 0.4500\n",
      "Epoch 67/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6843 - accuracy: 0.5500 - val_loss: 0.6991 - val_accuracy: 0.4500\n",
      "Epoch 68/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6843 - accuracy: 0.5500 - val_loss: 0.6990 - val_accuracy: 0.4500\n",
      "Epoch 69/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6842 - accuracy: 0.5500 - val_loss: 0.6990 - val_accuracy: 0.4500\n",
      "Epoch 70/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6842 - accuracy: 0.5375 - val_loss: 0.6990 - val_accuracy: 0.4500\n",
      "Epoch 71/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6843 - accuracy: 0.5375 - val_loss: 0.6988 - val_accuracy: 0.4000\n",
      "Epoch 72/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6840 - accuracy: 0.5375 - val_loss: 0.6987 - val_accuracy: 0.4000\n",
      "Epoch 73/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6840 - accuracy: 0.5375 - val_loss: 0.6987 - val_accuracy: 0.4000\n",
      "Epoch 74/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6840 - accuracy: 0.5375 - val_loss: 0.6988 - val_accuracy: 0.4000\n",
      "Epoch 75/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6840 - accuracy: 0.5375 - val_loss: 0.6988 - val_accuracy: 0.4000\n",
      "Epoch 76/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6839 - accuracy: 0.5375 - val_loss: 0.6987 - val_accuracy: 0.4000\n",
      "Epoch 77/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6839 - accuracy: 0.5375 - val_loss: 0.6988 - val_accuracy: 0.4000\n",
      "Epoch 78/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6838 - accuracy: 0.5375 - val_loss: 0.6988 - val_accuracy: 0.4000\n",
      "Epoch 79/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6839 - accuracy: 0.5375 - val_loss: 0.6988 - val_accuracy: 0.4000\n",
      "Epoch 80/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6840 - accuracy: 0.5375 - val_loss: 0.6991 - val_accuracy: 0.4000\n",
      "Epoch 81/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6838 - accuracy: 0.5375 - val_loss: 0.6994 - val_accuracy: 0.4000\n",
      "Epoch 82/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6837 - accuracy: 0.5375 - val_loss: 0.6993 - val_accuracy: 0.4000\n",
      "Epoch 83/300\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6837 - accuracy: 0.5375 - val_loss: 0.6993 - val_accuracy: 0.4000\n",
      "Epoch 84/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6837 - accuracy: 0.5375 - val_loss: 0.6992 - val_accuracy: 0.4000\n",
      "Epoch 85/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6836 - accuracy: 0.5375 - val_loss: 0.6993 - val_accuracy: 0.4000\n",
      "Epoch 86/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6836 - accuracy: 0.5375 - val_loss: 0.6994 - val_accuracy: 0.4000\n",
      "Epoch 87/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6836 - accuracy: 0.5375 - val_loss: 0.6996 - val_accuracy: 0.4000\n",
      "Epoch 88/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6835 - accuracy: 0.5375 - val_loss: 0.6996 - val_accuracy: 0.4000\n",
      "Epoch 89/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6834 - accuracy: 0.5375 - val_loss: 0.6997 - val_accuracy: 0.4000\n",
      "Epoch 90/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6834 - accuracy: 0.5375 - val_loss: 0.6997 - val_accuracy: 0.4000\n",
      "Epoch 91/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6835 - accuracy: 0.5375 - val_loss: 0.6996 - val_accuracy: 0.4000\n",
      "Epoch 92/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6834 - accuracy: 0.5375 - val_loss: 0.6997 - val_accuracy: 0.4000\n",
      "Epoch 93/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6834 - accuracy: 0.5375 - val_loss: 0.6997 - val_accuracy: 0.4000\n",
      "Epoch 94/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6833 - accuracy: 0.5375 - val_loss: 0.6997 - val_accuracy: 0.4000\n",
      "Epoch 95/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6832 - accuracy: 0.5375 - val_loss: 0.6997 - val_accuracy: 0.4000\n",
      "Epoch 96/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6834 - accuracy: 0.5375 - val_loss: 0.6998 - val_accuracy: 0.4000\n",
      "Epoch 97/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6832 - accuracy: 0.5375 - val_loss: 0.6998 - val_accuracy: 0.4000\n",
      "Epoch 98/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6833 - accuracy: 0.5375 - val_loss: 0.6999 - val_accuracy: 0.4000\n",
      "Epoch 99/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6831 - accuracy: 0.5375 - val_loss: 0.7001 - val_accuracy: 0.4000\n",
      "Epoch 100/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6831 - accuracy: 0.5375 - val_loss: 0.7001 - val_accuracy: 0.4000\n",
      "Epoch 101/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6830 - accuracy: 0.5375 - val_loss: 0.7001 - val_accuracy: 0.4000\n",
      "Epoch 102/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6831 - accuracy: 0.5375 - val_loss: 0.7003 - val_accuracy: 0.4000\n",
      "Epoch 103/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6831 - accuracy: 0.5375 - val_loss: 0.7005 - val_accuracy: 0.4000\n",
      "Epoch 104/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6830 - accuracy: 0.5375 - val_loss: 0.7003 - val_accuracy: 0.4000\n",
      "Epoch 105/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6830 - accuracy: 0.5375 - val_loss: 0.7002 - val_accuracy: 0.4000\n",
      "Epoch 106/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6828 - accuracy: 0.5500 - val_loss: 0.7002 - val_accuracy: 0.4000\n",
      "Epoch 107/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6828 - accuracy: 0.5500 - val_loss: 0.7002 - val_accuracy: 0.4000\n",
      "Epoch 108/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6828 - accuracy: 0.5500 - val_loss: 0.7002 - val_accuracy: 0.4000\n",
      "Epoch 109/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6827 - accuracy: 0.5500 - val_loss: 0.7002 - val_accuracy: 0.4000\n",
      "Epoch 110/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6827 - accuracy: 0.5500 - val_loss: 0.7004 - val_accuracy: 0.4000\n",
      "Epoch 111/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6828 - accuracy: 0.5500 - val_loss: 0.7002 - val_accuracy: 0.4000\n",
      "Epoch 112/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6826 - accuracy: 0.5500 - val_loss: 0.7001 - val_accuracy: 0.4000\n",
      "Epoch 113/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6826 - accuracy: 0.5500 - val_loss: 0.7000 - val_accuracy: 0.4000\n",
      "Epoch 114/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6826 - accuracy: 0.5500 - val_loss: 0.7000 - val_accuracy: 0.4000\n",
      "Epoch 115/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6825 - accuracy: 0.5500 - val_loss: 0.7002 - val_accuracy: 0.4000\n",
      "Epoch 116/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6825 - accuracy: 0.5500 - val_loss: 0.7003 - val_accuracy: 0.4000\n",
      "Epoch 117/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6824 - accuracy: 0.5500 - val_loss: 0.7002 - val_accuracy: 0.4000\n",
      "Epoch 118/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6825 - accuracy: 0.5500 - val_loss: 0.7004 - val_accuracy: 0.4000\n",
      "Epoch 119/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6824 - accuracy: 0.5500 - val_loss: 0.7003 - val_accuracy: 0.4000\n",
      "Epoch 120/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6823 - accuracy: 0.5500 - val_loss: 0.7003 - val_accuracy: 0.4000\n",
      "Epoch 121/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6823 - accuracy: 0.5500 - val_loss: 0.7003 - val_accuracy: 0.4000\n",
      "Epoch 122/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6825 - accuracy: 0.5500 - val_loss: 0.7001 - val_accuracy: 0.4000\n",
      "Epoch 123/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6822 - accuracy: 0.5500 - val_loss: 0.7001 - val_accuracy: 0.4000\n",
      "Epoch 124/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6822 - accuracy: 0.5500 - val_loss: 0.7002 - val_accuracy: 0.4000\n",
      "Epoch 125/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6822 - accuracy: 0.5500 - val_loss: 0.7001 - val_accuracy: 0.4000\n",
      "Epoch 126/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6822 - accuracy: 0.5500 - val_loss: 0.7002 - val_accuracy: 0.4000\n",
      "Epoch 127/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6821 - accuracy: 0.5500 - val_loss: 0.7002 - val_accuracy: 0.4000\n",
      "Epoch 128/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6821 - accuracy: 0.5500 - val_loss: 0.7004 - val_accuracy: 0.4000\n",
      "Epoch 129/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6821 - accuracy: 0.5500 - val_loss: 0.7007 - val_accuracy: 0.4000\n",
      "Epoch 130/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6819 - accuracy: 0.5500 - val_loss: 0.7008 - val_accuracy: 0.4000\n",
      "Epoch 131/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6820 - accuracy: 0.5500 - val_loss: 0.7009 - val_accuracy: 0.4000\n",
      "Epoch 132/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6820 - accuracy: 0.5500 - val_loss: 0.7011 - val_accuracy: 0.4000\n",
      "Epoch 133/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6819 - accuracy: 0.5500 - val_loss: 0.7012 - val_accuracy: 0.4000\n",
      "Epoch 134/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6818 - accuracy: 0.5500 - val_loss: 0.7012 - val_accuracy: 0.4000\n",
      "Epoch 135/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6818 - accuracy: 0.5500 - val_loss: 0.7013 - val_accuracy: 0.4000\n",
      "Epoch 136/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6818 - accuracy: 0.5500 - val_loss: 0.7013 - val_accuracy: 0.4000\n",
      "Epoch 137/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6817 - accuracy: 0.5500 - val_loss: 0.7014 - val_accuracy: 0.4000\n",
      "Epoch 138/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6817 - accuracy: 0.5500 - val_loss: 0.7014 - val_accuracy: 0.4000\n",
      "Epoch 139/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6817 - accuracy: 0.5500 - val_loss: 0.7013 - val_accuracy: 0.4000\n",
      "Epoch 140/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6818 - accuracy: 0.5500 - val_loss: 0.7014 - val_accuracy: 0.4000\n",
      "Epoch 141/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6816 - accuracy: 0.5500 - val_loss: 0.7014 - val_accuracy: 0.4000\n",
      "Epoch 142/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6816 - accuracy: 0.5500 - val_loss: 0.7014 - val_accuracy: 0.4000\n",
      "Epoch 143/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6816 - accuracy: 0.5500 - val_loss: 0.7014 - val_accuracy: 0.4000\n",
      "Epoch 144/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6816 - accuracy: 0.5500 - val_loss: 0.7013 - val_accuracy: 0.4000\n",
      "Epoch 145/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6815 - accuracy: 0.5500 - val_loss: 0.7012 - val_accuracy: 0.4000\n",
      "Epoch 146/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6816 - accuracy: 0.5500 - val_loss: 0.7012 - val_accuracy: 0.4000\n",
      "Epoch 147/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6818 - accuracy: 0.5500 - val_loss: 0.7010 - val_accuracy: 0.4000\n",
      "Epoch 148/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6815 - accuracy: 0.5500 - val_loss: 0.7009 - val_accuracy: 0.4000\n",
      "Epoch 149/300\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.6816 - accuracy: 0.5500 - val_loss: 0.7009 - val_accuracy: 0.4000\n",
      "Epoch 150/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6814 - accuracy: 0.5500 - val_loss: 0.7011 - val_accuracy: 0.4000\n",
      "Epoch 151/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6814 - accuracy: 0.5500 - val_loss: 0.7012 - val_accuracy: 0.4000\n",
      "Epoch 152/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6817 - accuracy: 0.5500 - val_loss: 0.7009 - val_accuracy: 0.4000\n",
      "Epoch 153/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6814 - accuracy: 0.5500 - val_loss: 0.7008 - val_accuracy: 0.4000\n",
      "Epoch 154/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6813 - accuracy: 0.5500 - val_loss: 0.7009 - val_accuracy: 0.4000\n",
      "Epoch 155/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6813 - accuracy: 0.5500 - val_loss: 0.7011 - val_accuracy: 0.4000\n",
      "Epoch 156/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6813 - accuracy: 0.5500 - val_loss: 0.7010 - val_accuracy: 0.4000\n",
      "Epoch 157/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6813 - accuracy: 0.5500 - val_loss: 0.7011 - val_accuracy: 0.4000\n",
      "Epoch 158/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6812 - accuracy: 0.5500 - val_loss: 0.7011 - val_accuracy: 0.4000\n",
      "Epoch 159/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6812 - accuracy: 0.5500 - val_loss: 0.7012 - val_accuracy: 0.4000\n",
      "Epoch 160/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6812 - accuracy: 0.5500 - val_loss: 0.7015 - val_accuracy: 0.4000\n",
      "Epoch 161/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6812 - accuracy: 0.5500 - val_loss: 0.7016 - val_accuracy: 0.4000\n",
      "Epoch 162/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6811 - accuracy: 0.5500 - val_loss: 0.7016 - val_accuracy: 0.4000\n",
      "Epoch 163/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6810 - accuracy: 0.5500 - val_loss: 0.7017 - val_accuracy: 0.4000\n",
      "Epoch 164/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6810 - accuracy: 0.5500 - val_loss: 0.7017 - val_accuracy: 0.4000\n",
      "Epoch 165/300\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6810 - accuracy: 0.5500 - val_loss: 0.7017 - val_accuracy: 0.4000\n",
      "Epoch 166/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6810 - accuracy: 0.5500 - val_loss: 0.7018 - val_accuracy: 0.4000\n",
      "Epoch 167/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6809 - accuracy: 0.5500 - val_loss: 0.7018 - val_accuracy: 0.4000\n",
      "Epoch 168/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6809 - accuracy: 0.5500 - val_loss: 0.7019 - val_accuracy: 0.4000\n",
      "Epoch 169/300\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.6808 - accuracy: 0.5500 - val_loss: 0.7019 - val_accuracy: 0.4000\n",
      "Epoch 170/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6808 - accuracy: 0.5500 - val_loss: 0.7019 - val_accuracy: 0.4000\n",
      "Epoch 171/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6809 - accuracy: 0.5500 - val_loss: 0.7019 - val_accuracy: 0.4000\n",
      "Epoch 172/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6808 - accuracy: 0.5500 - val_loss: 0.7018 - val_accuracy: 0.4000\n",
      "Epoch 173/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6808 - accuracy: 0.5500 - val_loss: 0.7018 - val_accuracy: 0.4000\n",
      "Epoch 174/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6808 - accuracy: 0.5625 - val_loss: 0.7017 - val_accuracy: 0.4000\n",
      "Epoch 175/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6808 - accuracy: 0.5625 - val_loss: 0.7017 - val_accuracy: 0.4000\n",
      "Epoch 176/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6808 - accuracy: 0.5500 - val_loss: 0.7017 - val_accuracy: 0.4000\n",
      "Epoch 177/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6808 - accuracy: 0.5625 - val_loss: 0.7018 - val_accuracy: 0.4000\n",
      "Epoch 178/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6807 - accuracy: 0.5500 - val_loss: 0.7019 - val_accuracy: 0.4000\n",
      "Epoch 179/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6806 - accuracy: 0.5625 - val_loss: 0.7019 - val_accuracy: 0.4000\n",
      "Epoch 180/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6806 - accuracy: 0.5625 - val_loss: 0.7018 - val_accuracy: 0.4000\n",
      "Epoch 181/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6806 - accuracy: 0.5625 - val_loss: 0.7019 - val_accuracy: 0.4000\n",
      "Epoch 182/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6806 - accuracy: 0.5625 - val_loss: 0.7021 - val_accuracy: 0.4000\n",
      "Epoch 183/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6805 - accuracy: 0.5625 - val_loss: 0.7021 - val_accuracy: 0.4000\n",
      "Epoch 184/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6805 - accuracy: 0.5625 - val_loss: 0.7020 - val_accuracy: 0.4000\n",
      "Epoch 185/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6806 - accuracy: 0.5625 - val_loss: 0.7018 - val_accuracy: 0.4000\n",
      "Epoch 186/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6804 - accuracy: 0.5625 - val_loss: 0.7017 - val_accuracy: 0.3000\n",
      "Epoch 187/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6804 - accuracy: 0.5625 - val_loss: 0.7018 - val_accuracy: 0.3000\n",
      "Epoch 188/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6806 - accuracy: 0.5625 - val_loss: 0.7018 - val_accuracy: 0.3000\n",
      "Epoch 189/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6804 - accuracy: 0.5500 - val_loss: 0.7020 - val_accuracy: 0.4000\n",
      "Epoch 190/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6804 - accuracy: 0.5625 - val_loss: 0.7023 - val_accuracy: 0.4000\n",
      "Epoch 191/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6803 - accuracy: 0.5625 - val_loss: 0.7023 - val_accuracy: 0.4000\n",
      "Epoch 192/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6802 - accuracy: 0.5625 - val_loss: 0.7024 - val_accuracy: 0.4000\n",
      "Epoch 193/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6804 - accuracy: 0.5625 - val_loss: 0.7025 - val_accuracy: 0.4000\n",
      "Epoch 194/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6803 - accuracy: 0.5625 - val_loss: 0.7027 - val_accuracy: 0.4000\n",
      "Epoch 195/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6802 - accuracy: 0.5500 - val_loss: 0.7026 - val_accuracy: 0.4000\n",
      "Epoch 196/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6802 - accuracy: 0.5625 - val_loss: 0.7025 - val_accuracy: 0.4000\n",
      "Epoch 197/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6802 - accuracy: 0.5625 - val_loss: 0.7025 - val_accuracy: 0.4000\n",
      "Epoch 198/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6802 - accuracy: 0.5625 - val_loss: 0.7027 - val_accuracy: 0.4000\n",
      "Epoch 199/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6801 - accuracy: 0.5625 - val_loss: 0.7026 - val_accuracy: 0.4000\n",
      "Epoch 200/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6801 - accuracy: 0.5625 - val_loss: 0.7028 - val_accuracy: 0.4000\n",
      "Epoch 201/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6801 - accuracy: 0.5625 - val_loss: 0.7026 - val_accuracy: 0.4000\n",
      "Epoch 202/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6800 - accuracy: 0.5625 - val_loss: 0.7025 - val_accuracy: 0.4000\n",
      "Epoch 203/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6800 - accuracy: 0.5625 - val_loss: 0.7026 - val_accuracy: 0.4000\n",
      "Epoch 204/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6800 - accuracy: 0.5625 - val_loss: 0.7027 - val_accuracy: 0.4000\n",
      "Epoch 205/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6801 - accuracy: 0.5625 - val_loss: 0.7029 - val_accuracy: 0.4000\n",
      "Epoch 206/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6800 - accuracy: 0.5625 - val_loss: 0.7027 - val_accuracy: 0.4000\n",
      "Epoch 207/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6800 - accuracy: 0.5625 - val_loss: 0.7027 - val_accuracy: 0.4000\n",
      "Epoch 208/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6800 - accuracy: 0.5625 - val_loss: 0.7027 - val_accuracy: 0.4000\n",
      "Epoch 209/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6800 - accuracy: 0.5625 - val_loss: 0.7025 - val_accuracy: 0.3000\n",
      "Epoch 210/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6799 - accuracy: 0.5625 - val_loss: 0.7026 - val_accuracy: 0.3500\n",
      "Epoch 211/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6799 - accuracy: 0.5625 - val_loss: 0.7028 - val_accuracy: 0.4000\n",
      "Epoch 212/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6799 - accuracy: 0.5625 - val_loss: 0.7028 - val_accuracy: 0.4000\n",
      "Epoch 213/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6798 - accuracy: 0.5625 - val_loss: 0.7029 - val_accuracy: 0.4000\n",
      "Epoch 214/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6800 - accuracy: 0.5625 - val_loss: 0.7029 - val_accuracy: 0.4000\n",
      "Epoch 215/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6798 - accuracy: 0.5625 - val_loss: 0.7027 - val_accuracy: 0.3000\n",
      "Epoch 216/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6798 - accuracy: 0.5625 - val_loss: 0.7028 - val_accuracy: 0.3500\n",
      "Epoch 217/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6799 - accuracy: 0.5750 - val_loss: 0.7028 - val_accuracy: 0.3500\n",
      "Epoch 218/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6799 - accuracy: 0.5625 - val_loss: 0.7028 - val_accuracy: 0.4000\n",
      "Epoch 219/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6800 - accuracy: 0.5625 - val_loss: 0.7025 - val_accuracy: 0.3000\n",
      "Epoch 220/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6797 - accuracy: 0.5625 - val_loss: 0.7025 - val_accuracy: 0.3000\n",
      "Epoch 221/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6798 - accuracy: 0.5750 - val_loss: 0.7028 - val_accuracy: 0.3000\n",
      "Epoch 222/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6798 - accuracy: 0.5625 - val_loss: 0.7028 - val_accuracy: 0.3000\n",
      "Epoch 223/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6799 - accuracy: 0.5750 - val_loss: 0.7025 - val_accuracy: 0.3000\n",
      "Epoch 224/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6796 - accuracy: 0.5750 - val_loss: 0.7025 - val_accuracy: 0.3000\n",
      "Epoch 225/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6796 - accuracy: 0.5875 - val_loss: 0.7025 - val_accuracy: 0.3000\n",
      "Epoch 226/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6796 - accuracy: 0.5750 - val_loss: 0.7024 - val_accuracy: 0.3000\n",
      "Epoch 227/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6797 - accuracy: 0.5875 - val_loss: 0.7027 - val_accuracy: 0.3000\n",
      "Epoch 228/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6796 - accuracy: 0.5625 - val_loss: 0.7026 - val_accuracy: 0.3000\n",
      "Epoch 229/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6796 - accuracy: 0.5875 - val_loss: 0.7027 - val_accuracy: 0.3000\n",
      "Epoch 230/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6795 - accuracy: 0.5875 - val_loss: 0.7028 - val_accuracy: 0.3000\n",
      "Epoch 231/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6795 - accuracy: 0.5875 - val_loss: 0.7027 - val_accuracy: 0.3000\n",
      "Epoch 232/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6795 - accuracy: 0.5875 - val_loss: 0.7027 - val_accuracy: 0.3000\n",
      "Epoch 233/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6794 - accuracy: 0.5875 - val_loss: 0.7029 - val_accuracy: 0.3000\n",
      "Epoch 234/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6794 - accuracy: 0.5875 - val_loss: 0.7030 - val_accuracy: 0.3000\n",
      "Epoch 235/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6795 - accuracy: 0.5875 - val_loss: 0.7029 - val_accuracy: 0.3000\n",
      "Epoch 236/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.6794 - accuracy: 0.5875 - val_loss: 0.7031 - val_accuracy: 0.3000\n",
      "Epoch 237/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6794 - accuracy: 0.5750 - val_loss: 0.7031 - val_accuracy: 0.3000\n",
      "Epoch 238/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6793 - accuracy: 0.5750 - val_loss: 0.7030 - val_accuracy: 0.3000\n",
      "Epoch 239/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6793 - accuracy: 0.5875 - val_loss: 0.7030 - val_accuracy: 0.3000\n",
      "Epoch 240/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6794 - accuracy: 0.5875 - val_loss: 0.7029 - val_accuracy: 0.3000\n",
      "Epoch 241/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6793 - accuracy: 0.5875 - val_loss: 0.7029 - val_accuracy: 0.3000\n",
      "Epoch 242/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6794 - accuracy: 0.5875 - val_loss: 0.7027 - val_accuracy: 0.3000\n",
      "Epoch 243/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6794 - accuracy: 0.5875 - val_loss: 0.7026 - val_accuracy: 0.2500\n",
      "Epoch 244/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6794 - accuracy: 0.5875 - val_loss: 0.7028 - val_accuracy: 0.3000\n",
      "Epoch 245/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6792 - accuracy: 0.6000 - val_loss: 0.7028 - val_accuracy: 0.3000\n",
      "Epoch 246/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6794 - accuracy: 0.5875 - val_loss: 0.7026 - val_accuracy: 0.2500\n",
      "Epoch 247/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6791 - accuracy: 0.6000 - val_loss: 0.7026 - val_accuracy: 0.2500\n",
      "Epoch 248/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6792 - accuracy: 0.6000 - val_loss: 0.7029 - val_accuracy: 0.3000\n",
      "Epoch 249/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6791 - accuracy: 0.6000 - val_loss: 0.7028 - val_accuracy: 0.2500\n",
      "Epoch 250/300\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 0.6792 - accuracy: 0.6000 - val_loss: 0.7029 - val_accuracy: 0.3000\n",
      "Epoch 251/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6792 - accuracy: 0.5875 - val_loss: 0.7032 - val_accuracy: 0.3000\n",
      "Epoch 252/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6790 - accuracy: 0.6000 - val_loss: 0.7033 - val_accuracy: 0.3000\n",
      "Epoch 253/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6791 - accuracy: 0.5875 - val_loss: 0.7034 - val_accuracy: 0.3000\n",
      "Epoch 254/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6790 - accuracy: 0.6000 - val_loss: 0.7036 - val_accuracy: 0.3000\n",
      "Epoch 255/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6790 - accuracy: 0.5875 - val_loss: 0.7037 - val_accuracy: 0.3000\n",
      "Epoch 256/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6790 - accuracy: 0.5875 - val_loss: 0.7038 - val_accuracy: 0.3000\n",
      "Epoch 257/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6790 - accuracy: 0.5875 - val_loss: 0.7037 - val_accuracy: 0.3000\n",
      "Epoch 258/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6790 - accuracy: 0.5875 - val_loss: 0.7038 - val_accuracy: 0.3000\n",
      "Epoch 259/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6789 - accuracy: 0.5875 - val_loss: 0.7038 - val_accuracy: 0.3000\n",
      "Epoch 260/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6789 - accuracy: 0.5875 - val_loss: 0.7038 - val_accuracy: 0.3000\n",
      "Epoch 261/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6789 - accuracy: 0.5875 - val_loss: 0.7038 - val_accuracy: 0.3000\n",
      "Epoch 262/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6789 - accuracy: 0.5875 - val_loss: 0.7037 - val_accuracy: 0.3000\n",
      "Epoch 263/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6789 - accuracy: 0.5875 - val_loss: 0.7036 - val_accuracy: 0.3000\n",
      "Epoch 264/300\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6788 - accuracy: 0.6000 - val_loss: 0.7035 - val_accuracy: 0.3000\n",
      "Epoch 265/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6789 - accuracy: 0.6000 - val_loss: 0.7035 - val_accuracy: 0.3000\n",
      "Epoch 266/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6788 - accuracy: 0.6000 - val_loss: 0.7034 - val_accuracy: 0.2500\n",
      "Epoch 267/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6788 - accuracy: 0.5875 - val_loss: 0.7034 - val_accuracy: 0.2500\n",
      "Epoch 268/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6788 - accuracy: 0.5875 - val_loss: 0.7033 - val_accuracy: 0.2500\n",
      "Epoch 269/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6789 - accuracy: 0.6000 - val_loss: 0.7035 - val_accuracy: 0.2500\n",
      "Epoch 270/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6787 - accuracy: 0.6000 - val_loss: 0.7035 - val_accuracy: 0.2500\n",
      "Epoch 271/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6788 - accuracy: 0.6000 - val_loss: 0.7033 - val_accuracy: 0.2500\n",
      "Epoch 272/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6787 - accuracy: 0.6000 - val_loss: 0.7034 - val_accuracy: 0.2500\n",
      "Epoch 273/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6787 - accuracy: 0.6000 - val_loss: 0.7033 - val_accuracy: 0.2500\n",
      "Epoch 274/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6787 - accuracy: 0.6000 - val_loss: 0.7034 - val_accuracy: 0.2500\n",
      "Epoch 275/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6788 - accuracy: 0.6000 - val_loss: 0.7032 - val_accuracy: 0.2500\n",
      "Epoch 276/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6787 - accuracy: 0.5875 - val_loss: 0.7033 - val_accuracy: 0.2500\n",
      "Epoch 277/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6787 - accuracy: 0.6000 - val_loss: 0.7033 - val_accuracy: 0.2500\n",
      "Epoch 278/300\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6786 - accuracy: 0.6000 - val_loss: 0.7034 - val_accuracy: 0.2500\n",
      "Epoch 279/300\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6785 - accuracy: 0.6000 - val_loss: 0.7035 - val_accuracy: 0.2500\n",
      "Epoch 280/300\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6787 - accuracy: 0.6000 - val_loss: 0.7033 - val_accuracy: 0.2500\n",
      "Epoch 281/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6786 - accuracy: 0.6000 - val_loss: 0.7033 - val_accuracy: 0.2500\n",
      "Epoch 282/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6787 - accuracy: 0.5750 - val_loss: 0.7035 - val_accuracy: 0.2500\n",
      "Epoch 283/300\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6785 - accuracy: 0.6000 - val_loss: 0.7035 - val_accuracy: 0.2500\n",
      "Epoch 284/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6785 - accuracy: 0.6000 - val_loss: 0.7035 - val_accuracy: 0.2500\n",
      "Epoch 285/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6785 - accuracy: 0.6000 - val_loss: 0.7036 - val_accuracy: 0.2500\n",
      "Epoch 286/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6785 - accuracy: 0.6000 - val_loss: 0.7037 - val_accuracy: 0.2500\n",
      "Epoch 287/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6784 - accuracy: 0.6000 - val_loss: 0.7037 - val_accuracy: 0.2500\n",
      "Epoch 288/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6784 - accuracy: 0.6000 - val_loss: 0.7037 - val_accuracy: 0.2500\n",
      "Epoch 289/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6784 - accuracy: 0.6000 - val_loss: 0.7037 - val_accuracy: 0.2500\n",
      "Epoch 290/300\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6784 - accuracy: 0.6000 - val_loss: 0.7036 - val_accuracy: 0.2500\n",
      "Epoch 291/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6784 - accuracy: 0.5875 - val_loss: 0.7037 - val_accuracy: 0.2500\n",
      "Epoch 292/300\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.6788 - accuracy: 0.6000 - val_loss: 0.7034 - val_accuracy: 0.2500\n",
      "Epoch 293/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6784 - accuracy: 0.5875 - val_loss: 0.7036 - val_accuracy: 0.2500\n",
      "Epoch 294/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6784 - accuracy: 0.5750 - val_loss: 0.7038 - val_accuracy: 0.2500\n",
      "Epoch 295/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6783 - accuracy: 0.5875 - val_loss: 0.7039 - val_accuracy: 0.2500\n",
      "Epoch 296/300\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6783 - accuracy: 0.6000 - val_loss: 0.7038 - val_accuracy: 0.2500\n",
      "Epoch 297/300\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6783 - accuracy: 0.6000 - val_loss: 0.7037 - val_accuracy: 0.2500\n",
      "Epoch 298/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6785 - accuracy: 0.5875 - val_loss: 0.7037 - val_accuracy: 0.2500\n",
      "Epoch 299/300\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6783 - accuracy: 0.5875 - val_loss: 0.7037 - val_accuracy: 0.2500\n",
      "Epoch 300/300\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6784 - accuracy: 0.5875 - val_loss: 0.7038 - val_accuracy: 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x15297ffdcd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from  tensorflow . keras.models import Sequential\n",
    "from  tensorflow . keras.layers import Bidirectional,LSTM, Dense\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv('Clinical data ppf and eph.csv')\n",
    "\n",
    "# Split the data into input (X) and output (y) samples\n",
    "X = data.drop(columns=['intraop_ppf', 'intraop_eph'])\n",
    "y = data[['intraop_ppf', 'intraop_eph']]\n",
    "\n",
    "# Convert non-numeric values to NaN for each column in X\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "# Convert non-numeric values to NaN for each column in y\n",
    "for col in y.columns:\n",
    "    y[col] = pd.to_numeric(y[col], errors='coerce')\n",
    "\n",
    "# Fill NaN values with a specific value (e.g., 0) in X and y\n",
    "X.fillna(0, inplace=True)\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Convert X to a numpy array\n",
    "X = X.to_numpy()\n",
    "\n",
    "# Fill NaN values in y with 0\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape the input data to be 3D for BI-LSTM input [samples, timesteps, features]\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "X_train = np.random.randn(100, 10, 1)  \n",
    "y_train = np.random.randint(0, 2, size=(100,))  \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False), input_shape=(10, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# Change the optimizer from 'adam' to 'sgd'\n",
    "model.compile(optimizer=SGD(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f1990e-dbdb-48a5-9dea-f84471ff8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape y_train and y_pred to have shape (n_samples, 2)\n",
    "y_train_reshaped = y_train.reshape(-1, 1)  # Assuming y_train is a 1D array with shape (n_samples,)\n",
    "y_pred_reshaped = y_pred.reshape(-1, 1)  # Assuming y_pred is a 1D array with shape (n_samples,)\n",
    "\n",
    "# Concatenate y_train_reshaped and y_pred_reshaped along the second axis to create a (n_samples, 2) array\n",
    "y_train_combined = np.concatenate((y_train_reshaped, y_pred_reshaped), axis=1)\n",
    "\n",
    "# Calculate precision for each output parameter separately\n",
    "precision_0 = precision_score(y_train_combined[:, 0], y_train_combined[:, 1])\n",
    "precision_1 = precision_score(y_train_combined[:, 0], y_train_combined[:, 1])\n",
    "\n",
    "print(\"Precision for output parameter 0:\", precision_0)\n",
    "print(\"Precision for output parameter 1:\", precision_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71993dac-0f75-4fb4-90bc-11cf23a44f51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Predict on the test data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert probabilities to binary predictions\u001b[39;00m\n\u001b[0;32m      5\u001b[0m y_pred_binary \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Calculate precision\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "\n",
    "print(\"Precision:\", precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec839914-9486-4821-ad0d-a0590f88ad36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_18948\\2678955929.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[col] = pd.to_numeric(y[col], errors='coerce')\n",
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_18948\\2678955929.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y.fillna(0, inplace=True)\n",
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_18948\\2678955929.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y.fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/245\n",
      "WARNING:tensorflow:From C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3/3 [==============================] - 4s 336ms/step - loss: 0.6925 - accuracy: 0.5125 - val_loss: 0.6943 - val_accuracy: 0.5000\n",
      "Epoch 2/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6880 - accuracy: 0.5875 - val_loss: 0.6977 - val_accuracy: 0.4500\n",
      "Epoch 3/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6839 - accuracy: 0.5750 - val_loss: 0.7000 - val_accuracy: 0.4500\n",
      "Epoch 4/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6803 - accuracy: 0.5750 - val_loss: 0.7025 - val_accuracy: 0.4500\n",
      "Epoch 5/245\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6774 - accuracy: 0.5750 - val_loss: 0.7066 - val_accuracy: 0.4500\n",
      "Epoch 6/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6764 - accuracy: 0.5750 - val_loss: 0.7104 - val_accuracy: 0.4500\n",
      "Epoch 7/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6726 - accuracy: 0.5625 - val_loss: 0.7136 - val_accuracy: 0.4500\n",
      "Epoch 8/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6720 - accuracy: 0.5625 - val_loss: 0.7174 - val_accuracy: 0.5000\n",
      "Epoch 9/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6707 - accuracy: 0.5625 - val_loss: 0.7222 - val_accuracy: 0.4500\n",
      "Epoch 10/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6707 - accuracy: 0.5625 - val_loss: 0.7282 - val_accuracy: 0.4500\n",
      "Epoch 11/245\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6700 - accuracy: 0.5625 - val_loss: 0.7323 - val_accuracy: 0.4500\n",
      "Epoch 12/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6700 - accuracy: 0.5625 - val_loss: 0.7347 - val_accuracy: 0.4500\n",
      "Epoch 13/245\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6700 - accuracy: 0.5750 - val_loss: 0.7342 - val_accuracy: 0.4000\n",
      "Epoch 14/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6689 - accuracy: 0.5750 - val_loss: 0.7343 - val_accuracy: 0.4000\n",
      "Epoch 15/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6686 - accuracy: 0.5750 - val_loss: 0.7342 - val_accuracy: 0.4000\n",
      "Epoch 16/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6682 - accuracy: 0.5875 - val_loss: 0.7355 - val_accuracy: 0.4000\n",
      "Epoch 17/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6678 - accuracy: 0.5875 - val_loss: 0.7361 - val_accuracy: 0.4000\n",
      "Epoch 18/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6673 - accuracy: 0.5875 - val_loss: 0.7330 - val_accuracy: 0.4000\n",
      "Epoch 19/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6665 - accuracy: 0.5875 - val_loss: 0.7299 - val_accuracy: 0.4000\n",
      "Epoch 20/245\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6667 - accuracy: 0.5750 - val_loss: 0.7265 - val_accuracy: 0.4500\n",
      "Epoch 21/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6653 - accuracy: 0.5750 - val_loss: 0.7240 - val_accuracy: 0.4500\n",
      "Epoch 22/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6650 - accuracy: 0.5750 - val_loss: 0.7236 - val_accuracy: 0.4500\n",
      "Epoch 23/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6649 - accuracy: 0.5750 - val_loss: 0.7218 - val_accuracy: 0.4500\n",
      "Epoch 24/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6653 - accuracy: 0.5875 - val_loss: 0.7215 - val_accuracy: 0.4000\n",
      "Epoch 25/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6637 - accuracy: 0.5875 - val_loss: 0.7253 - val_accuracy: 0.4000\n",
      "Epoch 26/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6635 - accuracy: 0.5875 - val_loss: 0.7289 - val_accuracy: 0.4000\n",
      "Epoch 27/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6629 - accuracy: 0.5875 - val_loss: 0.7318 - val_accuracy: 0.4000\n",
      "Epoch 28/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6621 - accuracy: 0.5875 - val_loss: 0.7334 - val_accuracy: 0.4000\n",
      "Epoch 29/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6618 - accuracy: 0.5875 - val_loss: 0.7352 - val_accuracy: 0.4000\n",
      "Epoch 30/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6612 - accuracy: 0.5875 - val_loss: 0.7362 - val_accuracy: 0.4000\n",
      "Epoch 31/245\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.6600 - accuracy: 0.5875 - val_loss: 0.7344 - val_accuracy: 0.4000\n",
      "Epoch 32/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6586 - accuracy: 0.5875 - val_loss: 0.7332 - val_accuracy: 0.4000\n",
      "Epoch 33/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6584 - accuracy: 0.6000 - val_loss: 0.7321 - val_accuracy: 0.4000\n",
      "Epoch 34/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6573 - accuracy: 0.6000 - val_loss: 0.7270 - val_accuracy: 0.4500\n",
      "Epoch 35/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6559 - accuracy: 0.6000 - val_loss: 0.7249 - val_accuracy: 0.4500\n",
      "Epoch 36/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6560 - accuracy: 0.6125 - val_loss: 0.7231 - val_accuracy: 0.5000\n",
      "Epoch 37/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6565 - accuracy: 0.6250 - val_loss: 0.7207 - val_accuracy: 0.5500\n",
      "Epoch 38/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6553 - accuracy: 0.6125 - val_loss: 0.7199 - val_accuracy: 0.5500\n",
      "Epoch 39/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6557 - accuracy: 0.6125 - val_loss: 0.7206 - val_accuracy: 0.5500\n",
      "Epoch 40/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6527 - accuracy: 0.6125 - val_loss: 0.7178 - val_accuracy: 0.5500\n",
      "Epoch 41/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6515 - accuracy: 0.6125 - val_loss: 0.7174 - val_accuracy: 0.4500\n",
      "Epoch 42/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6500 - accuracy: 0.5875 - val_loss: 0.7184 - val_accuracy: 0.4500\n",
      "Epoch 43/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6492 - accuracy: 0.6000 - val_loss: 0.7207 - val_accuracy: 0.4500\n",
      "Epoch 44/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6483 - accuracy: 0.6125 - val_loss: 0.7249 - val_accuracy: 0.4500\n",
      "Epoch 45/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6482 - accuracy: 0.6125 - val_loss: 0.7301 - val_accuracy: 0.4500\n",
      "Epoch 46/245\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6467 - accuracy: 0.5875 - val_loss: 0.7325 - val_accuracy: 0.4000\n",
      "Epoch 47/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6464 - accuracy: 0.5875 - val_loss: 0.7330 - val_accuracy: 0.4500\n",
      "Epoch 48/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6448 - accuracy: 0.6375 - val_loss: 0.7332 - val_accuracy: 0.4500\n",
      "Epoch 49/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6442 - accuracy: 0.6250 - val_loss: 0.7335 - val_accuracy: 0.5000\n",
      "Epoch 50/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6414 - accuracy: 0.6375 - val_loss: 0.7376 - val_accuracy: 0.4500\n",
      "Epoch 51/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6391 - accuracy: 0.6500 - val_loss: 0.7389 - val_accuracy: 0.4000\n",
      "Epoch 52/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6382 - accuracy: 0.6500 - val_loss: 0.7425 - val_accuracy: 0.3500\n",
      "Epoch 53/245\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6359 - accuracy: 0.6500 - val_loss: 0.7499 - val_accuracy: 0.3500\n",
      "Epoch 54/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6341 - accuracy: 0.6500 - val_loss: 0.7574 - val_accuracy: 0.3500\n",
      "Epoch 55/245\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6318 - accuracy: 0.6500 - val_loss: 0.7666 - val_accuracy: 0.4000\n",
      "Epoch 56/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6285 - accuracy: 0.6250 - val_loss: 0.7685 - val_accuracy: 0.4500\n",
      "Epoch 57/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6239 - accuracy: 0.6125 - val_loss: 0.7722 - val_accuracy: 0.4500\n",
      "Epoch 58/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6253 - accuracy: 0.6375 - val_loss: 0.7746 - val_accuracy: 0.4000\n",
      "Epoch 59/245\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6171 - accuracy: 0.6875 - val_loss: 0.7945 - val_accuracy: 0.4000\n",
      "Epoch 60/245\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6192 - accuracy: 0.6250 - val_loss: 0.8162 - val_accuracy: 0.5000\n",
      "Epoch 61/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6145 - accuracy: 0.6250 - val_loss: 0.8038 - val_accuracy: 0.4500\n",
      "Epoch 62/245\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6057 - accuracy: 0.6875 - val_loss: 0.8019 - val_accuracy: 0.4500\n",
      "Epoch 63/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6054 - accuracy: 0.7000 - val_loss: 0.8033 - val_accuracy: 0.4000\n",
      "Epoch 64/245\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6038 - accuracy: 0.6875 - val_loss: 0.8169 - val_accuracy: 0.4500\n",
      "Epoch 65/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5954 - accuracy: 0.6625 - val_loss: 0.8344 - val_accuracy: 0.4500\n",
      "Epoch 66/245\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6015 - accuracy: 0.6625 - val_loss: 0.8555 - val_accuracy: 0.4500\n",
      "Epoch 67/245\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5906 - accuracy: 0.6875 - val_loss: 0.8401 - val_accuracy: 0.4000\n",
      "Epoch 68/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5861 - accuracy: 0.7125 - val_loss: 0.8417 - val_accuracy: 0.4000\n",
      "Epoch 69/245\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5858 - accuracy: 0.6875 - val_loss: 0.8627 - val_accuracy: 0.4000\n",
      "Epoch 70/245\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.5743 - accuracy: 0.7000 - val_loss: 0.8985 - val_accuracy: 0.4000\n",
      "Epoch 71/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.5807 - accuracy: 0.6375 - val_loss: 0.9208 - val_accuracy: 0.4500\n",
      "Epoch 72/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5795 - accuracy: 0.6250 - val_loss: 0.8844 - val_accuracy: 0.4000\n",
      "Epoch 73/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5666 - accuracy: 0.6375 - val_loss: 0.8719 - val_accuracy: 0.4000\n",
      "Epoch 74/245\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5635 - accuracy: 0.6375 - val_loss: 0.9026 - val_accuracy: 0.3500\n",
      "Epoch 75/245\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.5570 - accuracy: 0.6750 - val_loss: 0.9372 - val_accuracy: 0.3500\n",
      "Epoch 76/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5591 - accuracy: 0.6625 - val_loss: 0.9471 - val_accuracy: 0.3500\n",
      "Epoch 77/245\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.5550 - accuracy: 0.6375 - val_loss: 0.9301 - val_accuracy: 0.3500\n",
      "Epoch 78/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5487 - accuracy: 0.6625 - val_loss: 0.9264 - val_accuracy: 0.4000\n",
      "Epoch 79/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5486 - accuracy: 0.6500 - val_loss: 0.9737 - val_accuracy: 0.4000\n",
      "Epoch 80/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5413 - accuracy: 0.6625 - val_loss: 0.9989 - val_accuracy: 0.3500\n",
      "Epoch 81/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5441 - accuracy: 0.6500 - val_loss: 1.0033 - val_accuracy: 0.4000\n",
      "Epoch 82/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5451 - accuracy: 0.6500 - val_loss: 0.9691 - val_accuracy: 0.4000\n",
      "Epoch 83/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5400 - accuracy: 0.6375 - val_loss: 1.0001 - val_accuracy: 0.4000\n",
      "Epoch 84/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5323 - accuracy: 0.6750 - val_loss: 1.0253 - val_accuracy: 0.4000\n",
      "Epoch 85/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5442 - accuracy: 0.6875 - val_loss: 1.0140 - val_accuracy: 0.4000\n",
      "Epoch 86/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5355 - accuracy: 0.6875 - val_loss: 0.9461 - val_accuracy: 0.4000\n",
      "Epoch 87/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5361 - accuracy: 0.6625 - val_loss: 0.9599 - val_accuracy: 0.4000\n",
      "Epoch 88/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5254 - accuracy: 0.6625 - val_loss: 0.9906 - val_accuracy: 0.3500\n",
      "Epoch 89/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5266 - accuracy: 0.7125 - val_loss: 0.9988 - val_accuracy: 0.3500\n",
      "Epoch 90/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5225 - accuracy: 0.7250 - val_loss: 0.9822 - val_accuracy: 0.3500\n",
      "Epoch 91/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5157 - accuracy: 0.7250 - val_loss: 0.9889 - val_accuracy: 0.3500\n",
      "Epoch 92/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5189 - accuracy: 0.6875 - val_loss: 0.9965 - val_accuracy: 0.4000\n",
      "Epoch 93/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.5130 - accuracy: 0.7125 - val_loss: 1.0095 - val_accuracy: 0.4000\n",
      "Epoch 94/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5131 - accuracy: 0.7250 - val_loss: 1.0263 - val_accuracy: 0.4000\n",
      "Epoch 95/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5106 - accuracy: 0.7000 - val_loss: 1.0291 - val_accuracy: 0.4000\n",
      "Epoch 96/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5039 - accuracy: 0.7125 - val_loss: 1.0200 - val_accuracy: 0.4000\n",
      "Epoch 97/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5046 - accuracy: 0.7125 - val_loss: 1.0328 - val_accuracy: 0.3500\n",
      "Epoch 98/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4988 - accuracy: 0.7000 - val_loss: 1.0209 - val_accuracy: 0.3500\n",
      "Epoch 99/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4957 - accuracy: 0.7250 - val_loss: 1.0214 - val_accuracy: 0.3500\n",
      "Epoch 100/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4945 - accuracy: 0.7500 - val_loss: 1.0398 - val_accuracy: 0.3500\n",
      "Epoch 101/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4896 - accuracy: 0.7500 - val_loss: 1.0468 - val_accuracy: 0.3500\n",
      "Epoch 102/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4872 - accuracy: 0.7375 - val_loss: 1.0577 - val_accuracy: 0.4000\n",
      "Epoch 103/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4874 - accuracy: 0.7250 - val_loss: 1.0859 - val_accuracy: 0.3500\n",
      "Epoch 104/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4828 - accuracy: 0.7750 - val_loss: 1.0959 - val_accuracy: 0.3500\n",
      "Epoch 105/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4854 - accuracy: 0.7500 - val_loss: 1.1139 - val_accuracy: 0.3500\n",
      "Epoch 106/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4902 - accuracy: 0.7500 - val_loss: 1.0593 - val_accuracy: 0.3500\n",
      "Epoch 107/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4799 - accuracy: 0.7500 - val_loss: 1.1078 - val_accuracy: 0.3500\n",
      "Epoch 108/245\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.4705 - accuracy: 0.7750 - val_loss: 1.1384 - val_accuracy: 0.3500\n",
      "Epoch 109/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4729 - accuracy: 0.7625 - val_loss: 1.1418 - val_accuracy: 0.3500\n",
      "Epoch 110/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4668 - accuracy: 0.7500 - val_loss: 1.0930 - val_accuracy: 0.4000\n",
      "Epoch 111/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4607 - accuracy: 0.7750 - val_loss: 1.0883 - val_accuracy: 0.4000\n",
      "Epoch 112/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4598 - accuracy: 0.7875 - val_loss: 1.1214 - val_accuracy: 0.3500\n",
      "Epoch 113/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4568 - accuracy: 0.7750 - val_loss: 1.1482 - val_accuracy: 0.3500\n",
      "Epoch 114/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4505 - accuracy: 0.7875 - val_loss: 1.1469 - val_accuracy: 0.3500\n",
      "Epoch 115/245\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.4500 - accuracy: 0.7625 - val_loss: 1.1464 - val_accuracy: 0.4000\n",
      "Epoch 116/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4487 - accuracy: 0.7625 - val_loss: 1.1727 - val_accuracy: 0.3500\n",
      "Epoch 117/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4442 - accuracy: 0.7875 - val_loss: 1.1764 - val_accuracy: 0.4000\n",
      "Epoch 118/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4379 - accuracy: 0.7875 - val_loss: 1.1885 - val_accuracy: 0.4000\n",
      "Epoch 119/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4356 - accuracy: 0.8000 - val_loss: 1.1551 - val_accuracy: 0.4000\n",
      "Epoch 120/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4339 - accuracy: 0.8000 - val_loss: 1.1424 - val_accuracy: 0.4000\n",
      "Epoch 121/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4269 - accuracy: 0.7750 - val_loss: 1.1653 - val_accuracy: 0.3500\n",
      "Epoch 122/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.4256 - accuracy: 0.7625 - val_loss: 1.1623 - val_accuracy: 0.4000\n",
      "Epoch 123/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4212 - accuracy: 0.7625 - val_loss: 1.1925 - val_accuracy: 0.4000\n",
      "Epoch 124/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4139 - accuracy: 0.7750 - val_loss: 1.1625 - val_accuracy: 0.4000\n",
      "Epoch 125/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4184 - accuracy: 0.7875 - val_loss: 1.2162 - val_accuracy: 0.4000\n",
      "Epoch 126/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4126 - accuracy: 0.8125 - val_loss: 1.2914 - val_accuracy: 0.4000\n",
      "Epoch 127/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4159 - accuracy: 0.8000 - val_loss: 1.2959 - val_accuracy: 0.4000\n",
      "Epoch 128/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4081 - accuracy: 0.8250 - val_loss: 1.2111 - val_accuracy: 0.4000\n",
      "Epoch 129/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4016 - accuracy: 0.8375 - val_loss: 1.1807 - val_accuracy: 0.4000\n",
      "Epoch 130/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3939 - accuracy: 0.8125 - val_loss: 1.2216 - val_accuracy: 0.4000\n",
      "Epoch 131/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.3940 - accuracy: 0.8250 - val_loss: 1.2623 - val_accuracy: 0.4000\n",
      "Epoch 132/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3886 - accuracy: 0.8125 - val_loss: 1.2581 - val_accuracy: 0.4000\n",
      "Epoch 133/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3779 - accuracy: 0.8250 - val_loss: 1.2374 - val_accuracy: 0.4000\n",
      "Epoch 134/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3831 - accuracy: 0.8500 - val_loss: 1.2402 - val_accuracy: 0.4000\n",
      "Epoch 135/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.3799 - accuracy: 0.8625 - val_loss: 1.2723 - val_accuracy: 0.4000\n",
      "Epoch 136/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3714 - accuracy: 0.8500 - val_loss: 1.2078 - val_accuracy: 0.4000\n",
      "Epoch 137/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3721 - accuracy: 0.8625 - val_loss: 1.1117 - val_accuracy: 0.4000\n",
      "Epoch 138/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3927 - accuracy: 0.8125 - val_loss: 1.1862 - val_accuracy: 0.3500\n",
      "Epoch 139/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3574 - accuracy: 0.8625 - val_loss: 1.4034 - val_accuracy: 0.4000\n",
      "Epoch 140/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4095 - accuracy: 0.8250 - val_loss: 1.3533 - val_accuracy: 0.4500\n",
      "Epoch 141/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.3590 - accuracy: 0.8500 - val_loss: 1.2283 - val_accuracy: 0.4500\n",
      "Epoch 142/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3554 - accuracy: 0.8500 - val_loss: 1.1802 - val_accuracy: 0.4500\n",
      "Epoch 143/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3481 - accuracy: 0.8500 - val_loss: 1.1835 - val_accuracy: 0.4500\n",
      "Epoch 144/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3364 - accuracy: 0.8500 - val_loss: 1.2448 - val_accuracy: 0.4500\n",
      "Epoch 145/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.3338 - accuracy: 0.8375 - val_loss: 1.3225 - val_accuracy: 0.4000\n",
      "Epoch 146/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3289 - accuracy: 0.8875 - val_loss: 1.3177 - val_accuracy: 0.4000\n",
      "Epoch 147/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3247 - accuracy: 0.8875 - val_loss: 1.2594 - val_accuracy: 0.4500\n",
      "Epoch 148/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.3150 - accuracy: 0.9000 - val_loss: 1.2624 - val_accuracy: 0.4500\n",
      "Epoch 149/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3145 - accuracy: 0.8625 - val_loss: 1.2684 - val_accuracy: 0.4500\n",
      "Epoch 150/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3073 - accuracy: 0.8875 - val_loss: 1.3062 - val_accuracy: 0.4500\n",
      "Epoch 151/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3005 - accuracy: 0.8750 - val_loss: 1.2961 - val_accuracy: 0.4000\n",
      "Epoch 152/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2957 - accuracy: 0.8750 - val_loss: 1.2239 - val_accuracy: 0.4000\n",
      "Epoch 153/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2922 - accuracy: 0.8875 - val_loss: 1.2467 - val_accuracy: 0.4500\n",
      "Epoch 154/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2881 - accuracy: 0.8875 - val_loss: 1.2909 - val_accuracy: 0.4500\n",
      "Epoch 155/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2859 - accuracy: 0.8750 - val_loss: 1.3025 - val_accuracy: 0.4000\n",
      "Epoch 156/245\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.2800 - accuracy: 0.8875 - val_loss: 1.2923 - val_accuracy: 0.4000\n",
      "Epoch 157/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2747 - accuracy: 0.9000 - val_loss: 1.3498 - val_accuracy: 0.4000\n",
      "Epoch 158/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2675 - accuracy: 0.9000 - val_loss: 1.4308 - val_accuracy: 0.4000\n",
      "Epoch 159/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2648 - accuracy: 0.9000 - val_loss: 1.4296 - val_accuracy: 0.4000\n",
      "Epoch 160/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2575 - accuracy: 0.9250 - val_loss: 1.3738 - val_accuracy: 0.4500\n",
      "Epoch 161/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2580 - accuracy: 0.9250 - val_loss: 1.3455 - val_accuracy: 0.5000\n",
      "Epoch 162/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2538 - accuracy: 0.9250 - val_loss: 1.3595 - val_accuracy: 0.4500\n",
      "Epoch 163/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2507 - accuracy: 0.9250 - val_loss: 1.3801 - val_accuracy: 0.4500\n",
      "Epoch 164/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2409 - accuracy: 0.9375 - val_loss: 1.4485 - val_accuracy: 0.4000\n",
      "Epoch 165/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2423 - accuracy: 0.9375 - val_loss: 1.5029 - val_accuracy: 0.4000\n",
      "Epoch 166/245\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.2340 - accuracy: 0.9375 - val_loss: 1.5346 - val_accuracy: 0.3500\n",
      "Epoch 167/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2438 - accuracy: 0.9125 - val_loss: 1.4742 - val_accuracy: 0.3500\n",
      "Epoch 168/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2237 - accuracy: 0.9375 - val_loss: 1.4132 - val_accuracy: 0.5000\n",
      "Epoch 169/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2295 - accuracy: 0.8875 - val_loss: 1.4529 - val_accuracy: 0.5000\n",
      "Epoch 170/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2158 - accuracy: 0.9250 - val_loss: 1.5234 - val_accuracy: 0.4000\n",
      "Epoch 171/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2143 - accuracy: 0.9500 - val_loss: 1.5550 - val_accuracy: 0.4000\n",
      "Epoch 172/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2043 - accuracy: 0.9375 - val_loss: 1.5518 - val_accuracy: 0.4500\n",
      "Epoch 173/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2074 - accuracy: 0.9250 - val_loss: 1.5913 - val_accuracy: 0.4000\n",
      "Epoch 174/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1991 - accuracy: 0.9250 - val_loss: 1.6457 - val_accuracy: 0.4000\n",
      "Epoch 175/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1995 - accuracy: 0.9375 - val_loss: 1.6058 - val_accuracy: 0.4000\n",
      "Epoch 176/245\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1898 - accuracy: 0.9500 - val_loss: 1.4792 - val_accuracy: 0.5000\n",
      "Epoch 177/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1929 - accuracy: 0.9375 - val_loss: 1.4737 - val_accuracy: 0.5000\n",
      "Epoch 178/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1834 - accuracy: 0.9375 - val_loss: 1.5247 - val_accuracy: 0.5000\n",
      "Epoch 179/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1753 - accuracy: 0.9500 - val_loss: 1.5580 - val_accuracy: 0.4500\n",
      "Epoch 180/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1677 - accuracy: 0.9500 - val_loss: 1.5924 - val_accuracy: 0.5000\n",
      "Epoch 181/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1670 - accuracy: 0.9125 - val_loss: 1.6032 - val_accuracy: 0.4000\n",
      "Epoch 182/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1619 - accuracy: 0.9500 - val_loss: 1.7158 - val_accuracy: 0.4000\n",
      "Epoch 183/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1545 - accuracy: 0.9625 - val_loss: 1.7345 - val_accuracy: 0.4500\n",
      "Epoch 184/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1482 - accuracy: 0.9750 - val_loss: 1.6348 - val_accuracy: 0.4500\n",
      "Epoch 185/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1480 - accuracy: 0.9625 - val_loss: 1.5771 - val_accuracy: 0.5000\n",
      "Epoch 186/245\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1512 - accuracy: 0.9500 - val_loss: 1.5451 - val_accuracy: 0.5000\n",
      "Epoch 187/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1385 - accuracy: 0.9500 - val_loss: 1.5966 - val_accuracy: 0.4500\n",
      "Epoch 188/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1318 - accuracy: 0.9750 - val_loss: 1.7285 - val_accuracy: 0.5000\n",
      "Epoch 189/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1269 - accuracy: 0.9625 - val_loss: 1.8721 - val_accuracy: 0.5000\n",
      "Epoch 190/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1243 - accuracy: 0.9750 - val_loss: 1.8773 - val_accuracy: 0.4500\n",
      "Epoch 191/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1174 - accuracy: 1.0000 - val_loss: 1.7938 - val_accuracy: 0.4500\n",
      "Epoch 192/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1163 - accuracy: 0.9875 - val_loss: 1.7622 - val_accuracy: 0.4500\n",
      "Epoch 193/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1138 - accuracy: 0.9875 - val_loss: 1.7655 - val_accuracy: 0.5000\n",
      "Epoch 194/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1078 - accuracy: 0.9875 - val_loss: 1.8003 - val_accuracy: 0.5000\n",
      "Epoch 195/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1077 - accuracy: 0.9875 - val_loss: 1.8173 - val_accuracy: 0.5000\n",
      "Epoch 196/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1004 - accuracy: 1.0000 - val_loss: 1.7745 - val_accuracy: 0.5000\n",
      "Epoch 197/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0938 - accuracy: 1.0000 - val_loss: 1.7193 - val_accuracy: 0.5000\n",
      "Epoch 198/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1031 - accuracy: 0.9875 - val_loss: 1.7697 - val_accuracy: 0.5000\n",
      "Epoch 199/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0921 - accuracy: 1.0000 - val_loss: 1.9000 - val_accuracy: 0.5000\n",
      "Epoch 200/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0834 - accuracy: 1.0000 - val_loss: 2.0653 - val_accuracy: 0.5000\n",
      "Epoch 201/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0872 - accuracy: 1.0000 - val_loss: 2.0239 - val_accuracy: 0.5000\n",
      "Epoch 202/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0847 - accuracy: 0.9875 - val_loss: 1.9029 - val_accuracy: 0.5000\n",
      "Epoch 203/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0781 - accuracy: 1.0000 - val_loss: 1.9418 - val_accuracy: 0.5000\n",
      "Epoch 204/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0716 - accuracy: 1.0000 - val_loss: 1.9958 - val_accuracy: 0.5000\n",
      "Epoch 205/245\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0733 - accuracy: 0.9875 - val_loss: 1.9269 - val_accuracy: 0.5000\n",
      "Epoch 206/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0715 - accuracy: 0.9875 - val_loss: 1.8490 - val_accuracy: 0.5500\n",
      "Epoch 207/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0664 - accuracy: 1.0000 - val_loss: 1.9039 - val_accuracy: 0.5500\n",
      "Epoch 208/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0669 - accuracy: 1.0000 - val_loss: 1.9711 - val_accuracy: 0.5000\n",
      "Epoch 209/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0621 - accuracy: 1.0000 - val_loss: 2.0604 - val_accuracy: 0.5000\n",
      "Epoch 210/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0628 - accuracy: 0.9875 - val_loss: 1.9415 - val_accuracy: 0.5000\n",
      "Epoch 211/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0557 - accuracy: 1.0000 - val_loss: 1.8797 - val_accuracy: 0.5000\n",
      "Epoch 212/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0554 - accuracy: 1.0000 - val_loss: 1.9816 - val_accuracy: 0.5000\n",
      "Epoch 213/245\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.0543 - accuracy: 1.0000 - val_loss: 2.1860 - val_accuracy: 0.5000\n",
      "Epoch 214/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0495 - accuracy: 1.0000 - val_loss: 2.2765 - val_accuracy: 0.5000\n",
      "Epoch 215/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0497 - accuracy: 1.0000 - val_loss: 2.2714 - val_accuracy: 0.5000\n",
      "Epoch 216/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0467 - accuracy: 1.0000 - val_loss: 2.1845 - val_accuracy: 0.5000\n",
      "Epoch 217/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0458 - accuracy: 1.0000 - val_loss: 2.0211 - val_accuracy: 0.5000\n",
      "Epoch 218/245\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0451 - accuracy: 1.0000 - val_loss: 2.0160 - val_accuracy: 0.5000\n",
      "Epoch 219/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 2.1107 - val_accuracy: 0.5000\n",
      "Epoch 220/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0420 - accuracy: 1.0000 - val_loss: 2.2852 - val_accuracy: 0.5000\n",
      "Epoch 221/245\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.0393 - accuracy: 1.0000 - val_loss: 2.3259 - val_accuracy: 0.5000\n",
      "Epoch 222/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0398 - accuracy: 1.0000 - val_loss: 2.3439 - val_accuracy: 0.5000\n",
      "Epoch 223/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0379 - accuracy: 1.0000 - val_loss: 2.3663 - val_accuracy: 0.5000\n",
      "Epoch 224/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 2.3121 - val_accuracy: 0.5000\n",
      "Epoch 225/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 2.1450 - val_accuracy: 0.5000\n",
      "Epoch 226/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 2.1011 - val_accuracy: 0.5000\n",
      "Epoch 227/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0325 - accuracy: 1.0000 - val_loss: 2.1083 - val_accuracy: 0.5000\n",
      "Epoch 228/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0306 - accuracy: 1.0000 - val_loss: 2.2084 - val_accuracy: 0.5000\n",
      "Epoch 229/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 2.2460 - val_accuracy: 0.5000\n",
      "Epoch 230/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 2.3271 - val_accuracy: 0.5000\n",
      "Epoch 231/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 2.3700 - val_accuracy: 0.5000\n",
      "Epoch 232/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 2.3520 - val_accuracy: 0.5000\n",
      "Epoch 233/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 2.3588 - val_accuracy: 0.5000\n",
      "Epoch 234/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0248 - accuracy: 1.0000 - val_loss: 2.2895 - val_accuracy: 0.5000\n",
      "Epoch 235/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 2.2824 - val_accuracy: 0.5000\n",
      "Epoch 236/245\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0232 - accuracy: 1.0000 - val_loss: 2.2971 - val_accuracy: 0.5000\n",
      "Epoch 237/245\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 2.3082 - val_accuracy: 0.5000\n",
      "Epoch 238/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 2.2956 - val_accuracy: 0.5500\n",
      "Epoch 239/245\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 2.3198 - val_accuracy: 0.5000\n",
      "Epoch 240/245\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 2.3708 - val_accuracy: 0.5000\n",
      "Epoch 241/245\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 2.3429 - val_accuracy: 0.5500\n",
      "Epoch 242/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 2.3119 - val_accuracy: 0.5500\n",
      "Epoch 243/245\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 2.3386 - val_accuracy: 0.5500\n",
      "Epoch 244/245\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 2.3456 - val_accuracy: 0.5500\n",
      "Epoch 245/245\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 2.4206 - val_accuracy: 0.5500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27963e55390>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from  tensorflow . keras.models import Sequential\n",
    "from  tensorflow . keras.layers import Bidirectional,LSTM, Dense\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv('Clinical data ppf and eph.csv')\n",
    "\n",
    "# Split the data into input (X) and output (y) samples\n",
    "X = data.drop(columns=['intraop_ppf', 'intraop_eph'])\n",
    "y = data[['intraop_ppf', 'intraop_eph']]\n",
    "\n",
    "# Convert non-numeric values to NaN for each column in X\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "# Convert non-numeric values to NaN for each column in y\n",
    "for col in y.columns:\n",
    "    y[col] = pd.to_numeric(y[col], errors='coerce')\n",
    "\n",
    "# Fill NaN values with a specific value (e.g., 0) in X and y\n",
    "X.fillna(0, inplace=True)\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Convert X to a numpy array\n",
    "X = X.to_numpy()\n",
    "\n",
    "# Fill NaN values in y with 0\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the input data to be 3D for BI-LSTM input [samples, timesteps, features]\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "X_train = np.random.randn(100, 10, 1)  \n",
    "y_train = np.random.randint(0, 2, size=(100,)) \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False), input_shape=(10, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=245, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6718369-47a8-49b5-905c-1dc55731e086",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 10, 1), found shape=(None, 42)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert the predicted probabilities to binary predictions (0 or 1)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m y_pred_binary \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileov1sw5v8.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 10, 1), found shape=(None, 42)\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert the predicted probabilities to binary predictions (0 or 1)\n",
    "y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Calculate precision for each class\n",
    "precision = precision_score(y_test, y_pred_binary, average=None)\n",
    "\n",
    "# Print the precision for each class\n",
    "print(\"Precision for intraop_ppf:\", precision[0])\n",
    "print(\"Precision for intraop_eph:\", precision[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a61443f3-95e8-4cbc-846d-2d4e2faa95ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 10, 1), found shape=(None, 42, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Convert the predicted probabilities to binary predictions (0 or 1)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y_pred_binary \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y_pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileov1sw5v8.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\varsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 10, 1), found shape=(None, 42, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the input data to be 3D for BI-LSTM input [samples, timesteps, features]\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert the predicted probabilities to binary predictions (0 or 1)\n",
    "y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Calculate precision for each class\n",
    "precision = precision_score(y_test, y_pred_binary, average=None)\n",
    "\n",
    "# Print the precision for each class\n",
    "print(\"Precision for intraop_ppf:\", precision[0])\n",
    "print(\"Precision for intraop_eph:\", precision[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a6cf44-ebea-4fa5-b4a4-41ff1c9837e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 53676 into shape (1278,10,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Reshape the input data to be 3D for BI-LSTM input [samples, timesteps, features]\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_test \u001b[38;5;241m=\u001b[39m \u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 53676 into shape (1278,10,4)"
     ]
    }
   ],
   "source": [
    "# Reshape the input data to be 3D for BI-LSTM input [samples, timesteps, features]\n",
    "X_test = X_test.reshape(X_test.shape[0], 10, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a93dbac-db62-489a-841a-5e41efe39970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input data to be 3D for BI-LSTM input [samples, timesteps, features]\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0185af32-c20f-47b3-b44a-e8d1a60fbecc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Convert the predicted probabilities to binary predictions (0 or 1)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m y_pred_binary \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[43my_pred\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate precision for each class\u001b[39;00m\n\u001b[0;32m     10\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(y_test, y_pred_binary, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Reshape the input data to be 3D for BI-LSTM input [samples, timesteps, features]\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "\n",
    "\n",
    "# Convert the predicted probabilities to binary predictions (0 or 1)\n",
    "y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Calculate precision for each class\n",
    "precision = precision_score(y_test, y_pred_binary, average=None)\n",
    "\n",
    "# Print the precision for each class\n",
    "print(\"Precision for intraop_ppf:\", precision[0])\n",
    "print(\"Precision for intraop_eph:\", precision[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d661ad0-1f1c-40cd-9e1d-80c1a411d23f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Reshape y_train and y_pred to have shape (n_samples, 2)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m y_train_reshaped \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Assuming y_train is a 1D array with shape (n_samples,)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m y_pred_reshaped \u001b[38;5;241m=\u001b[39m \u001b[43my_pred\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Assuming y_pred is a 1D array with shape (n_samples,)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Concatenate y_train_reshaped and y_pred_reshaped along the second axis to create a (n_samples, 2) array\u001b[39;00m\n\u001b[0;32m      6\u001b[0m y_train_combined \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((y_train_reshaped, y_pred_reshaped), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Reshape y_train and y_pred to have shape (n_samples, 2)\n",
    "y_train_reshaped = y_train.reshape(-1, 1)  # Assuming y_train is a 1D array with shape (n_samples,)\n",
    "y_pred_reshaped = y_pred.reshape(-1, 1)  # Assuming y_pred is a 1D array with shape (n_samples,)\n",
    "\n",
    "# Concatenate y_train_reshaped and y_pred_reshaped along the second axis to create a (n_samples, 2) array\n",
    "y_train_combined = np.concatenate((y_train_reshaped, y_pred_reshaped), axis=1)\n",
    "\n",
    "# Calculate precision for each output parameter separately\n",
    "precision_0 = precision_score(y_train_combined[:, 0], y_train_combined[:, 1])\n",
    "precision_1 = precision_score(y_train_combined[:, 0], y_train_combined[:, 1])\n",
    "\n",
    "print(\"Precision for output parameter 0:\", precision_0)\n",
    "print(\"Precision for output parameter 1:\", precision_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f659f2c6-d0b3-4fdf-b39d-2c91537ec27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 546ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 100 and the array at index 1 has size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m y_pred_reshaped \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)    \u001b[38;5;66;03m# Assuming y_pred is a 1D array with shape (n_samples,)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Concatenate y_train_reshaped and y_pred_reshaped along the second axis to create a (n_samples, 2) array\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m y_train_combined \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_reshaped\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Print the combined array\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_train_combined)\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 100 and the array at index 1 has size 10"
     ]
    }
   ],
   "source": [
    "# Assuming you have a test set X_test\n",
    "X_test = np.random.randn(10, 10, 1)  # Example data, replace with your actual test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Reshape y_train and y_pred to have shape (n_samples, 2)\n",
    "y_train_reshaped = y_train.reshape(-1, 1)  # Assuming y_train is a 1D array with shape (n_samples,)\n",
    "y_pred_reshaped = y_pred.reshape(-1, 1)    # Assuming y_pred is a 1D array with shape (n_samples,)\n",
    "\n",
    "# Concatenate y_train_reshaped and y_pred_reshaped along the second axis to create a (n_samples, 2) array\n",
    "y_train_combined = np.concatenate((y_train_reshaped, y_pred_reshaped), axis=1)\n",
    "\n",
    "# Print the combined array\n",
    "print(y_train_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d45d3a06-2d2e-4136-84dc-68e93578617c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_18948\\692645778.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[col] = pd.to_numeric(y[col], errors='coerce')\n",
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_18948\\692645778.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y.fillna(0, inplace=True)\n",
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_18948\\692645778.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y.fillna(0, inplace=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18948\\692645778.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Example data, replace with your actual test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# Reshape y_train and y_pred to have shape (n_samples, 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0my_train_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Assuming y_train is a 1D array with shape (n_samples,)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[0my_pred_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# Assuming y_pred is a 1D array with shape (n_samples,)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;31m# Concatenate y_train_reshaped and y_pred_reshaped along the second axis to create a (n_samples, 2) array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6289\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6290\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6291\u001b[0m         ):\n\u001b[0;32m   6292\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6293\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from  tensorflow . keras.models import Sequential\n",
    "from  tensorflow . keras.layers import Bidirectional,LSTM, Dense\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv('Clinical data ppf and eph.csv')\n",
    "\n",
    "# Split the data into input (X) and output (y) samples\n",
    "X = data.drop(columns=['intraop_ppf', 'intraop_eph'])\n",
    "y = data[['intraop_ppf', 'intraop_eph']]\n",
    "\n",
    "# Convert non-numeric values to NaN for each column in X\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "# Convert non-numeric values to NaN for each column in y\n",
    "for col in y.columns:\n",
    "    y[col] = pd.to_numeric(y[col], errors='coerce')\n",
    "\n",
    "# Fill NaN values with a specific value (e.g., 0) in X and y\n",
    "X.fillna(0, inplace=True)\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Convert X to a numpy array\n",
    "X = X.to_numpy()\n",
    "\n",
    "# Fill NaN values in y with 0\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Assuming X_test has the same number of samples as X_train\n",
    "X_test = np.random.randn(100, 10, 1)  # Example data, replace with your actual test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Reshape y_train and y_pred to have shape (n_samples, 2)\n",
    "y_train_reshaped = y_train.reshape(-1, 1)  # Assuming y_train is a 1D array with shape (n_samples,)\n",
    "y_pred_reshaped = y_pred.reshape(-1, 1)    # Assuming y_pred is a 1D array with shape (n_samples,)\n",
    "\n",
    "# Concatenate y_train_reshaped and y_pred_reshaped along the second axis to create a (n_samples, 2) array\n",
    "y_train_combined = np.concatenate((y_train_reshaped, y_pred_reshaped), axis=1)\n",
    "\n",
    "# Print the combined array\n",
    "print(y_train_combined)\n",
    "\n",
    "\n",
    "# Reshape the input data to be 3D for BI-LSTM input [samples, timesteps, features]\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "X_train = np.random.randn(100, 10, 1)  \n",
    "y_train = np.random.randint(0, 2, size=(100,)) \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False), input_shape=(10, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=245, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56a16d8a-f4b5-4fc4-8635-4ec06fe6cb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_18948\\3831536639.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[col] = pd.to_numeric(y[col], errors='coerce')\n",
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_18948\\3831536639.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y.fillna(0, inplace=True)\n",
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_18948\\3831536639.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y.fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/225\n",
      "3/3 [==============================] - 3s 313ms/step - loss: 0.6938 - accuracy: 0.5375 - val_loss: 0.6909 - val_accuracy: 0.4500\n",
      "Epoch 2/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6935 - accuracy: 0.5750 - val_loss: 0.6888 - val_accuracy: 0.5000\n",
      "Epoch 3/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6928 - accuracy: 0.5375 - val_loss: 0.6892 - val_accuracy: 0.4500\n",
      "Epoch 4/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6924 - accuracy: 0.5125 - val_loss: 0.6893 - val_accuracy: 0.5500\n",
      "Epoch 5/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6922 - accuracy: 0.5000 - val_loss: 0.6897 - val_accuracy: 0.5500\n",
      "Epoch 6/225\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6920 - accuracy: 0.4875 - val_loss: 0.6895 - val_accuracy: 0.5500\n",
      "Epoch 7/225\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6917 - accuracy: 0.5125 - val_loss: 0.6881 - val_accuracy: 0.7000\n",
      "Epoch 8/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6914 - accuracy: 0.5125 - val_loss: 0.6869 - val_accuracy: 0.7000\n",
      "Epoch 9/225\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6921 - accuracy: 0.4875 - val_loss: 0.6853 - val_accuracy: 0.7000\n",
      "Epoch 10/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6913 - accuracy: 0.5250 - val_loss: 0.6854 - val_accuracy: 0.5500\n",
      "Epoch 11/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6909 - accuracy: 0.5250 - val_loss: 0.6854 - val_accuracy: 0.6000\n",
      "Epoch 12/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6907 - accuracy: 0.5250 - val_loss: 0.6847 - val_accuracy: 0.6500\n",
      "Epoch 13/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6907 - accuracy: 0.5000 - val_loss: 0.6842 - val_accuracy: 0.6500\n",
      "Epoch 14/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6909 - accuracy: 0.5000 - val_loss: 0.6827 - val_accuracy: 0.7000\n",
      "Epoch 15/225\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.6901 - accuracy: 0.5125 - val_loss: 0.6825 - val_accuracy: 0.6500\n",
      "Epoch 16/225\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6900 - accuracy: 0.5000 - val_loss: 0.6820 - val_accuracy: 0.6000\n",
      "Epoch 17/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6895 - accuracy: 0.5125 - val_loss: 0.6808 - val_accuracy: 0.6500\n",
      "Epoch 18/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6893 - accuracy: 0.5125 - val_loss: 0.6798 - val_accuracy: 0.7000\n",
      "Epoch 19/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6890 - accuracy: 0.5250 - val_loss: 0.6788 - val_accuracy: 0.7000\n",
      "Epoch 20/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6886 - accuracy: 0.5125 - val_loss: 0.6779 - val_accuracy: 0.7000\n",
      "Epoch 21/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6884 - accuracy: 0.5000 - val_loss: 0.6767 - val_accuracy: 0.7000\n",
      "Epoch 22/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6883 - accuracy: 0.5250 - val_loss: 0.6757 - val_accuracy: 0.7000\n",
      "Epoch 23/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6877 - accuracy: 0.5250 - val_loss: 0.6757 - val_accuracy: 0.7000\n",
      "Epoch 24/225\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6877 - accuracy: 0.5250 - val_loss: 0.6760 - val_accuracy: 0.7000\n",
      "Epoch 25/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6869 - accuracy: 0.5500 - val_loss: 0.6754 - val_accuracy: 0.7000\n",
      "Epoch 26/225\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6865 - accuracy: 0.5750 - val_loss: 0.6739 - val_accuracy: 0.7000\n",
      "Epoch 27/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6861 - accuracy: 0.5500 - val_loss: 0.6722 - val_accuracy: 0.7000\n",
      "Epoch 28/225\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6858 - accuracy: 0.5500 - val_loss: 0.6714 - val_accuracy: 0.7000\n",
      "Epoch 29/225\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6850 - accuracy: 0.5375 - val_loss: 0.6706 - val_accuracy: 0.7000\n",
      "Epoch 30/225\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6844 - accuracy: 0.5500 - val_loss: 0.6709 - val_accuracy: 0.7000\n",
      "Epoch 31/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6833 - accuracy: 0.5750 - val_loss: 0.6691 - val_accuracy: 0.7000\n",
      "Epoch 32/225\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6829 - accuracy: 0.5625 - val_loss: 0.6662 - val_accuracy: 0.7000\n",
      "Epoch 33/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6817 - accuracy: 0.5500 - val_loss: 0.6632 - val_accuracy: 0.7000\n",
      "Epoch 34/225\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6810 - accuracy: 0.5500 - val_loss: 0.6595 - val_accuracy: 0.7000\n",
      "Epoch 35/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6804 - accuracy: 0.5750 - val_loss: 0.6566 - val_accuracy: 0.7000\n",
      "Epoch 36/225\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.6791 - accuracy: 0.5625 - val_loss: 0.6546 - val_accuracy: 0.7000\n",
      "Epoch 37/225\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6782 - accuracy: 0.5625 - val_loss: 0.6553 - val_accuracy: 0.7000\n",
      "Epoch 38/225\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6768 - accuracy: 0.5625 - val_loss: 0.6534 - val_accuracy: 0.7000\n",
      "Epoch 39/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6746 - accuracy: 0.5625 - val_loss: 0.6541 - val_accuracy: 0.7000\n",
      "Epoch 40/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6777 - accuracy: 0.5625 - val_loss: 0.6571 - val_accuracy: 0.6000\n",
      "Epoch 41/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6748 - accuracy: 0.5500 - val_loss: 0.6513 - val_accuracy: 0.6000\n",
      "Epoch 42/225\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6714 - accuracy: 0.5625 - val_loss: 0.6468 - val_accuracy: 0.5500\n",
      "Epoch 43/225\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6697 - accuracy: 0.5625 - val_loss: 0.6426 - val_accuracy: 0.5500\n",
      "Epoch 44/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6708 - accuracy: 0.5750 - val_loss: 0.6384 - val_accuracy: 0.5500\n",
      "Epoch 45/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6721 - accuracy: 0.5875 - val_loss: 0.6362 - val_accuracy: 0.5500\n",
      "Epoch 46/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6700 - accuracy: 0.5875 - val_loss: 0.6335 - val_accuracy: 0.5500\n",
      "Epoch 47/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6692 - accuracy: 0.6000 - val_loss: 0.6297 - val_accuracy: 0.5500\n",
      "Epoch 48/225\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6652 - accuracy: 0.5875 - val_loss: 0.6325 - val_accuracy: 0.5500\n",
      "Epoch 49/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6641 - accuracy: 0.5625 - val_loss: 0.6361 - val_accuracy: 0.5500\n",
      "Epoch 50/225\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6675 - accuracy: 0.5375 - val_loss: 0.6339 - val_accuracy: 0.5500\n",
      "Epoch 51/225\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6617 - accuracy: 0.5250 - val_loss: 0.6271 - val_accuracy: 0.5500\n",
      "Epoch 52/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6589 - accuracy: 0.5500 - val_loss: 0.6221 - val_accuracy: 0.6000\n",
      "Epoch 53/225\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6612 - accuracy: 0.5875 - val_loss: 0.6184 - val_accuracy: 0.6500\n",
      "Epoch 54/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6640 - accuracy: 0.5875 - val_loss: 0.6169 - val_accuracy: 0.6500\n",
      "Epoch 55/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6594 - accuracy: 0.5625 - val_loss: 0.6264 - val_accuracy: 0.7000\n",
      "Epoch 56/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6541 - accuracy: 0.5625 - val_loss: 0.6327 - val_accuracy: 0.6000\n",
      "Epoch 57/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6559 - accuracy: 0.5625 - val_loss: 0.6365 - val_accuracy: 0.6000\n",
      "Epoch 58/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6539 - accuracy: 0.5625 - val_loss: 0.6321 - val_accuracy: 0.6000\n",
      "Epoch 59/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6525 - accuracy: 0.5625 - val_loss: 0.6321 - val_accuracy: 0.5500\n",
      "Epoch 60/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6506 - accuracy: 0.5875 - val_loss: 0.6318 - val_accuracy: 0.6000\n",
      "Epoch 61/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6472 - accuracy: 0.5875 - val_loss: 0.6338 - val_accuracy: 0.6000\n",
      "Epoch 62/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6455 - accuracy: 0.5750 - val_loss: 0.6330 - val_accuracy: 0.6000\n",
      "Epoch 63/225\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6431 - accuracy: 0.5875 - val_loss: 0.6318 - val_accuracy: 0.6000\n",
      "Epoch 64/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6405 - accuracy: 0.6000 - val_loss: 0.6304 - val_accuracy: 0.6000\n",
      "Epoch 65/225\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6375 - accuracy: 0.6250 - val_loss: 0.6331 - val_accuracy: 0.6000\n",
      "Epoch 66/225\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6338 - accuracy: 0.6250 - val_loss: 0.6352 - val_accuracy: 0.6000\n",
      "Epoch 67/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6322 - accuracy: 0.6375 - val_loss: 0.6420 - val_accuracy: 0.6000\n",
      "Epoch 68/225\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6274 - accuracy: 0.6250 - val_loss: 0.6490 - val_accuracy: 0.6000\n",
      "Epoch 69/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6233 - accuracy: 0.6375 - val_loss: 0.6607 - val_accuracy: 0.6000\n",
      "Epoch 70/225\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6233 - accuracy: 0.6250 - val_loss: 0.6680 - val_accuracy: 0.6000\n",
      "Epoch 71/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6170 - accuracy: 0.6375 - val_loss: 0.6718 - val_accuracy: 0.6000\n",
      "Epoch 72/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6203 - accuracy: 0.6500 - val_loss: 0.6759 - val_accuracy: 0.5500\n",
      "Epoch 73/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6153 - accuracy: 0.6500 - val_loss: 0.6846 - val_accuracy: 0.6000\n",
      "Epoch 74/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6213 - accuracy: 0.6500 - val_loss: 0.6875 - val_accuracy: 0.6000\n",
      "Epoch 75/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6010 - accuracy: 0.6500 - val_loss: 0.6705 - val_accuracy: 0.6500\n",
      "Epoch 76/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6182 - accuracy: 0.6125 - val_loss: 0.6746 - val_accuracy: 0.6000\n",
      "Epoch 77/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6012 - accuracy: 0.6375 - val_loss: 0.6991 - val_accuracy: 0.6000\n",
      "Epoch 78/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6035 - accuracy: 0.6375 - val_loss: 0.7101 - val_accuracy: 0.5500\n",
      "Epoch 79/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6015 - accuracy: 0.6375 - val_loss: 0.6853 - val_accuracy: 0.5500\n",
      "Epoch 80/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5862 - accuracy: 0.6625 - val_loss: 0.6736 - val_accuracy: 0.5500\n",
      "Epoch 81/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5989 - accuracy: 0.6500 - val_loss: 0.6732 - val_accuracy: 0.5000\n",
      "Epoch 82/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5866 - accuracy: 0.6625 - val_loss: 0.6943 - val_accuracy: 0.6000\n",
      "Epoch 83/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5782 - accuracy: 0.6625 - val_loss: 0.7141 - val_accuracy: 0.6000\n",
      "Epoch 84/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5861 - accuracy: 0.6625 - val_loss: 0.6997 - val_accuracy: 0.5500\n",
      "Epoch 85/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5707 - accuracy: 0.6875 - val_loss: 0.6869 - val_accuracy: 0.5500\n",
      "Epoch 86/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5772 - accuracy: 0.6625 - val_loss: 0.6846 - val_accuracy: 0.5000\n",
      "Epoch 87/225\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5720 - accuracy: 0.6500 - val_loss: 0.6979 - val_accuracy: 0.6000\n",
      "Epoch 88/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5667 - accuracy: 0.6875 - val_loss: 0.7332 - val_accuracy: 0.6000\n",
      "Epoch 89/225\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5741 - accuracy: 0.6500 - val_loss: 0.7041 - val_accuracy: 0.6000\n",
      "Epoch 90/225\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.5564 - accuracy: 0.7000 - val_loss: 0.6802 - val_accuracy: 0.5500\n",
      "Epoch 91/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5768 - accuracy: 0.6625 - val_loss: 0.6849 - val_accuracy: 0.6000\n",
      "Epoch 92/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5530 - accuracy: 0.6875 - val_loss: 0.7436 - val_accuracy: 0.6000\n",
      "Epoch 93/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5678 - accuracy: 0.6500 - val_loss: 0.7315 - val_accuracy: 0.6000\n",
      "Epoch 94/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5389 - accuracy: 0.7125 - val_loss: 0.7034 - val_accuracy: 0.6000\n",
      "Epoch 95/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5602 - accuracy: 0.6625 - val_loss: 0.7070 - val_accuracy: 0.6000\n",
      "Epoch 96/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5376 - accuracy: 0.6875 - val_loss: 0.7548 - val_accuracy: 0.6500\n",
      "Epoch 97/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5497 - accuracy: 0.6750 - val_loss: 0.7833 - val_accuracy: 0.6000\n",
      "Epoch 98/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5530 - accuracy: 0.6625 - val_loss: 0.7787 - val_accuracy: 0.6000\n",
      "Epoch 99/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5491 - accuracy: 0.6875 - val_loss: 0.7493 - val_accuracy: 0.6000\n",
      "Epoch 100/225\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5626 - accuracy: 0.6625 - val_loss: 0.7396 - val_accuracy: 0.6000\n",
      "Epoch 101/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5537 - accuracy: 0.7000 - val_loss: 0.7532 - val_accuracy: 0.6000\n",
      "Epoch 102/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5345 - accuracy: 0.6875 - val_loss: 0.7474 - val_accuracy: 0.6500\n",
      "Epoch 103/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5359 - accuracy: 0.7000 - val_loss: 0.7220 - val_accuracy: 0.6000\n",
      "Epoch 104/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5272 - accuracy: 0.6750 - val_loss: 0.6962 - val_accuracy: 0.6500\n",
      "Epoch 105/225\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.5380 - accuracy: 0.7000 - val_loss: 0.6917 - val_accuracy: 0.6500\n",
      "Epoch 106/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5210 - accuracy: 0.7125 - val_loss: 0.7199 - val_accuracy: 0.7000\n",
      "Epoch 107/225\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.5250 - accuracy: 0.6875 - val_loss: 0.7210 - val_accuracy: 0.7000\n",
      "Epoch 108/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5152 - accuracy: 0.7000 - val_loss: 0.7102 - val_accuracy: 0.7000\n",
      "Epoch 109/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5162 - accuracy: 0.6750 - val_loss: 0.7229 - val_accuracy: 0.6500\n",
      "Epoch 110/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5092 - accuracy: 0.7250 - val_loss: 0.7479 - val_accuracy: 0.6500\n",
      "Epoch 111/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5029 - accuracy: 0.7250 - val_loss: 0.7605 - val_accuracy: 0.7000\n",
      "Epoch 112/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4961 - accuracy: 0.7375 - val_loss: 0.7532 - val_accuracy: 0.7000\n",
      "Epoch 113/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4989 - accuracy: 0.7250 - val_loss: 0.7626 - val_accuracy: 0.6000\n",
      "Epoch 114/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4877 - accuracy: 0.7375 - val_loss: 0.7942 - val_accuracy: 0.6500\n",
      "Epoch 115/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4952 - accuracy: 0.7125 - val_loss: 0.7818 - val_accuracy: 0.6500\n",
      "Epoch 116/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4819 - accuracy: 0.7375 - val_loss: 0.7688 - val_accuracy: 0.5500\n",
      "Epoch 117/225\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4819 - accuracy: 0.7250 - val_loss: 0.7836 - val_accuracy: 0.6000\n",
      "Epoch 118/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4747 - accuracy: 0.7250 - val_loss: 0.8077 - val_accuracy: 0.6000\n",
      "Epoch 119/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4739 - accuracy: 0.7375 - val_loss: 0.7930 - val_accuracy: 0.6000\n",
      "Epoch 120/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4705 - accuracy: 0.7250 - val_loss: 0.7797 - val_accuracy: 0.5500\n",
      "Epoch 121/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4682 - accuracy: 0.7250 - val_loss: 0.7958 - val_accuracy: 0.6000\n",
      "Epoch 122/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4645 - accuracy: 0.7250 - val_loss: 0.7894 - val_accuracy: 0.6000\n",
      "Epoch 123/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4601 - accuracy: 0.7375 - val_loss: 0.7889 - val_accuracy: 0.6000\n",
      "Epoch 124/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4604 - accuracy: 0.7500 - val_loss: 0.7680 - val_accuracy: 0.6500\n",
      "Epoch 125/225\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4592 - accuracy: 0.7500 - val_loss: 0.7707 - val_accuracy: 0.6500\n",
      "Epoch 126/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4526 - accuracy: 0.7750 - val_loss: 0.7849 - val_accuracy: 0.6500\n",
      "Epoch 127/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4485 - accuracy: 0.7750 - val_loss: 0.7650 - val_accuracy: 0.7000\n",
      "Epoch 128/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4447 - accuracy: 0.7750 - val_loss: 0.7637 - val_accuracy: 0.7500\n",
      "Epoch 129/225\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4416 - accuracy: 0.7500 - val_loss: 0.7593 - val_accuracy: 0.7000\n",
      "Epoch 130/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4390 - accuracy: 0.7625 - val_loss: 0.7755 - val_accuracy: 0.6500\n",
      "Epoch 131/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4454 - accuracy: 0.7250 - val_loss: 0.7546 - val_accuracy: 0.6500\n",
      "Epoch 132/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4323 - accuracy: 0.7625 - val_loss: 0.7404 - val_accuracy: 0.7000\n",
      "Epoch 133/225\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4371 - accuracy: 0.7500 - val_loss: 0.7648 - val_accuracy: 0.6500\n",
      "Epoch 134/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4238 - accuracy: 0.7750 - val_loss: 0.8392 - val_accuracy: 0.5500\n",
      "Epoch 135/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4479 - accuracy: 0.7500 - val_loss: 0.8547 - val_accuracy: 0.5500\n",
      "Epoch 136/225\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4526 - accuracy: 0.7250 - val_loss: 0.8683 - val_accuracy: 0.5500\n",
      "Epoch 137/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4388 - accuracy: 0.7500 - val_loss: 0.9803 - val_accuracy: 0.5000\n",
      "Epoch 138/225\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.5412 - accuracy: 0.7000 - val_loss: 0.8068 - val_accuracy: 0.7000\n",
      "Epoch 139/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4157 - accuracy: 0.7250 - val_loss: 0.7716 - val_accuracy: 0.7500\n",
      "Epoch 140/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4112 - accuracy: 0.8000 - val_loss: 0.8163 - val_accuracy: 0.6500\n",
      "Epoch 141/225\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.4679 - accuracy: 0.7500 - val_loss: 0.7647 - val_accuracy: 0.7000\n",
      "Epoch 142/225\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4280 - accuracy: 0.7750 - val_loss: 0.7592 - val_accuracy: 0.6500\n",
      "Epoch 143/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4336 - accuracy: 0.7375 - val_loss: 0.7525 - val_accuracy: 0.7000\n",
      "Epoch 144/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4117 - accuracy: 0.7625 - val_loss: 0.7685 - val_accuracy: 0.7000\n",
      "Epoch 145/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.4157 - accuracy: 0.8125 - val_loss: 0.7697 - val_accuracy: 0.6500\n",
      "Epoch 146/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3813 - accuracy: 0.8000 - val_loss: 0.8243 - val_accuracy: 0.7000\n",
      "Epoch 147/225\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.4103 - accuracy: 0.7750 - val_loss: 0.8673 - val_accuracy: 0.6000\n",
      "Epoch 148/225\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.4179 - accuracy: 0.8000 - val_loss: 0.8293 - val_accuracy: 0.7000\n",
      "Epoch 149/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3811 - accuracy: 0.8125 - val_loss: 0.7834 - val_accuracy: 0.7000\n",
      "Epoch 150/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3617 - accuracy: 0.8125 - val_loss: 0.8035 - val_accuracy: 0.6500\n",
      "Epoch 151/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3919 - accuracy: 0.7750 - val_loss: 0.7777 - val_accuracy: 0.7000\n",
      "Epoch 152/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3522 - accuracy: 0.8125 - val_loss: 0.8126 - val_accuracy: 0.6000\n",
      "Epoch 153/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3770 - accuracy: 0.8000 - val_loss: 0.8184 - val_accuracy: 0.6000\n",
      "Epoch 154/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3518 - accuracy: 0.8125 - val_loss: 0.7921 - val_accuracy: 0.7000\n",
      "Epoch 155/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3752 - accuracy: 0.7875 - val_loss: 0.7877 - val_accuracy: 0.7000\n",
      "Epoch 156/225\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.3713 - accuracy: 0.7875 - val_loss: 0.8391 - val_accuracy: 0.7000\n",
      "Epoch 157/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3849 - accuracy: 0.7500 - val_loss: 0.8926 - val_accuracy: 0.6000\n",
      "Epoch 158/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3714 - accuracy: 0.7875 - val_loss: 0.8977 - val_accuracy: 0.6000\n",
      "Epoch 159/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3719 - accuracy: 0.7875 - val_loss: 0.8752 - val_accuracy: 0.6500\n",
      "Epoch 160/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3630 - accuracy: 0.7875 - val_loss: 0.8515 - val_accuracy: 0.7000\n",
      "Epoch 161/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3616 - accuracy: 0.8375 - val_loss: 0.8184 - val_accuracy: 0.6500\n",
      "Epoch 162/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3317 - accuracy: 0.8250 - val_loss: 0.8272 - val_accuracy: 0.6000\n",
      "Epoch 163/225\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.3622 - accuracy: 0.7875 - val_loss: 0.8312 - val_accuracy: 0.5500\n",
      "Epoch 164/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3375 - accuracy: 0.8250 - val_loss: 0.8256 - val_accuracy: 0.6500\n",
      "Epoch 165/225\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3217 - accuracy: 0.8375 - val_loss: 0.8327 - val_accuracy: 0.6500\n",
      "Epoch 166/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3375 - accuracy: 0.8500 - val_loss: 0.8264 - val_accuracy: 0.6500\n",
      "Epoch 167/225\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.3242 - accuracy: 0.8875 - val_loss: 0.8164 - val_accuracy: 0.6500\n",
      "Epoch 168/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3074 - accuracy: 0.8500 - val_loss: 0.8244 - val_accuracy: 0.5500\n",
      "Epoch 169/225\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.3114 - accuracy: 0.8250 - val_loss: 0.8271 - val_accuracy: 0.6000\n",
      "Epoch 170/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3046 - accuracy: 0.8375 - val_loss: 0.8321 - val_accuracy: 0.6000\n",
      "Epoch 171/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2988 - accuracy: 0.8625 - val_loss: 0.8310 - val_accuracy: 0.6000\n",
      "Epoch 172/225\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2907 - accuracy: 0.8750 - val_loss: 0.8326 - val_accuracy: 0.6000\n",
      "Epoch 173/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2923 - accuracy: 0.8875 - val_loss: 0.8365 - val_accuracy: 0.5500\n",
      "Epoch 174/225\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2994 - accuracy: 0.9125 - val_loss: 0.8427 - val_accuracy: 0.6000\n",
      "Epoch 175/225\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2780 - accuracy: 0.9000 - val_loss: 0.8605 - val_accuracy: 0.5500\n",
      "Epoch 176/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2833 - accuracy: 0.8625 - val_loss: 0.8701 - val_accuracy: 0.5500\n",
      "Epoch 177/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2702 - accuracy: 0.9125 - val_loss: 0.8692 - val_accuracy: 0.6500\n",
      "Epoch 178/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2744 - accuracy: 0.8875 - val_loss: 0.8854 - val_accuracy: 0.5500\n",
      "Epoch 179/225\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2637 - accuracy: 0.9000 - val_loss: 0.8987 - val_accuracy: 0.5500\n",
      "Epoch 180/225\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2536 - accuracy: 0.9125 - val_loss: 0.9148 - val_accuracy: 0.5000\n",
      "Epoch 181/225\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.2531 - accuracy: 0.9125 - val_loss: 0.9132 - val_accuracy: 0.5500\n",
      "Epoch 182/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2413 - accuracy: 0.8875 - val_loss: 0.9045 - val_accuracy: 0.6000\n",
      "Epoch 183/225\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2468 - accuracy: 0.9125 - val_loss: 0.9125 - val_accuracy: 0.6500\n",
      "Epoch 184/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2367 - accuracy: 0.9125 - val_loss: 0.9530 - val_accuracy: 0.5000\n",
      "Epoch 185/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2370 - accuracy: 0.9125 - val_loss: 0.9622 - val_accuracy: 0.6000\n",
      "Epoch 186/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2298 - accuracy: 0.9125 - val_loss: 0.9822 - val_accuracy: 0.5000\n",
      "Epoch 187/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2205 - accuracy: 0.9250 - val_loss: 0.9777 - val_accuracy: 0.5500\n",
      "Epoch 188/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2069 - accuracy: 0.9375 - val_loss: 1.0233 - val_accuracy: 0.5500\n",
      "Epoch 189/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2106 - accuracy: 0.9250 - val_loss: 1.0015 - val_accuracy: 0.6000\n",
      "Epoch 190/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2069 - accuracy: 0.9125 - val_loss: 1.0282 - val_accuracy: 0.5500\n",
      "Epoch 191/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1893 - accuracy: 0.9625 - val_loss: 1.0742 - val_accuracy: 0.5500\n",
      "Epoch 192/225\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.2116 - accuracy: 0.9375 - val_loss: 1.0521 - val_accuracy: 0.6500\n",
      "Epoch 193/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1929 - accuracy: 0.9375 - val_loss: 1.0464 - val_accuracy: 0.5500\n",
      "Epoch 194/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2374 - accuracy: 0.9250 - val_loss: 1.0599 - val_accuracy: 0.6000\n",
      "Epoch 195/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2085 - accuracy: 0.9375 - val_loss: 1.1754 - val_accuracy: 0.5500\n",
      "Epoch 196/225\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.3177 - accuracy: 0.8875 - val_loss: 1.4210 - val_accuracy: 0.5000\n",
      "Epoch 197/225\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.3113 - accuracy: 0.8750 - val_loss: 1.4438 - val_accuracy: 0.5000\n",
      "Epoch 198/225\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.3096 - accuracy: 0.8625 - val_loss: 1.4052 - val_accuracy: 0.5000\n",
      "Epoch 199/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2919 - accuracy: 0.8875 - val_loss: 1.4065 - val_accuracy: 0.5000\n",
      "Epoch 200/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3005 - accuracy: 0.8875 - val_loss: 1.3707 - val_accuracy: 0.5000\n",
      "Epoch 201/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2758 - accuracy: 0.9125 - val_loss: 1.2528 - val_accuracy: 0.5000\n",
      "Epoch 202/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2716 - accuracy: 0.8750 - val_loss: 1.1681 - val_accuracy: 0.5000\n",
      "Epoch 203/225\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2575 - accuracy: 0.9000 - val_loss: 1.1045 - val_accuracy: 0.5000\n",
      "Epoch 204/225\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2344 - accuracy: 0.8875 - val_loss: 1.0674 - val_accuracy: 0.6500\n",
      "Epoch 205/225\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2241 - accuracy: 0.9250 - val_loss: 1.0540 - val_accuracy: 0.6000\n",
      "Epoch 206/225\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2189 - accuracy: 0.9250 - val_loss: 1.0557 - val_accuracy: 0.5500\n",
      "Epoch 207/225\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2109 - accuracy: 0.9375 - val_loss: 1.1120 - val_accuracy: 0.5500\n",
      "Epoch 208/225\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2143 - accuracy: 0.9250 - val_loss: 1.1332 - val_accuracy: 0.5500\n",
      "Epoch 209/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2199 - accuracy: 0.9125 - val_loss: 1.1183 - val_accuracy: 0.5500\n",
      "Epoch 210/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1898 - accuracy: 0.9375 - val_loss: 1.0923 - val_accuracy: 0.5500\n",
      "Epoch 211/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2011 - accuracy: 0.9125 - val_loss: 1.0947 - val_accuracy: 0.5500\n",
      "Epoch 212/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1878 - accuracy: 0.9500 - val_loss: 1.1247 - val_accuracy: 0.5500\n",
      "Epoch 213/225\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1855 - accuracy: 0.9250 - val_loss: 1.1521 - val_accuracy: 0.6000\n",
      "Epoch 214/225\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1774 - accuracy: 0.9500 - val_loss: 1.1476 - val_accuracy: 0.5500\n",
      "Epoch 215/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1768 - accuracy: 0.9500 - val_loss: 1.1609 - val_accuracy: 0.5500\n",
      "Epoch 216/225\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1718 - accuracy: 0.9625 - val_loss: 1.2120 - val_accuracy: 0.5500\n",
      "Epoch 217/225\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1688 - accuracy: 0.9750 - val_loss: 1.2238 - val_accuracy: 0.5500\n",
      "Epoch 218/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1640 - accuracy: 0.9625 - val_loss: 1.2315 - val_accuracy: 0.5500\n",
      "Epoch 219/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1597 - accuracy: 0.9750 - val_loss: 1.2419 - val_accuracy: 0.5500\n",
      "Epoch 220/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1600 - accuracy: 0.9625 - val_loss: 1.2623 - val_accuracy: 0.5500\n",
      "Epoch 221/225\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1583 - accuracy: 0.9625 - val_loss: 1.2411 - val_accuracy: 0.5500\n",
      "Epoch 222/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1518 - accuracy: 0.9625 - val_loss: 1.2436 - val_accuracy: 0.5500\n",
      "Epoch 223/225\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1429 - accuracy: 0.9625 - val_loss: 1.2621 - val_accuracy: 0.5500\n",
      "Epoch 224/225\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1482 - accuracy: 0.9625 - val_loss: 1.2895 - val_accuracy: 0.5000\n",
      "Epoch 225/225\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1417 - accuracy: 0.9750 - val_loss: 1.3269 - val_accuracy: 0.5500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2796c9b1710>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from  tensorflow . keras.models import Sequential\n",
    "from  tensorflow . keras.layers import Bidirectional,LSTM, Dense\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv('Clinical data ppf and eph.csv')\n",
    "\n",
    "# Split the data into input (X) and output (y) samples\n",
    "X = data.drop(columns=['intraop_ppf', 'intraop_eph'])\n",
    "y = data[['intraop_ppf', 'intraop_eph']]\n",
    "\n",
    "# Convert non-numeric values to NaN for each column in X\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "# Convert non-numeric values to NaN for each column in y\n",
    "for col in y.columns:\n",
    "    y[col] = pd.to_numeric(y[col], errors='coerce')\n",
    "\n",
    "# Fill NaN values with a specific value (e.g., 0) in X and y\n",
    "X.fillna(0, inplace=True)\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Convert X to a numpy array\n",
    "X = X.to_numpy()\n",
    "\n",
    "# Fill NaN values in y with 0\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape the input data to be 3D for BI-LSTM input [samples, timesteps, features]\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "X_train = np.random.randn(100, 10, 1)  \n",
    "y_train = np.random.randint(0, 2, size=(100,)) \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False), input_shape=(10, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=225, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a15e8644-6604-4896-bc40-8eaadbe5fa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_18948\\2385855753.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[col] = pd.to_numeric(y[col], errors='coerce')\n",
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_18948\\2385855753.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y.fillna(0, inplace=True)\n",
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_18948\\2385855753.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y.fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/235\n",
      "3/3 [==============================] - 3s 302ms/step - loss: 0.6937 - accuracy: 0.4375 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
      "Epoch 2/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6923 - accuracy: 0.5000 - val_loss: 0.6906 - val_accuracy: 0.6000\n",
      "Epoch 3/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6920 - accuracy: 0.4875 - val_loss: 0.6883 - val_accuracy: 0.5500\n",
      "Epoch 4/235\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6912 - accuracy: 0.5000 - val_loss: 0.6871 - val_accuracy: 0.5500\n",
      "Epoch 5/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6903 - accuracy: 0.5250 - val_loss: 0.6861 - val_accuracy: 0.5500\n",
      "Epoch 6/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6897 - accuracy: 0.5125 - val_loss: 0.6854 - val_accuracy: 0.5500\n",
      "Epoch 7/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6895 - accuracy: 0.5250 - val_loss: 0.6849 - val_accuracy: 0.5500\n",
      "Epoch 8/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6895 - accuracy: 0.5500 - val_loss: 0.6841 - val_accuracy: 0.5500\n",
      "Epoch 9/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6895 - accuracy: 0.5500 - val_loss: 0.6836 - val_accuracy: 0.5500\n",
      "Epoch 10/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6888 - accuracy: 0.5500 - val_loss: 0.6841 - val_accuracy: 0.5500\n",
      "Epoch 11/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6887 - accuracy: 0.5500 - val_loss: 0.6850 - val_accuracy: 0.5000\n",
      "Epoch 12/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6885 - accuracy: 0.5625 - val_loss: 0.6858 - val_accuracy: 0.5000\n",
      "Epoch 13/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6881 - accuracy: 0.5625 - val_loss: 0.6863 - val_accuracy: 0.5000\n",
      "Epoch 14/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6883 - accuracy: 0.5375 - val_loss: 0.6872 - val_accuracy: 0.5000\n",
      "Epoch 15/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6880 - accuracy: 0.5250 - val_loss: 0.6870 - val_accuracy: 0.4500\n",
      "Epoch 16/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6877 - accuracy: 0.5250 - val_loss: 0.6867 - val_accuracy: 0.4500\n",
      "Epoch 17/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6875 - accuracy: 0.5125 - val_loss: 0.6861 - val_accuracy: 0.4500\n",
      "Epoch 18/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6873 - accuracy: 0.5125 - val_loss: 0.6861 - val_accuracy: 0.4500\n",
      "Epoch 19/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6872 - accuracy: 0.5125 - val_loss: 0.6857 - val_accuracy: 0.4500\n",
      "Epoch 20/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6869 - accuracy: 0.5250 - val_loss: 0.6862 - val_accuracy: 0.4500\n",
      "Epoch 21/235\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6866 - accuracy: 0.5000 - val_loss: 0.6863 - val_accuracy: 0.4500\n",
      "Epoch 22/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6866 - accuracy: 0.5000 - val_loss: 0.6861 - val_accuracy: 0.4500\n",
      "Epoch 23/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6867 - accuracy: 0.5125 - val_loss: 0.6859 - val_accuracy: 0.4500\n",
      "Epoch 24/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6863 - accuracy: 0.5125 - val_loss: 0.6865 - val_accuracy: 0.4500\n",
      "Epoch 25/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6860 - accuracy: 0.5125 - val_loss: 0.6883 - val_accuracy: 0.4500\n",
      "Epoch 26/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6857 - accuracy: 0.5250 - val_loss: 0.6899 - val_accuracy: 0.4000\n",
      "Epoch 27/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6853 - accuracy: 0.5500 - val_loss: 0.6912 - val_accuracy: 0.4000\n",
      "Epoch 28/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6850 - accuracy: 0.5375 - val_loss: 0.6919 - val_accuracy: 0.4000\n",
      "Epoch 29/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6849 - accuracy: 0.5500 - val_loss: 0.6922 - val_accuracy: 0.4000\n",
      "Epoch 30/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6846 - accuracy: 0.5625 - val_loss: 0.6932 - val_accuracy: 0.3500\n",
      "Epoch 31/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6843 - accuracy: 0.5625 - val_loss: 0.6925 - val_accuracy: 0.4000\n",
      "Epoch 32/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6840 - accuracy: 0.5625 - val_loss: 0.6921 - val_accuracy: 0.4000\n",
      "Epoch 33/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6838 - accuracy: 0.5625 - val_loss: 0.6912 - val_accuracy: 0.4000\n",
      "Epoch 34/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6833 - accuracy: 0.5500 - val_loss: 0.6901 - val_accuracy: 0.4000\n",
      "Epoch 35/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6830 - accuracy: 0.5625 - val_loss: 0.6882 - val_accuracy: 0.4000\n",
      "Epoch 36/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6829 - accuracy: 0.5375 - val_loss: 0.6867 - val_accuracy: 0.4000\n",
      "Epoch 37/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6831 - accuracy: 0.5375 - val_loss: 0.6847 - val_accuracy: 0.4000\n",
      "Epoch 38/235\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6825 - accuracy: 0.5500 - val_loss: 0.6845 - val_accuracy: 0.4000\n",
      "Epoch 39/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6820 - accuracy: 0.5500 - val_loss: 0.6854 - val_accuracy: 0.4000\n",
      "Epoch 40/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6819 - accuracy: 0.5875 - val_loss: 0.6873 - val_accuracy: 0.4000\n",
      "Epoch 41/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6811 - accuracy: 0.6000 - val_loss: 0.6884 - val_accuracy: 0.3500\n",
      "Epoch 42/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6808 - accuracy: 0.6125 - val_loss: 0.6912 - val_accuracy: 0.3500\n",
      "Epoch 43/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6803 - accuracy: 0.6250 - val_loss: 0.6921 - val_accuracy: 0.3500\n",
      "Epoch 44/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6799 - accuracy: 0.6125 - val_loss: 0.6934 - val_accuracy: 0.3500\n",
      "Epoch 45/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6793 - accuracy: 0.6250 - val_loss: 0.6948 - val_accuracy: 0.3500\n",
      "Epoch 46/235\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6792 - accuracy: 0.6125 - val_loss: 0.6961 - val_accuracy: 0.4000\n",
      "Epoch 47/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6785 - accuracy: 0.6125 - val_loss: 0.6977 - val_accuracy: 0.3500\n",
      "Epoch 48/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6780 - accuracy: 0.6125 - val_loss: 0.6990 - val_accuracy: 0.3500\n",
      "Epoch 49/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6772 - accuracy: 0.6125 - val_loss: 0.6990 - val_accuracy: 0.4000\n",
      "Epoch 50/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6765 - accuracy: 0.6125 - val_loss: 0.6987 - val_accuracy: 0.4000\n",
      "Epoch 51/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6757 - accuracy: 0.6125 - val_loss: 0.6999 - val_accuracy: 0.4000\n",
      "Epoch 52/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6746 - accuracy: 0.6125 - val_loss: 0.7009 - val_accuracy: 0.3500\n",
      "Epoch 53/235\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6737 - accuracy: 0.6000 - val_loss: 0.7020 - val_accuracy: 0.4000\n",
      "Epoch 54/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6730 - accuracy: 0.6125 - val_loss: 0.7038 - val_accuracy: 0.5000\n",
      "Epoch 55/235\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6722 - accuracy: 0.6125 - val_loss: 0.7058 - val_accuracy: 0.5000\n",
      "Epoch 56/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6709 - accuracy: 0.6250 - val_loss: 0.7028 - val_accuracy: 0.4500\n",
      "Epoch 57/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6694 - accuracy: 0.5875 - val_loss: 0.7006 - val_accuracy: 0.4000\n",
      "Epoch 58/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6698 - accuracy: 0.6125 - val_loss: 0.6952 - val_accuracy: 0.4000\n",
      "Epoch 59/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6698 - accuracy: 0.6375 - val_loss: 0.6948 - val_accuracy: 0.4000\n",
      "Epoch 60/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6665 - accuracy: 0.6500 - val_loss: 0.6999 - val_accuracy: 0.4000\n",
      "Epoch 61/235\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6655 - accuracy: 0.6000 - val_loss: 0.7056 - val_accuracy: 0.5500\n",
      "Epoch 62/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6642 - accuracy: 0.6125 - val_loss: 0.7089 - val_accuracy: 0.5500\n",
      "Epoch 63/235\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.6646 - accuracy: 0.5875 - val_loss: 0.7116 - val_accuracy: 0.5500\n",
      "Epoch 64/235\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.6621 - accuracy: 0.6000 - val_loss: 0.7067 - val_accuracy: 0.4500\n",
      "Epoch 65/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6612 - accuracy: 0.6250 - val_loss: 0.7015 - val_accuracy: 0.4500\n",
      "Epoch 66/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6598 - accuracy: 0.6250 - val_loss: 0.7027 - val_accuracy: 0.4500\n",
      "Epoch 67/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6566 - accuracy: 0.6250 - val_loss: 0.7101 - val_accuracy: 0.4500\n",
      "Epoch 68/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6546 - accuracy: 0.6250 - val_loss: 0.7145 - val_accuracy: 0.5500\n",
      "Epoch 69/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6539 - accuracy: 0.6125 - val_loss: 0.7168 - val_accuracy: 0.5000\n",
      "Epoch 70/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6521 - accuracy: 0.6125 - val_loss: 0.7139 - val_accuracy: 0.4500\n",
      "Epoch 71/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6486 - accuracy: 0.6250 - val_loss: 0.7211 - val_accuracy: 0.4500\n",
      "Epoch 72/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6470 - accuracy: 0.6500 - val_loss: 0.7246 - val_accuracy: 0.4500\n",
      "Epoch 73/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6455 - accuracy: 0.6625 - val_loss: 0.7166 - val_accuracy: 0.4000\n",
      "Epoch 74/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6399 - accuracy: 0.6625 - val_loss: 0.7162 - val_accuracy: 0.4000\n",
      "Epoch 75/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6361 - accuracy: 0.6750 - val_loss: 0.7252 - val_accuracy: 0.4500\n",
      "Epoch 76/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6369 - accuracy: 0.6625 - val_loss: 0.7267 - val_accuracy: 0.4500\n",
      "Epoch 77/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6295 - accuracy: 0.6625 - val_loss: 0.7439 - val_accuracy: 0.5000\n",
      "Epoch 78/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6260 - accuracy: 0.6500 - val_loss: 0.7507 - val_accuracy: 0.5500\n",
      "Epoch 79/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6235 - accuracy: 0.6625 - val_loss: 0.7376 - val_accuracy: 0.5500\n",
      "Epoch 80/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6166 - accuracy: 0.6625 - val_loss: 0.7494 - val_accuracy: 0.5500\n",
      "Epoch 81/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6087 - accuracy: 0.6250 - val_loss: 0.7833 - val_accuracy: 0.5000\n",
      "Epoch 82/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.6098 - accuracy: 0.6500 - val_loss: 0.8043 - val_accuracy: 0.5500\n",
      "Epoch 83/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.6053 - accuracy: 0.6375 - val_loss: 0.7925 - val_accuracy: 0.5500\n",
      "Epoch 84/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5945 - accuracy: 0.6875 - val_loss: 0.7424 - val_accuracy: 0.5000\n",
      "Epoch 85/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.5889 - accuracy: 0.7125 - val_loss: 0.7353 - val_accuracy: 0.5000\n",
      "Epoch 86/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5874 - accuracy: 0.7125 - val_loss: 0.7793 - val_accuracy: 0.4500\n",
      "Epoch 87/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5800 - accuracy: 0.7125 - val_loss: 0.8112 - val_accuracy: 0.4500\n",
      "Epoch 88/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5742 - accuracy: 0.7250 - val_loss: 0.8479 - val_accuracy: 0.4500\n",
      "Epoch 89/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5671 - accuracy: 0.7125 - val_loss: 0.8162 - val_accuracy: 0.4000\n",
      "Epoch 90/235\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.5623 - accuracy: 0.7375 - val_loss: 0.7899 - val_accuracy: 0.4500\n",
      "Epoch 91/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5566 - accuracy: 0.7250 - val_loss: 0.8217 - val_accuracy: 0.4000\n",
      "Epoch 92/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5524 - accuracy: 0.7375 - val_loss: 0.8471 - val_accuracy: 0.4000\n",
      "Epoch 93/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5423 - accuracy: 0.7500 - val_loss: 0.8072 - val_accuracy: 0.4000\n",
      "Epoch 94/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5425 - accuracy: 0.7250 - val_loss: 0.8540 - val_accuracy: 0.4000\n",
      "Epoch 95/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5284 - accuracy: 0.7375 - val_loss: 0.9593 - val_accuracy: 0.4500\n",
      "Epoch 96/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5285 - accuracy: 0.7125 - val_loss: 0.9248 - val_accuracy: 0.4000\n",
      "Epoch 97/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5207 - accuracy: 0.7625 - val_loss: 0.8237 - val_accuracy: 0.4000\n",
      "Epoch 98/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.5141 - accuracy: 0.7375 - val_loss: 0.8533 - val_accuracy: 0.4000\n",
      "Epoch 99/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5072 - accuracy: 0.7625 - val_loss: 0.9315 - val_accuracy: 0.4000\n",
      "Epoch 100/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.5071 - accuracy: 0.7500 - val_loss: 0.9062 - val_accuracy: 0.3500\n",
      "Epoch 101/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4899 - accuracy: 0.7625 - val_loss: 0.7715 - val_accuracy: 0.4000\n",
      "Epoch 102/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4958 - accuracy: 0.7625 - val_loss: 0.8209 - val_accuracy: 0.4000\n",
      "Epoch 103/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4830 - accuracy: 0.8000 - val_loss: 0.9377 - val_accuracy: 0.3500\n",
      "Epoch 104/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4733 - accuracy: 0.7750 - val_loss: 0.9415 - val_accuracy: 0.3500\n",
      "Epoch 105/235\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4660 - accuracy: 0.8000 - val_loss: 0.9296 - val_accuracy: 0.3500\n",
      "Epoch 106/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4578 - accuracy: 0.7875 - val_loss: 0.9648 - val_accuracy: 0.3500\n",
      "Epoch 107/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4505 - accuracy: 0.8000 - val_loss: 0.8969 - val_accuracy: 0.5000\n",
      "Epoch 108/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4429 - accuracy: 0.8125 - val_loss: 0.9149 - val_accuracy: 0.5500\n",
      "Epoch 109/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4325 - accuracy: 0.8250 - val_loss: 0.8847 - val_accuracy: 0.5500\n",
      "Epoch 110/235\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.4276 - accuracy: 0.8125 - val_loss: 0.8414 - val_accuracy: 0.6000\n",
      "Epoch 111/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.4150 - accuracy: 0.8125 - val_loss: 0.9596 - val_accuracy: 0.5000\n",
      "Epoch 112/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4143 - accuracy: 0.8125 - val_loss: 0.9611 - val_accuracy: 0.5000\n",
      "Epoch 113/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3999 - accuracy: 0.8250 - val_loss: 0.8549 - val_accuracy: 0.6000\n",
      "Epoch 114/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3973 - accuracy: 0.8375 - val_loss: 0.9280 - val_accuracy: 0.6000\n",
      "Epoch 115/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3816 - accuracy: 0.8375 - val_loss: 1.0362 - val_accuracy: 0.6000\n",
      "Epoch 116/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3780 - accuracy: 0.8500 - val_loss: 1.0651 - val_accuracy: 0.6000\n",
      "Epoch 117/235\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.3606 - accuracy: 0.8500 - val_loss: 0.9891 - val_accuracy: 0.6000\n",
      "Epoch 118/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3619 - accuracy: 0.8625 - val_loss: 1.0088 - val_accuracy: 0.6000\n",
      "Epoch 119/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3560 - accuracy: 0.8500 - val_loss: 1.1006 - val_accuracy: 0.6000\n",
      "Epoch 120/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3389 - accuracy: 0.8375 - val_loss: 0.9533 - val_accuracy: 0.6500\n",
      "Epoch 121/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3349 - accuracy: 0.8375 - val_loss: 0.8270 - val_accuracy: 0.6500\n",
      "Epoch 122/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3311 - accuracy: 0.8625 - val_loss: 0.9371 - val_accuracy: 0.6500\n",
      "Epoch 123/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3176 - accuracy: 0.8625 - val_loss: 1.1149 - val_accuracy: 0.6500\n",
      "Epoch 124/235\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.3105 - accuracy: 0.8625 - val_loss: 1.1056 - val_accuracy: 0.6500\n",
      "Epoch 125/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.3062 - accuracy: 0.8750 - val_loss: 1.0601 - val_accuracy: 0.6500\n",
      "Epoch 126/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.3227 - accuracy: 0.8375 - val_loss: 1.1307 - val_accuracy: 0.6500\n",
      "Epoch 127/235\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2876 - accuracy: 0.8750 - val_loss: 1.1822 - val_accuracy: 0.6000\n",
      "Epoch 128/235\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.3776 - accuracy: 0.8375 - val_loss: 1.0836 - val_accuracy: 0.6000\n",
      "Epoch 129/235\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2943 - accuracy: 0.8750 - val_loss: 1.0308 - val_accuracy: 0.6500\n",
      "Epoch 130/235\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.3372 - accuracy: 0.8500 - val_loss: 1.2209 - val_accuracy: 0.6000\n",
      "Epoch 131/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2988 - accuracy: 0.8750 - val_loss: 1.4199 - val_accuracy: 0.6000\n",
      "Epoch 132/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2892 - accuracy: 0.9000 - val_loss: 1.2020 - val_accuracy: 0.6000\n",
      "Epoch 133/235\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2853 - accuracy: 0.8625 - val_loss: 1.0174 - val_accuracy: 0.6000\n",
      "Epoch 134/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2695 - accuracy: 0.8875 - val_loss: 1.1266 - val_accuracy: 0.6000\n",
      "Epoch 135/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2570 - accuracy: 0.9000 - val_loss: 1.1854 - val_accuracy: 0.6000\n",
      "Epoch 136/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2577 - accuracy: 0.8875 - val_loss: 1.1249 - val_accuracy: 0.6000\n",
      "Epoch 137/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2540 - accuracy: 0.9000 - val_loss: 1.0847 - val_accuracy: 0.6500\n",
      "Epoch 138/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.2495 - accuracy: 0.9000 - val_loss: 1.1119 - val_accuracy: 0.6500\n",
      "Epoch 139/235\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2419 - accuracy: 0.9125 - val_loss: 1.2166 - val_accuracy: 0.6000\n",
      "Epoch 140/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2339 - accuracy: 0.9125 - val_loss: 1.3230 - val_accuracy: 0.6000\n",
      "Epoch 141/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2291 - accuracy: 0.9250 - val_loss: 1.3447 - val_accuracy: 0.6000\n",
      "Epoch 142/235\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2277 - accuracy: 0.9250 - val_loss: 1.3817 - val_accuracy: 0.6000\n",
      "Epoch 143/235\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.2211 - accuracy: 0.9125 - val_loss: 1.2192 - val_accuracy: 0.6000\n",
      "Epoch 144/235\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2165 - accuracy: 0.9250 - val_loss: 1.2730 - val_accuracy: 0.6000\n",
      "Epoch 145/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.2097 - accuracy: 0.9250 - val_loss: 1.4613 - val_accuracy: 0.5500\n",
      "Epoch 146/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.2071 - accuracy: 0.9250 - val_loss: 1.5038 - val_accuracy: 0.6000\n",
      "Epoch 147/235\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.2005 - accuracy: 0.9250 - val_loss: 1.4308 - val_accuracy: 0.6000\n",
      "Epoch 148/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1960 - accuracy: 0.9125 - val_loss: 1.3737 - val_accuracy: 0.6000\n",
      "Epoch 149/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1942 - accuracy: 0.9250 - val_loss: 1.4177 - val_accuracy: 0.6000\n",
      "Epoch 150/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1863 - accuracy: 0.9250 - val_loss: 1.5378 - val_accuracy: 0.6000\n",
      "Epoch 151/235\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1835 - accuracy: 0.9250 - val_loss: 1.5806 - val_accuracy: 0.6000\n",
      "Epoch 152/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.1787 - accuracy: 0.9250 - val_loss: 1.5194 - val_accuracy: 0.6000\n",
      "Epoch 153/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1748 - accuracy: 0.9375 - val_loss: 1.4606 - val_accuracy: 0.6000\n",
      "Epoch 154/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1699 - accuracy: 0.9375 - val_loss: 1.3692 - val_accuracy: 0.6000\n",
      "Epoch 155/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1665 - accuracy: 0.9375 - val_loss: 1.4294 - val_accuracy: 0.6000\n",
      "Epoch 156/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1614 - accuracy: 0.9375 - val_loss: 1.5639 - val_accuracy: 0.6000\n",
      "Epoch 157/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1606 - accuracy: 0.9375 - val_loss: 1.5582 - val_accuracy: 0.6000\n",
      "Epoch 158/235\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1506 - accuracy: 0.9500 - val_loss: 1.5640 - val_accuracy: 0.6000\n",
      "Epoch 159/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1558 - accuracy: 0.9375 - val_loss: 1.6590 - val_accuracy: 0.6000\n",
      "Epoch 160/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1481 - accuracy: 0.9375 - val_loss: 1.6410 - val_accuracy: 0.6000\n",
      "Epoch 161/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1388 - accuracy: 0.9500 - val_loss: 1.5700 - val_accuracy: 0.6500\n",
      "Epoch 162/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1433 - accuracy: 0.9625 - val_loss: 1.4833 - val_accuracy: 0.6500\n",
      "Epoch 163/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1369 - accuracy: 0.9625 - val_loss: 1.5400 - val_accuracy: 0.6500\n",
      "Epoch 164/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1320 - accuracy: 0.9625 - val_loss: 1.6942 - val_accuracy: 0.6000\n",
      "Epoch 165/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1337 - accuracy: 0.9500 - val_loss: 1.7058 - val_accuracy: 0.6000\n",
      "Epoch 166/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1217 - accuracy: 0.9625 - val_loss: 1.5431 - val_accuracy: 0.6500\n",
      "Epoch 167/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1197 - accuracy: 0.9750 - val_loss: 1.4568 - val_accuracy: 0.6500\n",
      "Epoch 168/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1261 - accuracy: 0.9625 - val_loss: 1.4986 - val_accuracy: 0.6500\n",
      "Epoch 169/235\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1185 - accuracy: 0.9625 - val_loss: 1.5702 - val_accuracy: 0.6500\n",
      "Epoch 170/235\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1114 - accuracy: 0.9750 - val_loss: 1.6142 - val_accuracy: 0.6500\n",
      "Epoch 171/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1078 - accuracy: 0.9750 - val_loss: 1.6865 - val_accuracy: 0.6500\n",
      "Epoch 172/235\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1052 - accuracy: 0.9875 - val_loss: 1.6811 - val_accuracy: 0.6500\n",
      "Epoch 173/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1018 - accuracy: 0.9875 - val_loss: 1.5972 - val_accuracy: 0.6500\n",
      "Epoch 174/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.1006 - accuracy: 0.9750 - val_loss: 1.5980 - val_accuracy: 0.6500\n",
      "Epoch 175/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0968 - accuracy: 0.9875 - val_loss: 1.6585 - val_accuracy: 0.6500\n",
      "Epoch 176/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0933 - accuracy: 0.9875 - val_loss: 1.7090 - val_accuracy: 0.6500\n",
      "Epoch 177/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0906 - accuracy: 0.9875 - val_loss: 1.7340 - val_accuracy: 0.6500\n",
      "Epoch 178/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0880 - accuracy: 0.9875 - val_loss: 1.6667 - val_accuracy: 0.6500\n",
      "Epoch 179/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0876 - accuracy: 0.9875 - val_loss: 1.5342 - val_accuracy: 0.6500\n",
      "Epoch 180/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0846 - accuracy: 0.9875 - val_loss: 1.4791 - val_accuracy: 0.6500\n",
      "Epoch 181/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0814 - accuracy: 0.9875 - val_loss: 1.6108 - val_accuracy: 0.6500\n",
      "Epoch 182/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0840 - accuracy: 0.9875 - val_loss: 1.7213 - val_accuracy: 0.6500\n",
      "Epoch 183/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0755 - accuracy: 0.9875 - val_loss: 1.6746 - val_accuracy: 0.6500\n",
      "Epoch 184/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0762 - accuracy: 0.9875 - val_loss: 1.4857 - val_accuracy: 0.6500\n",
      "Epoch 185/235\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.0861 - accuracy: 0.9750 - val_loss: 1.4689 - val_accuracy: 0.6500\n",
      "Epoch 186/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0714 - accuracy: 0.9875 - val_loss: 1.6407 - val_accuracy: 0.6500\n",
      "Epoch 187/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0725 - accuracy: 0.9875 - val_loss: 1.7354 - val_accuracy: 0.6000\n",
      "Epoch 188/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0667 - accuracy: 0.9875 - val_loss: 1.7296 - val_accuracy: 0.6500\n",
      "Epoch 189/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0660 - accuracy: 0.9875 - val_loss: 1.7076 - val_accuracy: 0.6500\n",
      "Epoch 190/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0639 - accuracy: 0.9875 - val_loss: 1.6825 - val_accuracy: 0.6500\n",
      "Epoch 191/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0590 - accuracy: 0.9875 - val_loss: 1.6650 - val_accuracy: 0.6500\n",
      "Epoch 192/235\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0594 - accuracy: 0.9875 - val_loss: 1.6669 - val_accuracy: 0.6500\n",
      "Epoch 193/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0568 - accuracy: 0.9875 - val_loss: 1.6671 - val_accuracy: 0.6500\n",
      "Epoch 194/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0541 - accuracy: 0.9875 - val_loss: 1.6601 - val_accuracy: 0.6500\n",
      "Epoch 195/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0528 - accuracy: 0.9875 - val_loss: 1.6030 - val_accuracy: 0.6500\n",
      "Epoch 196/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0503 - accuracy: 0.9875 - val_loss: 1.6174 - val_accuracy: 0.6500\n",
      "Epoch 197/235\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.0499 - accuracy: 0.9875 - val_loss: 1.6423 - val_accuracy: 0.6500\n",
      "Epoch 198/235\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0481 - accuracy: 0.9875 - val_loss: 1.6381 - val_accuracy: 0.6500\n",
      "Epoch 199/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0457 - accuracy: 0.9875 - val_loss: 1.6511 - val_accuracy: 0.6000\n",
      "Epoch 200/235\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0444 - accuracy: 0.9875 - val_loss: 1.6955 - val_accuracy: 0.6000\n",
      "Epoch 201/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0427 - accuracy: 0.9875 - val_loss: 1.7285 - val_accuracy: 0.6500\n",
      "Epoch 202/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0423 - accuracy: 0.9875 - val_loss: 1.7200 - val_accuracy: 0.6000\n",
      "Epoch 203/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0401 - accuracy: 0.9875 - val_loss: 1.6170 - val_accuracy: 0.6000\n",
      "Epoch 204/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0375 - accuracy: 0.9875 - val_loss: 1.5455 - val_accuracy: 0.6000\n",
      "Epoch 205/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0378 - accuracy: 0.9875 - val_loss: 1.5617 - val_accuracy: 0.6000\n",
      "Epoch 206/235\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0361 - accuracy: 0.9875 - val_loss: 1.6508 - val_accuracy: 0.6000\n",
      "Epoch 207/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0335 - accuracy: 0.9875 - val_loss: 1.7511 - val_accuracy: 0.5500\n",
      "Epoch 208/235\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0326 - accuracy: 0.9875 - val_loss: 1.7891 - val_accuracy: 0.5500\n",
      "Epoch 209/235\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.0313 - accuracy: 0.9875 - val_loss: 1.7635 - val_accuracy: 0.6000\n",
      "Epoch 210/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0298 - accuracy: 0.9875 - val_loss: 1.7516 - val_accuracy: 0.6000\n",
      "Epoch 211/235\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0290 - accuracy: 0.9875 - val_loss: 1.7130 - val_accuracy: 0.6000\n",
      "Epoch 212/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0280 - accuracy: 0.9875 - val_loss: 1.7054 - val_accuracy: 0.6000\n",
      "Epoch 213/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0269 - accuracy: 0.9875 - val_loss: 1.6981 - val_accuracy: 0.6000\n",
      "Epoch 214/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0260 - accuracy: 0.9875 - val_loss: 1.7527 - val_accuracy: 0.6000\n",
      "Epoch 215/235\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0246 - accuracy: 0.9875 - val_loss: 1.8166 - val_accuracy: 0.5500\n",
      "Epoch 216/235\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0240 - accuracy: 0.9875 - val_loss: 1.8745 - val_accuracy: 0.5500\n",
      "Epoch 217/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 1.8729 - val_accuracy: 0.5500\n",
      "Epoch 218/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 1.8559 - val_accuracy: 0.5500\n",
      "Epoch 219/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 1.8438 - val_accuracy: 0.5500\n",
      "Epoch 220/235\n",
      "3/3 [==============================] - 0s 34ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 1.8283 - val_accuracy: 0.5000\n",
      "Epoch 221/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 1.8134 - val_accuracy: 0.5500\n",
      "Epoch 222/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 1.8027 - val_accuracy: 0.5500\n",
      "Epoch 223/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 1.7906 - val_accuracy: 0.5500\n",
      "Epoch 224/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 1.8113 - val_accuracy: 0.5500\n",
      "Epoch 225/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 1.8386 - val_accuracy: 0.5500\n",
      "Epoch 226/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 1.8597 - val_accuracy: 0.5000\n",
      "Epoch 227/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 1.8726 - val_accuracy: 0.5000\n",
      "Epoch 228/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 1.8770 - val_accuracy: 0.5000\n",
      "Epoch 229/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 1.9088 - val_accuracy: 0.5000\n",
      "Epoch 230/235\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 1.9575 - val_accuracy: 0.5000\n",
      "Epoch 231/235\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 1.9907 - val_accuracy: 0.5000\n",
      "Epoch 232/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 2.0044 - val_accuracy: 0.5000\n",
      "Epoch 233/235\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 1.9927 - val_accuracy: 0.5000\n",
      "Epoch 234/235\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 1.9806 - val_accuracy: 0.5000\n",
      "Epoch 235/235\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 1.9539 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2796f855c90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from  tensorflow . keras.models import Sequential\n",
    "from  tensorflow . keras.layers import Bidirectional,LSTM, Dense\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv('Clinical data ppf and eph.csv')\n",
    "\n",
    "# Split the data into input (X) and output (y) samples\n",
    "X = data.drop(columns=['intraop_ppf', 'intraop_eph'])\n",
    "y = data[['intraop_ppf', 'intraop_eph']]\n",
    "\n",
    "# Convert non-numeric values to NaN for each column in X\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "# Convert non-numeric values to NaN for each column in y\n",
    "for col in y.columns:\n",
    "    y[col] = pd.to_numeric(y[col], errors='coerce')\n",
    "\n",
    "# Fill NaN values with a specific value (e.g., 0) in X and y\n",
    "X.fillna(0, inplace=True)\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Convert X to a numpy array\n",
    "X = X.to_numpy()\n",
    "\n",
    "# Fill NaN values in y with 0\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape the input data to be 3D for BI-LSTM input [samples, timesteps, features]\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "X_train = np.random.randn(100, 10, 1)  \n",
    "y_train = np.random.randint(0, 2, size=(100,)) \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=False), input_shape=(10, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=235, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04a3d226-07f1-4d0c-adc9-2e3ef41fb14d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_score\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming you have already defined y_train_combined\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m precision_0 \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train_combined\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_combined\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m precision_1 \u001b[38;5;241m=\u001b[39m precision_score(y_train_combined[:, \u001b[38;5;241m0\u001b[39m], y_train_combined[:, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision for output parameter 0:\u001b[39m\u001b[38;5;124m\"\u001b[39m, precision_0)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2170\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   2004\u001b[0m     {\n\u001b[0;32m   2005\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2030\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2031\u001b[0m ):\n\u001b[0;32m   2032\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   2033\u001b[0m \n\u001b[0;32m   2034\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2168\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   2169\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2170\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2171\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2177\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1755\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m \n\u001b[0;32m   1594\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1752\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m _check_zero_division(zero_division)\n\u001b[1;32m-> 1755\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1527\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1527\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1530\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:94\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     91\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     96\u001b[0m             type_true, type_pred\n\u001b[0;32m     97\u001b[0m         )\n\u001b[0;32m     98\u001b[0m     )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    101\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Assuming you have already defined y_train_combined\n",
    "precision_0 = precision_score(y_train_combined[:, 0], y_train_combined[:, 1])\n",
    "precision_1 = precision_score(y_train_combined[:, 0], y_train_combined[:, 1])\n",
    "\n",
    "print(\"Precision for output parameter 0:\", precision_0)\n",
    "print(\"Precision for output parameter 1:\", precision_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44b33019-d38e-49f4-8aa7-c42dc471916a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate precision for each output parameter separately\u001b[39;00m\n\u001b[0;32m      8\u001b[0m precision_0 \u001b[38;5;241m=\u001b[39m precision_score(y_train_combined[:, \u001b[38;5;241m0\u001b[39m], y_pred_binary)\n\u001b[1;32m----> 9\u001b[0m precision_1 \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train_combined\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_binary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision for output parameter 0:\u001b[39m\u001b[38;5;124m\"\u001b[39m, precision_0)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision for output parameter 1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, precision_1)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2170\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   2004\u001b[0m     {\n\u001b[0;32m   2005\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2030\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2031\u001b[0m ):\n\u001b[0;32m   2032\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   2033\u001b[0m \n\u001b[0;32m   2034\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2168\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   2169\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2170\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2171\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2177\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1755\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m \n\u001b[0;32m   1594\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1752\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m _check_zero_division(zero_division)\n\u001b[1;32m-> 1755\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1527\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1527\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1530\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:94\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     91\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     96\u001b[0m             type_true, type_pred\n\u001b[0;32m     97\u001b[0m         )\n\u001b[0;32m     98\u001b[0m     )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    101\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
     ]
    }
   ],
   "source": [
    "# Assuming you have already defined y_train_combined\n",
    "threshold = 0.5  # Define your threshold here\n",
    "\n",
    "# Convert y_pred to binary predictions based on the threshold\n",
    "y_pred_binary = np.where(y_pred_reshaped > threshold, 1, 0)\n",
    "\n",
    "# Calculate precision for each output parameter separately\n",
    "precision_0 = precision_score(y_train_combined[:, 0], y_pred_binary)\n",
    "precision_1 = precision_score(y_train_combined[:, 1], y_pred_binary)\n",
    "\n",
    "print(\"Precision for output parameter 0:\", precision_0)\n",
    "print(\"Precision for output parameter 1:\", precision_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4594050-9047-4b27-ba02-b14e88ca102a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate precision for each output parameter separately\u001b[39;00m\n\u001b[0;32m      8\u001b[0m precision_0 \u001b[38;5;241m=\u001b[39m precision_score(y_train_combined[:, \u001b[38;5;241m0\u001b[39m], y_pred_binary[:, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m----> 9\u001b[0m precision_1 \u001b[38;5;241m=\u001b[39m precision_score(y_train_combined[:, \u001b[38;5;241m1\u001b[39m], \u001b[43my_pred_binary\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision for output parameter 0:\u001b[39m\u001b[38;5;124m\"\u001b[39m, precision_0)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision for output parameter 1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, precision_1)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "# Assuming you have already defined y_train_combined\n",
    "threshold = 0.5  # Define your threshold here\n",
    "\n",
    "# Convert y_pred to binary predictions based on the threshold\n",
    "y_pred_binary = np.where(y_pred_reshaped > threshold, 1, 0)\n",
    "\n",
    "# Calculate precision for each output parameter separately\n",
    "precision_0 = precision_score(y_train_combined[:, 0], y_pred_binary[:, 0])\n",
    "precision_1 = precision_score(y_train_combined[:, 1], y_pred_binary[:, 1])\n",
    "\n",
    "print(\"Precision for output parameter 0:\", precision_0)\n",
    "print(\"Precision for output parameter 1:\", precision_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16913a4e-d0aa-495f-aba5-555a3a9c0b57",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m y_combined \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((y_train_combined, y_pred_binary), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate precision for each output parameter separately\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m precision_0 \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_combined\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_combined\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m precision_1 \u001b[38;5;241m=\u001b[39m precision_score(y_combined[:, \u001b[38;5;241m2\u001b[39m], y_combined[:, \u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision for output parameter 0:\u001b[39m\u001b[38;5;124m\"\u001b[39m, precision_0)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2170\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   2004\u001b[0m     {\n\u001b[0;32m   2005\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2030\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2031\u001b[0m ):\n\u001b[0;32m   2032\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   2033\u001b[0m \n\u001b[0;32m   2034\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2168\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   2169\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2170\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2171\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2177\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1755\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m \n\u001b[0;32m   1594\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1752\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m _check_zero_division(zero_division)\n\u001b[1;32m-> 1755\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1527\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1527\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1530\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\Jupyter\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:94\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     91\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     96\u001b[0m             type_true, type_pred\n\u001b[0;32m     97\u001b[0m         )\n\u001b[0;32m     98\u001b[0m     )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    101\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "# Assuming you have already defined y_train_combined and y_pred_binary\n",
    "\n",
    "# Concatenate y_train_combined and y_pred_binary along the second axis to create a (n_samples, 2) array\n",
    "y_combined = np.concatenate((y_train_combined, y_pred_binary), axis=1)\n",
    "\n",
    "# Calculate precision for each output parameter separately\n",
    "precision_0 = precision_score(y_combined[:, 0], y_combined[:, 1])\n",
    "precision_1 = precision_score(y_combined[:, 2], y_combined[:, 3])\n",
    "\n",
    "print(\"Precision for output parameter 0:\", precision_0)\n",
    "print(\"Precision for output parameter 1:\", precision_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cdc3b1c-e2b8-4fde-8678-ceaddfc7019c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate precision for each output parameter separately\u001b[39;00m\n\u001b[0;32m      5\u001b[0m precision_0 \u001b[38;5;241m=\u001b[39m precision_score(y_combined[:, \u001b[38;5;241m0\u001b[39m], y_pred_binary[:, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m precision_1 \u001b[38;5;241m=\u001b[39m precision_score(y_combined[:, \u001b[38;5;241m1\u001b[39m], \u001b[43my_pred_binary\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision for output parameter 0:\u001b[39m\u001b[38;5;124m\"\u001b[39m, precision_0)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision for output parameter 1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, precision_1)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "threshold = 0.5  # Define the threshold for binary classification\n",
    "y_pred_binary = (y_pred_binary >= threshold).astype(int)  # Convert to binary labels\n",
    "\n",
    "# Calculate precision for each output parameter separately\n",
    "precision_0 = precision_score(y_combined[:, 0], y_pred_binary[:, 0])\n",
    "precision_1 = precision_score(y_combined[:, 1], y_pred_binary[:, 1])\n",
    "\n",
    "print(\"Precision for output parameter 0:\", precision_0)\n",
    "print(\"Precision for output parameter 1:\", precision_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e56515d-7eb8-4740-8e7b-2253688af477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
